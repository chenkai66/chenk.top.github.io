<!DOCTYPE html>



<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            自然语言处理（十二）—— 前沿技术与实战应用 |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"zh-CN","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">自然语言处理（十二）—— 前沿技术与实战应用</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-03-31 00:00:00</span>
        <span class="mobile">2025-03-31 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Applications/">Applications</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>5.2k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>23 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>大语言模型的能力边界正在快速扩展：从简单的文本生成到复杂的工具调用，从代码补全到长文档理解，从单轮对话到多轮推理。这些能力背后是
Agent 架构、代码专用模型、长上下文技术等前沿研究的突破。</p>
<p>但能力提升也带来了新挑战。模型会"编造"看似合理但实际不存在的信息（幻觉问题），可能生成有害内容（安全性问题），需要与人类价值观对齐（对齐问题）。更重要的是，如何将这些技术落地到生产环境？如何设计可扩展的架构？如何监控和优化性能？</p>
<p>本文深入解析 NLP 领域的前沿技术：从 Function Calling 和 ReAct 的
Agent 架构设计，到 CodeLlama 和 StarCoder 的代码生成原理，从 LongLoRA 和
LongLLaMA
的长上下文实现，到幻觉缓解和安全对齐的技术方案。更重要的是，本文提供了完整的生产级部署方案：从
FastAPI 服务设计到 Docker
容器化，从监控系统到性能优化，每个环节都配有可运行的代码和最佳实践。</p>
<span id="more"></span>
<h2 id="agent-与工具使用">Agent 与工具使用</h2>
<h3 id="function-calling让-llm-调用外部工具">Function Calling：让 LLM
调用外部工具</h3>
<p>Function Calling 是 OpenAI 在 GPT-4
中引入的功能，允许模型在生成文本的过程中调用外部函数和 API。这使得 LLM
能够突破纯文本的限制，与外部系统（如数据库、API、工具）进行交互，实现更强大的功能。</p>
<p><strong>核心概念</strong>：</p>
<p>Function Calling 的工作流程分为三个步骤：</p>
<ol type="1">
<li><p><strong>函数定义</strong>：开发者定义可用的函数及其参数，包括函数名、描述、参数类型和约束。这些定义以
JSON Schema 格式提供给模型。</p></li>
<li><p><strong>函数决策</strong>：模型根据用户查询和函数定义，决定是否需要调用函数。如果需要，模型会生成符合函数签名的参数（JSON
格式）。</p></li>
<li><p><strong>函数执行与结果整合</strong>：系统执行函数调用，将结果返回给模型，模型基于函数结果生成最终回答。</p></li>
</ol>
<p><strong>为什么有效</strong>：</p>
<p>Function Calling
的优势在于它将"理解"和"执行"分离：模型负责理解用户意图并生成正确的参数，外部系统负责执行具体操作。这种设计既保证了安全性（函数执行在受控环境中），又提供了灵活性（可以轻松添加新函数）。</p>
<p><strong>实现示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义可用函数</span></span><br><span class="line">functions = [</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_weather"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定城市的天气信息"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: {</span><br><span class="line">                <span class="string">"location"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"城市名称，例如：北京、上海"</span></span><br><span class="line">                },</span><br><span class="line">                <span class="string">"unit"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"enum"</span>: [<span class="string">"celsius"</span>, <span class="string">"fahrenheit"</span>],</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"温度单位"</span></span><br><span class="line">                }</span><br><span class="line">            },</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"location"</span>]</span><br><span class="line">        }</span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"send_email"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"发送电子邮件"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: {</span><br><span class="line">                <span class="string">"to"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"description"</span>: <span class="string">"收件人邮箱"</span>},</span><br><span class="line">                <span class="string">"subject"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"description"</span>: <span class="string">"邮件主题"</span>},</span><br><span class="line">                <span class="string">"body"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"description"</span>: <span class="string">"邮件内容"</span>}</span><br><span class="line">            },</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"to"</span>, <span class="string">"subject"</span>, <span class="string">"body"</span>]</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weather</span>(<span class="params">location, unit=<span class="string">"celsius"</span></span>):</span><br><span class="line">    <span class="string">"""模拟天气查询函数"""</span></span><br><span class="line">    <span class="comment"># 实际应用中调用真实天气 API</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f"<span class="subst">{location}</span>的天气：25度<span class="subst">{unit}</span>，晴天"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">send_email</span>(<span class="params">to, subject, body</span>):</span><br><span class="line">    <span class="string">"""模拟邮件发送函数"""</span></span><br><span class="line">    <span class="comment"># 实际应用中调用邮件服务</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f"邮件已发送至 <span class="subst">{to}</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat_with_functions</span>(<span class="params">user_message</span>):</span><br><span class="line">    <span class="string">"""使用 Function Calling 进行对话"""</span></span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">"gpt-4"</span>,</span><br><span class="line">        messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: user_message}],</span><br><span class="line">        functions=functions,</span><br><span class="line">        function_call=<span class="string">"auto"</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    message = response.choices[<span class="number">0</span>].message</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查是否需要调用函数</span></span><br><span class="line">    <span class="keyword">if</span> message.function_call:</span><br><span class="line">        function_name = message.function_call.name</span><br><span class="line">        function_args = json.loads(message.function_call.arguments)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 调用对应函数</span></span><br><span class="line">        <span class="keyword">if</span> function_name == <span class="string">"get_weather"</span>:</span><br><span class="line">            result = get_weather(**function_args)</span><br><span class="line">        <span class="keyword">elif</span> function_name == <span class="string">"send_email"</span>:</span><br><span class="line">            result = send_email(**function_args)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result = <span class="string">"未知函数"</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将函数结果返回给模型</span></span><br><span class="line">        second_response = client.chat.completions.create(</span><br><span class="line">            model=<span class="string">"gpt-4"</span>,</span><br><span class="line">            messages=[</span><br><span class="line">                {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: user_message},</span><br><span class="line">                message,</span><br><span class="line">                {</span><br><span class="line">                    <span class="string">"role"</span>: <span class="string">"function"</span>,</span><br><span class="line">                    <span class="string">"name"</span>: function_name,</span><br><span class="line">                    <span class="string">"content"</span>: result</span><br><span class="line">                }</span><br><span class="line">            ],</span><br><span class="line">            functions=functions</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> second_response.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> message.content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">response = chat_with_functions(<span class="string">"北京今天天气怎么样？"</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<h3 id="react推理与行动结合">ReAct：推理与行动结合</h3>
<p>ReAct（Reasoning + Acting）是 Google Research
提出的框架，它将推理和行动交织在一起，让模型能够通过迭代的"思考-行动-观察"循环完成复杂任务。与
Function Calling 的静态工具调用不同，ReAct
允许模型动态规划步骤，根据中间结果调整策略。</p>
<p><strong>ReAct 循环详解</strong>：</p>
<p>ReAct 的核心是一个迭代循环，每次迭代包含三个步骤：</p>
<ol type="1">
<li><p><strong>思考（Thought）</strong>：模型分析当前状态、任务目标、可用工具，决定下一步应该执行什么操作。思考过程是显式的，以文本形式输出，这使得推理过程可解释。</p></li>
<li><p><strong>行动（Action）</strong>：模型根据思考结果，选择要调用的工具并生成参数。行动格式通常是
<code>Action: [tool_name](parameters)</code>。</p></li>
<li><p><strong>观察（Observation）</strong>：系统执行行动并返回结果，模型观察结果并更新内部状态。观察结果会被添加到上下文中，供下一轮思考使用。</p></li>
<li><p><strong>迭代</strong>：重复上述过程，直到模型输出
<code>Final Answer</code> 或达到最大迭代次数。</p></li>
</ol>
<p><strong>为什么有效</strong>：</p>
<p>ReAct
的优势在于它将推理过程显式化，让模型能够"看到"自己的思考过程。这使得模型能够：
- 根据中间结果调整策略 - 处理需要多步推理的复杂任务 -
从错误中学习（通过观察失败的行动结果） - 提供可解释的推理路径</p>
<p><strong>实现示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> initialize_agent, Tool</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> SerpAPIWrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReActAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义工具</span></span><br><span class="line">        search = SerpAPIWrapper()</span><br><span class="line">        self.tools = [</span><br><span class="line">            Tool(</span><br><span class="line">                name=<span class="string">"Search"</span>,</span><br><span class="line">                func=search.run,</span><br><span class="line">                description=<span class="string">"用于搜索最新信息，输入应该是搜索查询"</span></span><br><span class="line">            ),</span><br><span class="line">            Tool(</span><br><span class="line">                name=<span class="string">"Calculator"</span>,</span><br><span class="line">                func=self.calculator,</span><br><span class="line">                description=<span class="string">"用于执行数学计算，输入应该是数学表达式"</span></span><br><span class="line">            )</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        self.agent = initialize_agent(</span><br><span class="line">            self.tools,</span><br><span class="line">            self.llm,</span><br><span class="line">            agent=<span class="string">"react-docstore"</span>,</span><br><span class="line">            verbose=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculator</span>(<span class="params">self, expression</span>):</span><br><span class="line">        <span class="string">"""计算器工具"""</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result = <span class="built_in">eval</span>(expression)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">str</span>(result)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"计算错误"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="string">"""执行查询"""</span></span><br><span class="line">        <span class="keyword">return</span> self.agent.run(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">agent = ReActAgent()</span><br><span class="line">result = agent.run(<span class="string">"搜索 OpenAI 的最新模型，然后计算 2024 减去 2015 的结果"</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p><strong>自定义 ReAct 实现</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomReActAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm, tools</span>):</span><br><span class="line">        self.llm = llm</span><br><span class="line">        self.tools = {tool.name: tool <span class="keyword">for</span> tool <span class="keyword">in</span> tools}</span><br><span class="line">        self.max_iterations = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="string">"""执行 ReAct 循环"""</span></span><br><span class="line">        history = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iterations):</span><br><span class="line">            <span class="comment"># 构建提示</span></span><br><span class="line">            prompt = self.build_prompt(query, history)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取模型响应</span></span><br><span class="line">            response = self.llm(prompt)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 解析响应</span></span><br><span class="line">            thought, action, action_input = self.parse_response(response)</span><br><span class="line">            </span><br><span class="line">            history.append({</span><br><span class="line">                <span class="string">"thought"</span>: thought,</span><br><span class="line">                <span class="string">"action"</span>: action,</span><br><span class="line">                <span class="string">"action_input"</span>: action_input</span><br><span class="line">            })</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 检查是否完成</span></span><br><span class="line">            <span class="keyword">if</span> action == <span class="string">"Final Answer"</span>:</span><br><span class="line">                <span class="keyword">return</span> action_input</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 执行行动</span></span><br><span class="line">            <span class="keyword">if</span> action <span class="keyword">in</span> self.tools:</span><br><span class="line">                observation = self.tools[action].run(action_input)</span><br><span class="line">                history.append({<span class="string">"observation"</span>: observation})</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                history.append({<span class="string">"observation"</span>: <span class="string">f"未知工具: <span class="subst">{action}</span>"</span>})</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">"达到最大迭代次数，未能完成任务"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_prompt</span>(<span class="params">self, query, history</span>):</span><br><span class="line">        <span class="string">"""构建提示"""</span></span><br><span class="line">        prompt = <span class="string">f"问题: <span class="subst">{query}</span>\n\n"</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> history:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"thought"</span> <span class="keyword">in</span> step:</span><br><span class="line">                prompt += <span class="string">f"思考: <span class="subst">{step[<span class="string">'thought'</span>]}</span>\n"</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"action"</span> <span class="keyword">in</span> step:</span><br><span class="line">                prompt += <span class="string">f"行动: <span class="subst">{step[<span class="string">'action'</span>]}</span>(<span class="subst">{step[<span class="string">'action_input'</span>]}</span>)\n"</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"observation"</span> <span class="keyword">in</span> step:</span><br><span class="line">                prompt += <span class="string">f"观察: <span class="subst">{step[<span class="string">'observation'</span>]}</span>\n"</span></span><br><span class="line">        </span><br><span class="line">        prompt += <span class="string">"\n可用工具: "</span> + <span class="string">", "</span>.join(self.tools.keys())</span><br><span class="line">        prompt += <span class="string">"\n请按照 '思考 -&gt; 行动 -&gt; 观察' 的格式继续。"</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> prompt</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_response</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">"""解析模型响应"""</span></span><br><span class="line">        <span class="comment"># 简化实现，实际应该更robust</span></span><br><span class="line">        lines = response.strip().split(<span class="string">"\n"</span>)</span><br><span class="line">        thought = <span class="string">""</span></span><br><span class="line">        action = <span class="string">""</span></span><br><span class="line">        action_input = <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">"思考:"</span>):</span><br><span class="line">                thought = line.replace(<span class="string">"思考:"</span>, <span class="string">""</span>).strip()</span><br><span class="line">            <span class="keyword">elif</span> line.startswith(<span class="string">"行动:"</span>):</span><br><span class="line">                parts = line.replace(<span class="string">"行动:"</span>, <span class="string">""</span>).strip().split(<span class="string">"("</span>)</span><br><span class="line">                action = parts[<span class="number">0</span>].strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(parts) &gt; <span class="number">1</span>:</span><br><span class="line">                    action_input = parts[<span class="number">1</span>].rstrip(<span class="string">")"</span>).strip()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> thought, action, action_input</span><br></pre></td></tr></table></figure>
<h2 id="代码生成与理解">代码生成与理解</h2>
<h3 id="codellama代码专用大模型">CodeLlama：代码专用大模型</h3>
<p>CodeLlama 是 Meta 基于 LLaMA 2 开发的代码生成和理解模型。</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>多语言支持</strong>：Python、C++、Java、PHP、TypeScript、C#、Bash
等</li>
<li><strong>多种变体</strong>：基础模型、Python 专用、指令微调版本</li>
<li><strong>长上下文</strong>：支持最多 100K tokens 的上下文</li>
</ul>
<p><strong>使用示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CodeLlamaGenerator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"codellama/CodeLlama-7b-Instruct-hf"</span></span>):</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">            model_name,</span><br><span class="line">            torch_dtype=torch.float16,</span><br><span class="line">            device_map=<span class="string">"auto"</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_code</span>(<span class="params">self, prompt, max_length=<span class="number">512</span>, temperature=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="string">"""生成代码"""</span></span><br><span class="line">        <span class="comment"># 构建提示格式</span></span><br><span class="line">        formatted_prompt = <span class="string">f"&lt;s&gt;[INST] <span class="subst">{prompt}</span> [/INST]"</span></span><br><span class="line">        </span><br><span class="line">        inputs = self.tokenizer(formatted_prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=max_length,</span><br><span class="line">                temperature=temperature,</span><br><span class="line">                top_p=<span class="number">0.9</span>,</span><br><span class="line">                do_sample=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        generated_text = self.tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 提取生成的代码部分</span></span><br><span class="line">        code = generated_text.split(<span class="string">"[/INST]"</span>)[-<span class="number">1</span>].strip()</span><br><span class="line">        <span class="keyword">return</span> code</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">complete_code</span>(<span class="params">self, code_context, language=<span class="string">"python"</span></span>):</span><br><span class="line">        <span class="string">"""代码补全"""</span></span><br><span class="line">        prompt = <span class="string">f"Complete the following <span class="subst">{language}</span> code:\n\n<span class="subst">{code_context}</span>"</span></span><br><span class="line">        <span class="keyword">return</span> self.generate_code(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">explain_code</span>(<span class="params">self, code</span>):</span><br><span class="line">        <span class="string">"""代码解释"""</span></span><br><span class="line">        prompt = <span class="string">f"Explain what the following code does:\n\n<span class="subst">{code}</span>"</span></span><br><span class="line">        <span class="keyword">return</span> self.generate_code(prompt, max_length=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">generator = CodeLlamaGenerator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成代码</span></span><br><span class="line">code = generator.generate_code(<span class="string">"Write a Python function to calculate fibonacci numbers"</span>)</span><br><span class="line"><span class="built_in">print</span>(code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代码补全</span></span><br><span class="line">partial_code = <span class="string">"""</span></span><br><span class="line"><span class="string">def quicksort(arr):</span></span><br><span class="line"><span class="string">    if len(arr) &lt;= 1:</span></span><br><span class="line"><span class="string">        return arr</span></span><br><span class="line"><span class="string">    pivot = arr[len(arr) // 2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">completed = generator.complete_code(partial_code)</span><br><span class="line"><span class="built_in">print</span>(completed)</span><br></pre></td></tr></table></figure>
<h3 id="starcodergithub-代码训练">StarCoder：GitHub 代码训练</h3>
<p>StarCoder 是 BigCode 项目开发的代码模型，在 GitHub 代码上训练。</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>大规模训练</strong>：在 80+ 编程语言的代码上训练</li>
<li><strong>长上下文</strong>：支持 8K tokens</li>
<li><strong>代码补全</strong>：专门优化的代码补全能力</li>
</ul>
<p><strong>使用示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StarCoderGenerator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"bigcode/starcoder"</span></span>):</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">            model_name,</span><br><span class="line">            device_map=<span class="string">"auto"</span>,</span><br><span class="line">            trust_remote_code=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">complete</span>(<span class="params">self, code, max_new_tokens=<span class="number">256</span></span>):</span><br><span class="line">        <span class="string">"""代码补全"""</span></span><br><span class="line">        inputs = self.tokenizer(code, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=max_new_tokens,</span><br><span class="line">                temperature=<span class="number">0.2</span>,</span><br><span class="line">                top_p=<span class="number">0.95</span>,</span><br><span class="line">                do_sample=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        completed_code = self.tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> completed_code</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">starcoder = StarCoderGenerator()</span><br><span class="line">code = <span class="string">"def binary_search(arr, target):"</span></span><br><span class="line">completed = starcoder.complete(code)</span><br><span class="line"><span class="built_in">print</span>(completed)</span><br></pre></td></tr></table></figure>
<h3 id="代码理解与问答">代码理解与问答</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CodeUnderstandingSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"codellama/CodeLlama-7b-Instruct-hf"</span></span>):</span><br><span class="line">        self.generator = CodeLlamaGenerator(model_name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">self, code, question</span>):</span><br><span class="line">        <span class="string">"""回答关于代码的问题"""</span></span><br><span class="line">        prompt = <span class="string">f"""Given the following code:</span></span><br><span class="line"><span class="string"><span class="subst">{code}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: <span class="subst">{question}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Please provide a detailed answer."""</span></span><br><span class="line">        <span class="keyword">return</span> self.generator.generate_code(prompt, max_length=<span class="number">256</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find_bugs</span>(<span class="params">self, code</span>):</span><br><span class="line">        <span class="string">"""查找代码中的bug"""</span></span><br><span class="line">        prompt = <span class="string">f"""Analyze the following code and identify any bugs or potential issues:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">{code}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">List all bugs found:"""</span></span><br><span class="line">        <span class="keyword">return</span> self.generator.generate_code(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">refactor_code</span>(<span class="params">self, code, instructions</span>):</span><br><span class="line">        <span class="string">"""重构代码"""</span></span><br><span class="line">        prompt = <span class="string">f"""Refactor the following code according to these instructions: <span class="subst">{instructions}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original code:</span></span><br><span class="line"><span class="string"><span class="subst">{code}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Refactored code:"""</span></span><br><span class="line">        <span class="keyword">return</span> self.generator.generate_code(prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">code_understanding = CodeUnderstandingSystem()</span><br><span class="line"></span><br><span class="line">code = <span class="string">"""</span></span><br><span class="line"><span class="string">def calculate_total(items):</span></span><br><span class="line"><span class="string">    total = 0</span></span><br><span class="line"><span class="string">    for item in items:</span></span><br><span class="line"><span class="string">        total += item.price</span></span><br><span class="line"><span class="string">    return total</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回答问题</span></span><br><span class="line">answer = code_understanding.answer_question(code, <span class="string">"What does this function do?"</span>)</span><br><span class="line"><span class="built_in">print</span>(answer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找bug</span></span><br><span class="line">bugs = code_understanding.find_bugs(code)</span><br><span class="line"><span class="built_in">print</span>(bugs)</span><br></pre></td></tr></table></figure>
<h2 id="长上下文建模">长上下文建模</h2>
<h3 id="挑战与解决方案">挑战与解决方案</h3>
<p>传统 Transformer 的注意力机制复杂度为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.832ex" height="2.452ex" role="img" focusable="false" viewbox="0 -833.9 2577.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>，其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span>
是序列长度，这限制了模型处理长上下文的能力。</p>
<p><strong>主要挑战</strong>：</p>
<ol type="1">
<li><strong>计算复杂度</strong>：注意力矩阵随序列长度平方增长</li>
<li><strong>内存占用</strong>：需要存储完整的注意力矩阵</li>
<li><strong>位置编码</strong>：需要处理超出训练时最大长度的位置</li>
</ol>
<h3 id="longlora高效长上下文微调">LongLoRA：高效长上下文微调</h3>
<p>LongLoRA 通过稀疏注意力机制实现高效的长上下文微调。</p>
<p><strong>核心思想</strong>：</p>
<ul>
<li><strong>Shifted Sparse
Attention</strong>：只计算局部和全局注意力，降低复杂度</li>
<li><strong>LoRA 微调</strong>：只微调少量参数，保持效率</li>
</ul>
<p><strong>实现示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LongLoRAModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_model_name, max_length=<span class="number">8192</span></span>):</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">            base_model_name,</span><br><span class="line">            torch_dtype=torch.float16,</span><br><span class="line">            device_map=<span class="string">"auto"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 配置 LoRA</span></span><br><span class="line">        lora_config = LoraConfig(</span><br><span class="line">            r=<span class="number">16</span>,</span><br><span class="line">            lora_alpha=<span class="number">32</span>,</span><br><span class="line">            target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],</span><br><span class="line">            lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">            bias=<span class="string">"none"</span>,</span><br><span class="line">            task_type=<span class="string">"CAUSAL_LM"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.model = get_peft_model(self.model, lora_config)</span><br><span class="line">        self.max_length = max_length</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, prompt, max_new_tokens=<span class="number">256</span></span>):</span><br><span class="line">        <span class="string">"""生成长文本"""</span></span><br><span class="line">        inputs = self.tokenizer(</span><br><span class="line">            prompt,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_length</span><br><span class="line">        ).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=max_new_tokens,</span><br><span class="line">                temperature=<span class="number">0.7</span>,</span><br><span class="line">                top_p=<span class="number">0.9</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        generated_text = self.tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> generated_text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">long_model = LongLoRAModel(<span class="string">"meta-llama/Llama-2-7b-hf"</span>, max_length=<span class="number">8192</span>)</span><br><span class="line">long_prompt = <span class="string">"..."</span>  <span class="comment"># 很长的提示</span></span><br><span class="line">result = long_model.generate(long_prompt)</span><br></pre></td></tr></table></figure>
<h3 id="longllama扩展上下文窗口">LongLLaMA：扩展上下文窗口</h3>
<p>LongLLaMA 通过 FoT（Focus on Transformer）机制扩展上下文窗口。</p>
<p><strong>FoT 机制</strong>：</p>
<ul>
<li><strong>记忆层</strong>：存储长期记忆</li>
<li><strong>注意力机制</strong>：在记忆层和当前上下文之间建立连接</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LongLLaMAModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_model, memory_size=<span class="number">4096</span></span>):</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">        self.memory_size = memory_size</span><br><span class="line">        self.memory = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_with_memory</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="string">"""带记忆的前向传播"""</span></span><br><span class="line">        <span class="comment"># 更新记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 将当前输入与记忆结合</span></span><br><span class="line">            extended_input = torch.cat([self.memory, input_ids], dim=<span class="number">1</span>)</span><br><span class="line">            extended_mask = torch.cat([</span><br><span class="line">                torch.ones_like(self.memory),</span><br><span class="line">                attention_mask</span><br><span class="line">            ], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            extended_input = input_ids</span><br><span class="line">            extended_mask = attention_mask</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = self.base_model(</span><br><span class="line">            input_ids=extended_input,</span><br><span class="line">            attention_mask=extended_mask</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新记忆（保留最后 memory_size 个token）</span></span><br><span class="line">        <span class="keyword">if</span> extended_input.size(<span class="number">1</span>) &gt; self.memory_size:</span><br><span class="line">            self.memory = extended_input[:, -self.memory_size:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.memory = extended_input</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h2 id="幻觉问题与缓解">幻觉问题与缓解</h2>
<h3 id="幻觉的定义与类型">幻觉的定义与类型</h3>
<p><strong>幻觉类型</strong>：</p>
<ol type="1">
<li><strong>事实性幻觉</strong>：生成与事实不符的内容</li>
<li><strong>逻辑性幻觉</strong>：推理过程错误</li>
<li><strong>一致性幻觉</strong>：前后矛盾</li>
</ol>
<h3 id="缓解策略">缓解策略</h3>
<p><strong>检索增强生成（RAG）</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RAGSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, documents, llm_model=<span class="string">"gpt-3.5-turbo"</span></span>):</span><br><span class="line">        <span class="comment"># 文本分割</span></span><br><span class="line">        text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">            chunk_size=<span class="number">1000</span>,</span><br><span class="line">            chunk_overlap=<span class="number">200</span></span><br><span class="line">        )</span><br><span class="line">        texts = text_splitter.split_documents(documents)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建向量存储</span></span><br><span class="line">        embeddings = OpenAIEmbeddings()</span><br><span class="line">        self.vectorstore = FAISS.from_documents(texts, embeddings)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建检索链</span></span><br><span class="line">        self.qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">            llm=OpenAI(model_name=llm_model),</span><br><span class="line">            chain_type=<span class="string">"stuff"</span>,</span><br><span class="line">            retriever=self.vectorstore.as_retriever(),</span><br><span class="line">            return_source_documents=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">self, question</span>):</span><br><span class="line">        <span class="string">"""查询并返回答案和来源"""</span></span><br><span class="line">        result = self.qa_chain({<span class="string">"query"</span>: question})</span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">"answer"</span>: result[<span class="string">"result"</span>],</span><br><span class="line">            <span class="string">"sources"</span>: result[<span class="string">"source_documents"</span>]</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">documents = [...]  <span class="comment"># 文档列表</span></span><br><span class="line">rag = RAGSystem(documents)</span><br><span class="line">result = rag.query(<span class="string">"什么是机器学习？"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"答案: <span class="subst">{result[<span class="string">'answer'</span>]}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"来源: <span class="subst">{result[<span class="string">'sources'</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>置信度评估</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConfidenceScorer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score_response</span>(<span class="params">self, question, answer, context=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""评估回答的置信度"""</span></span><br><span class="line">        <span class="comment"># 方法1: 基于概率的置信度</span></span><br><span class="line">        probs = self.get_token_probabilities(question, answer)</span><br><span class="line">        avg_prob = probs.mean()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 方法2: 基于一致性的置信度</span></span><br><span class="line">        consistency = self.check_consistency(answer, context)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 方法3: 基于事实性的置信度</span></span><br><span class="line">        factuality = self.check_factuality(answer, context)</span><br><span class="line">        </span><br><span class="line">        confidence = (avg_prob + consistency + factuality) / <span class="number">3</span></span><br><span class="line">        <span class="keyword">return</span> confidence</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_token_probabilities</span>(<span class="params">self, question, answer</span>):</span><br><span class="line">        <span class="string">"""获取token概率"""</span></span><br><span class="line">        <span class="comment"># 实现token概率计算</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_consistency</span>(<span class="params">self, answer, context</span>):</span><br><span class="line">        <span class="string">"""检查一致性"""</span></span><br><span class="line">        <span class="comment"># 实现一致性检查</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_factuality</span>(<span class="params">self, answer, context</span>):</span><br><span class="line">        <span class="string">"""检查事实性"""</span></span><br><span class="line">        <span class="comment"># 实现事实性检查</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="安全性与对齐">安全性与对齐</h2>
<h3 id="安全性挑战">安全性挑战</h3>
<p><strong>主要风险</strong>：</p>
<ol type="1">
<li><strong>有害内容生成</strong>：生成暴力、歧视性内容</li>
<li><strong>隐私泄露</strong>：泄露训练数据中的敏感信息</li>
<li><strong>误用</strong>：被用于恶意目的</li>
</ol>
<h3 id="对齐技术">对齐技术</h3>
<p><strong>RLHF（Reinforcement Learning from Human
Feedback）</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RLHFTrainer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, reward_model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.reward_model = reward_model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, prompts, responses</span>):</span><br><span class="line">        <span class="string">"""RLHF训练步骤"""</span></span><br><span class="line">        <span class="comment"># 1. 生成响应</span></span><br><span class="line">        logits = self.model(prompts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 计算奖励</span></span><br><span class="line">        rewards = self.reward_model(prompts, responses)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 计算策略梯度</span></span><br><span class="line">        loss = self.compute_policy_gradient(logits, rewards)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. 更新模型</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">return</span> loss.item()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_policy_gradient</span>(<span class="params">self, logits, rewards</span>):</span><br><span class="line">        <span class="string">"""计算策略梯度"""</span></span><br><span class="line">        <span class="comment"># PPO或其他RL算法</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>安全过滤器</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SafetyFilter</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.harmful_patterns = [</span><br><span class="line">            <span class="string">r"violence|暴力"</span>,</span><br><span class="line">            <span class="string">r"discrimination|歧视"</span>,</span><br><span class="line">            <span class="comment"># ... 更多模式</span></span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">"""过滤有害内容"""</span></span><br><span class="line">        <span class="keyword">for</span> pattern <span class="keyword">in</span> self.harmful_patterns:</span><br><span class="line">            <span class="keyword">if</span> re.search(pattern, text, re.IGNORECASE):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span>, <span class="string">"检测到有害内容"</span></span><br><span class="line">        <span class="keyword">return</span> text, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="模型评估体系">模型评估体系</h2>
<h3 id="评估维度">评估维度</h3>
<p><strong>1. 能力评估</strong>：</p>
<ul>
<li><strong>语言理解</strong>：GLUE、SuperGLUE</li>
<li><strong>语言生成</strong>：BLEU、ROUGE、METEOR</li>
<li><strong>推理能力</strong>：GSM8K、HellaSwag</li>
</ul>
<p><strong>2. 安全性评估</strong>：</p>
<ul>
<li><strong>有害内容检测</strong></li>
<li><strong>偏见检测</strong></li>
<li><strong>隐私风险评估</strong></li>
</ul>
<p><strong>3. 效率评估</strong>：</p>
<ul>
<li><strong>推理速度</strong></li>
<li><strong>内存占用</strong></li>
<li><strong>能耗</strong></li>
</ul>
<h3 id="评估框架">评估框架</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ModelEvaluator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, tokenizer</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_glue</span>(<span class="params">self, dataset</span>):</span><br><span class="line">        <span class="string">"""评估GLUE任务"""</span></span><br><span class="line">        results = {}</span><br><span class="line">        <span class="keyword">for</span> task_name, task_data <span class="keyword">in</span> dataset.items():</span><br><span class="line">            accuracy = self.evaluate_task(task_name, task_data)</span><br><span class="line">            results[task_name] = accuracy</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_generation</span>(<span class="params">self, test_set</span>):</span><br><span class="line">        <span class="string">"""评估生成质量"""</span></span><br><span class="line">        bleu_scores = []</span><br><span class="line">        rouge_scores = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> example <span class="keyword">in</span> test_set:</span><br><span class="line">            generated = self.generate(example[<span class="string">"input"</span>])</span><br><span class="line">            bleu = self.compute_bleu(example[<span class="string">"reference"</span>], generated)</span><br><span class="line">            rouge = self.compute_rouge(example[<span class="string">"reference"</span>], generated)</span><br><span class="line">            bleu_scores.append(bleu)</span><br><span class="line">            rouge_scores.append(rouge)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">"bleu"</span>: np.mean(bleu_scores),</span><br><span class="line">            <span class="string">"rouge"</span>: np.mean(rouge_scores)</span><br><span class="line">        }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_safety</span>(<span class="params">self, test_prompts</span>):</span><br><span class="line">        <span class="string">"""评估安全性"""</span></span><br><span class="line">        harmful_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> prompt <span class="keyword">in</span> test_prompts:</span><br><span class="line">            response = self.generate(prompt)</span><br><span class="line">            <span class="keyword">if</span> self.is_harmful(response):</span><br><span class="line">                harmful_count += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">"harmful_rate"</span>: harmful_count / <span class="built_in">len</span>(test_prompts),</span><br><span class="line">            <span class="string">"safe_rate"</span>: <span class="number">1</span> - harmful_count / <span class="built_in">len</span>(test_prompts)</span><br><span class="line">        }</span><br></pre></td></tr></table></figure>
<h2 id="实战完整-nlp-项目部署">实战：完整 NLP 项目部署</h2>
<h3 id="项目结构">项目结构</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">nlp-service/</span><br><span class="line">├── app/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── main.py          # FastAPI应用</span><br><span class="line">│   ├── models.py         # 模型加载</span><br><span class="line">│   ├── routers/         # API路由</span><br><span class="line">│   │   ├── chat.py</span><br><span class="line">│   │   ├── embedding.py</span><br><span class="line">│   │   └── generation.py</span><br><span class="line">│   └── utils/</span><br><span class="line">│       ├── logging.py</span><br><span class="line">│       └── monitoring.py</span><br><span class="line">├── tests/</span><br><span class="line">├── docker/</span><br><span class="line">│   └── Dockerfile</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── docker-compose.yml</span><br><span class="line">└── README.md</span><br></pre></td></tr></table></figure>
<h3 id="fastapi-应用">FastAPI 应用</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/main.py</span></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, HTTPException</span><br><span class="line"><span class="keyword">from</span> fastapi.middleware.cors <span class="keyword">import</span> CORSMiddleware</span><br><span class="line"><span class="keyword">from</span> app.models <span class="keyword">import</span> ModelManager</span><br><span class="line"><span class="keyword">from</span> app.routers <span class="keyword">import</span> chat, embedding, generation</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置日志</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span></span><br><span class="line">)</span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">app = FastAPI(title=<span class="string">"NLP Service API"</span>, version=<span class="string">"1.0.0"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CORS配置</span></span><br><span class="line">app.add_middleware(</span><br><span class="line">    CORSMiddleware,</span><br><span class="line">    allow_origins=[<span class="string">"*"</span>],</span><br><span class="line">    allow_credentials=<span class="literal">True</span>,</span><br><span class="line">    allow_methods=[<span class="string">"*"</span>],</span><br><span class="line">    allow_headers=[<span class="string">"*"</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局模型管理器</span></span><br><span class="line">model_manager = ModelManager()</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.on_event(<span class="params"><span class="string">"startup"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">startup_event</span>():</span><br><span class="line">    <span class="string">"""启动时加载模型"""</span></span><br><span class="line">    logger.info(<span class="string">"Loading models..."</span>)</span><br><span class="line">    <span class="keyword">await</span> model_manager.load_models()</span><br><span class="line">    logger.info(<span class="string">"Models loaded successfully"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.on_event(<span class="params"><span class="string">"shutdown"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">shutdown_event</span>():</span><br><span class="line">    <span class="string">"""关闭时清理资源"""</span></span><br><span class="line">    logger.info(<span class="string">"Shutting down..."</span>)</span><br><span class="line">    <span class="keyword">await</span> model_manager.cleanup()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册路由</span></span><br><span class="line">app.include_router(chat.router, prefix=<span class="string">"/api/v1"</span>, tags=[<span class="string">"chat"</span>])</span><br><span class="line">app.include_router(embedding.router, prefix=<span class="string">"/api/v1"</span>, tags=[<span class="string">"embedding"</span>])</span><br><span class="line">app.include_router(generation.router, prefix=<span class="string">"/api/v1"</span>, tags=[<span class="string">"generation"</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/health"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">health_check</span>():</span><br><span class="line">    <span class="string">"""健康检查"""</span></span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"status"</span>: <span class="string">"healthy"</span>, <span class="string">"models_loaded"</span>: model_manager.models_loaded}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app, host=<span class="string">"0.0.0.0"</span>, port=<span class="number">8000</span>)</span><br></pre></td></tr></table></figure>
<h3 id="模型管理">模型管理</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/models.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelManager</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.models = {}</span><br><span class="line">        self.tokenizers = {}</span><br><span class="line">        self.models_loaded = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">load_models</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""异步加载模型"""</span></span><br><span class="line">        <span class="comment"># 聊天模型</span></span><br><span class="line">        <span class="keyword">await</span> self._load_model_async(</span><br><span class="line">            <span class="string">"chat"</span>,</span><br><span class="line">            <span class="string">"meta-llama/Llama-2-7b-chat-hf"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 嵌入模型</span></span><br><span class="line">        <span class="keyword">await</span> self._load_model_async(</span><br><span class="line">            <span class="string">"embedding"</span>,</span><br><span class="line">            <span class="string">"sentence-transformers/all-MiniLM-L6-v2"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.models_loaded = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">_load_model_async</span>(<span class="params">self, name, model_path</span>):</span><br><span class="line">        <span class="string">"""异步加载单个模型"""</span></span><br><span class="line">        loop = asyncio.get_event_loop()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在线程池中执行CPU密集型操作</span></span><br><span class="line">        tokenizer = <span class="keyword">await</span> loop.run_in_executor(</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            AutoTokenizer.from_pretrained,</span><br><span class="line">            model_path</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        model = <span class="keyword">await</span> loop.run_in_executor(</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            AutoModelForCausalLM.from_pretrained,</span><br><span class="line">            model_path,</span><br><span class="line">            {<span class="string">"torch_dtype"</span>: torch.float16, <span class="string">"device_map"</span>: <span class="string">"auto"</span>}</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.tokenizers[name] = tokenizer</span><br><span class="line">        self.models[name] = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">cleanup</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""清理资源"""</span></span><br><span class="line">        <span class="keyword">for</span> model <span class="keyword">in</span> self.models.values():</span><br><span class="line">            <span class="keyword">del</span> model</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_model</span>(<span class="params">self, name: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="string">"""获取模型"""</span></span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> self.models:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Model <span class="subst">{name}</span> not loaded"</span>)</span><br><span class="line">        <span class="keyword">return</span> self.models[name]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_tokenizer</span>(<span class="params">self, name: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="string">"""获取tokenizer"""</span></span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> self.tokenizers:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Tokenizer <span class="subst">{name}</span> not loaded"</span>)</span><br><span class="line">        <span class="keyword">return</span> self.tokenizers[name]</span><br></pre></td></tr></table></figure>
<h3 id="api-路由">API 路由</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/routers/chat.py</span></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> APIRouter, HTTPException</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"><span class="keyword">from</span> app.models <span class="keyword">import</span> ModelManager</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">router = APIRouter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatRequest</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    message: <span class="built_in">str</span></span><br><span class="line">    max_tokens: <span class="built_in">int</span> = <span class="number">256</span></span><br><span class="line">    temperature: <span class="built_in">float</span> = <span class="number">0.7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatResponse</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    response: <span class="built_in">str</span></span><br><span class="line">    tokens_used: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@router.post(<span class="params"><span class="string">"/chat"</span>, response_model=ChatResponse</span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">request: ChatRequest, model_manager: ModelManager</span>):</span><br><span class="line">    <span class="string">"""聊天接口"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        model = model_manager.get_model(<span class="string">"chat"</span>)</span><br><span class="line">        tokenizer = model_manager.get_tokenizer(<span class="string">"chat"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码输入</span></span><br><span class="line">        inputs = tokenizer(request.message, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 生成</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=request.max_tokens,</span><br><span class="line">                temperature=request.temperature,</span><br><span class="line">                do_sample=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 解码</span></span><br><span class="line">        response_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        tokens_used = outputs.shape[<span class="number">1</span>] - inputs.input_ids.shape[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ChatResponse(</span><br><span class="line">            response=response_text,</span><br><span class="line">            tokens_used=tokens_used</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="built_in">str</span>(e))</span><br></pre></td></tr></table></figure>
<h3 id="docker-配置">Docker 配置</h3>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker/Dockerfile</span></span><br><span class="line"><span class="keyword">FROM</span> nvidia/cuda:<span class="number">11.8</span>.<span class="number">0</span>-cudnn8-runtime-ubuntu22.<span class="number">04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装Python</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get update &amp;&amp; apt-get install -y \</span></span><br><span class="line"><span class="language-bash">    python3.9 \</span></span><br><span class="line"><span class="language-bash">    python3-pip \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> requirements.txt .</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip3 install --no-cache-dir -r requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制应用代码</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> app/ ./app/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="keyword">ENV</span> PYTHONUNBUFFERED=<span class="number">1</span></span><br><span class="line"><span class="keyword">ENV</span> CUDA_VISIBLE_DEVICES=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动命令</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">"uvicorn"</span>, <span class="string">"app.main:app"</span>, <span class="string">"--host"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"--port"</span>, <span class="string">"8000"</span>, <span class="string">"--workers"</span>, <span class="string">"1"</span>]</span></span><br></pre></td></tr></table></figure>
<h3 id="docker-compose">Docker Compose</h3>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker-compose.yml</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3.8'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nlp-service:</span></span><br><span class="line">    <span class="attr">build:</span></span><br><span class="line">      <span class="attr">context:</span> <span class="string">.</span></span><br><span class="line">      <span class="attr">dockerfile:</span> <span class="string">docker/Dockerfile</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"8000:8000"</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">CUDA_VISIBLE_DEVICES=0</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">reservations:</span></span><br><span class="line">          <span class="attr">devices:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">driver:</span> <span class="string">nvidia</span></span><br><span class="line">              <span class="attr">count:</span> <span class="number">1</span></span><br><span class="line">              <span class="attr">capabilities:</span> [<span class="string">gpu</span>]</span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./models:/app/models</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br><span class="line">  </span><br><span class="line">  <span class="attr">monitoring:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">prometheus/prometheus</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"9090:9090"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml</span></span><br></pre></td></tr></table></figure>
<h3 id="监控系统">监控系统</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/utils/monitoring.py</span></span><br><span class="line"><span class="keyword">from</span> prometheus_client <span class="keyword">import</span> Counter, Histogram, Gauge</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指标定义</span></span><br><span class="line">request_count = Counter(</span><br><span class="line">    <span class="string">'nlp_requests_total'</span>,</span><br><span class="line">    <span class="string">'Total number of requests'</span>,</span><br><span class="line">    [<span class="string">'endpoint'</span>, <span class="string">'status'</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">request_duration = Histogram(</span><br><span class="line">    <span class="string">'nlp_request_duration_seconds'</span>,</span><br><span class="line">    <span class="string">'Request duration in seconds'</span>,</span><br><span class="line">    [<span class="string">'endpoint'</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model_memory = Gauge(</span><br><span class="line">    <span class="string">'nlp_model_memory_bytes'</span>,</span><br><span class="line">    <span class="string">'Model memory usage in bytes'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">track_request</span>(<span class="params">endpoint, status</span>):</span><br><span class="line">    <span class="string">"""记录请求"""</span></span><br><span class="line">    request_count.labels(endpoint=endpoint, status=status).inc()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">track_duration</span>(<span class="params">endpoint, duration</span>):</span><br><span class="line">    <span class="string">"""记录请求时长"""</span></span><br><span class="line">    request_duration.labels(endpoint=endpoint).observe(duration)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_memory_usage</span>():</span><br><span class="line">    <span class="string">"""更新内存使用"""</span></span><br><span class="line">    <span class="keyword">import</span> torch</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        memory = torch.cuda.memory_allocated()</span><br><span class="line">        model_memory.<span class="built_in">set</span>(memory)</span><br></pre></td></tr></table></figure>
<h3 id="部署脚本">部署脚本</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># deploy.sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Building Docker image..."</span></span><br><span class="line">docker build -t nlp-service:latest -f docker/Dockerfile .</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Stopping existing containers..."</span></span><br><span class="line">docker-compose down</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Starting services..."</span></span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Waiting for services to be ready..."</span></span><br><span class="line"><span class="built_in">sleep</span> 10</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Checking health..."</span></span><br><span class="line">curl http://localhost:8000/health</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Deployment complete!"</span></span><br></pre></td></tr></table></figure>
<h2 id="qa-前沿技术与实战应用常见问题">❓ Q&amp;A:
前沿技术与实战应用常见问题</h2>
<p><strong>Q1: Function Calling 和 ReAct 的区别是什么？</strong></p>
<p>A: Function Calling
是静态的工具调用机制，模型根据函数定义决定是否调用。ReAct
是动态的推理-行动循环，模型可以自主规划步骤并迭代执行。Function Calling
更适合结构化工具调用，ReAct 更适合复杂任务规划。</p>
<p><strong>Q2: CodeLlama 和 StarCoder 如何选择？</strong></p>
<p>A: CodeLlama 基于 LLaMA
2，指令跟随能力更强，适合代码生成和问答。StarCoder
在更大规模的代码上训练，代码补全能力更强。选择取决于具体需求：需要对话式代码生成选
CodeLlama，需要代码补全选 StarCoder。</p>
<p><strong>Q3: 长上下文模型的实际应用场景有哪些？</strong></p>
<p>A: 主要场景：1) 长文档问答和摘要；2) 代码库理解和生成；3)
多轮对话历史保持；4)
长文本分析。注意长上下文会增加计算成本，需要权衡。</p>
<p><strong>Q4: 如何有效缓解模型幻觉？</strong></p>
<p>A: 综合策略：1) 使用 RAG 提供外部知识；2)
实现置信度评估和不确定性量化；3) 添加事实检查步骤；4)
使用更可靠的模型；5) 人工审核关键输出。</p>
<p><strong>Q5: RLHF 训练需要多少人工标注？</strong></p>
<p>A:
通常需要数千到数万条人工反馈数据。可以使用半自动方法：先用规则或模型生成初始反馈，再人工审核和修正，提高效率。</p>
<p><strong>Q6: 生产环境部署 NLP 模型的关键考虑因素？</strong></p>
<p>A: 关键因素：1) 模型大小和推理速度；2) GPU 内存和成本；3)
并发处理能力；4) 错误处理和降级策略；5) 监控和日志；6)
安全性和访问控制。</p>
<p><strong>Q7: 如何优化 NLP 服务的性能？</strong></p>
<p>A: 优化策略：1) 模型量化（INT8/INT4）；2) 批处理请求；3) 使用 KV
缓存；4) 模型蒸馏；5) 使用更小的模型变体；6) 异步处理；7) CDN
缓存静态内容。</p>
<p><strong>Q8: Docker 部署时如何处理大模型文件？</strong></p>
<p>A: 方案：1) 使用 Docker 卷挂载模型目录；2) 使用模型缓存服务（如
HuggingFace Cache）；3) 在构建时预下载模型；4) 使用模型服务器（如
TensorRT Inference Server）。</p>
<p><strong>Q9: 如何监控 NLP 服务的健康状态？</strong></p>
<p>A: 监控指标：1) 请求量和响应时间；2) 错误率和异常；3) GPU
使用率和内存；4) 模型输出质量（采样评估）；5) 用户反馈。使用 Prometheus
+ Grafana 可视化。</p>
<p><strong>Q10: 多模型服务如何管理资源？</strong></p>
<p>A: 策略：1) 使用模型队列和优先级调度；2) 动态加载/卸载模型；3)
使用模型服务器（如 TorchServe、Triton）；4) 实现资源配额和限流；5) 使用
Kubernetes 进行资源管理和扩缩容。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>本文标题：自然语言处理（十二）—— 前沿技术与实战应用</li>
        <li>本文作者：Chen Kai</li>
        <li>创建时间：2025-03-31 00:00:00</li>
        <li>
            本文链接：https://www.chenk.top/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94-%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/Applications/">#Applications</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94-%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">自然语言处理（十一）—— 多模态大模型</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/%E7%A5%9E%E7%BB%8F%E7%AE%97%E5%AD%90%E7%90%86%E8%AE%BA%EF%BC%9A%E5%AD%A6%E4%B9%A0%E6%97%A0%E9%99%90%E7%BB%B4%E6%98%A0%E5%B0%84/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">神经算子理论：学习无限维映射</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- 由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#agent-%E4%B8%8E%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8"><span class="nav-number">1.</span> <span class="nav-text">Agent 与工具使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#function-calling%E8%AE%A9-llm-%E8%B0%83%E7%94%A8%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7"><span class="nav-number">1.1.</span> <span class="nav-text">Function Calling：让 LLM
调用外部工具</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#react%E6%8E%A8%E7%90%86%E4%B8%8E%E8%A1%8C%E5%8A%A8%E7%BB%93%E5%90%88"><span class="nav-number">1.2.</span> <span class="nav-text">ReAct：推理与行动结合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E4%B8%8E%E7%90%86%E8%A7%A3"><span class="nav-number">2.</span> <span class="nav-text">代码生成与理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#codellama%E4%BB%A3%E7%A0%81%E4%B8%93%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">CodeLlama：代码专用大模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#starcodergithub-%E4%BB%A3%E7%A0%81%E8%AE%AD%E7%BB%83"><span class="nav-number">2.2.</span> <span class="nav-text">StarCoder：GitHub 代码训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%90%86%E8%A7%A3%E4%B8%8E%E9%97%AE%E7%AD%94"><span class="nav-number">2.3.</span> <span class="nav-text">代码理解与问答</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1"><span class="nav-number">3.</span> <span class="nav-text">长上下文建模</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%91%E6%88%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">3.1.</span> <span class="nav-text">挑战与解决方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#longlora%E9%AB%98%E6%95%88%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BE%AE%E8%B0%83"><span class="nav-number">3.2.</span> <span class="nav-text">LongLoRA：高效长上下文微调</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#longllama%E6%89%A9%E5%B1%95%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3"><span class="nav-number">3.3.</span> <span class="nav-text">LongLLaMA：扩展上下文窗口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BC%93%E8%A7%A3"><span class="nav-number">4.</span> <span class="nav-text">幻觉问题与缓解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BB%E8%A7%89%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%8E%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">幻觉的定义与类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%93%E8%A7%A3%E7%AD%96%E7%95%A5"><span class="nav-number">4.2.</span> <span class="nav-text">缓解策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E5%AF%B9%E9%BD%90"><span class="nav-number">5.</span> <span class="nav-text">安全性与对齐</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E5%85%A8%E6%80%A7%E6%8C%91%E6%88%98"><span class="nav-number">5.1.</span> <span class="nav-text">安全性挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E9%BD%90%E6%8A%80%E6%9C%AF"><span class="nav-number">5.2.</span> <span class="nav-text">对齐技术</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB"><span class="nav-number">6.</span> <span class="nav-text">模型评估体系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E7%BB%B4%E5%BA%A6"><span class="nav-number">6.1.</span> <span class="nav-text">评估维度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6"><span class="nav-number">6.2.</span> <span class="nav-text">评估框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98%E5%AE%8C%E6%95%B4-nlp-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2"><span class="nav-number">7.</span> <span class="nav-text">实战：完整 NLP 项目部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84"><span class="nav-number">7.1.</span> <span class="nav-text">项目结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fastapi-%E5%BA%94%E7%94%A8"><span class="nav-number">7.2.</span> <span class="nav-text">FastAPI 应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86"><span class="nav-number">7.3.</span> <span class="nav-text">模型管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#api-%E8%B7%AF%E7%94%B1"><span class="nav-number">7.4.</span> <span class="nav-text">API 路由</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#docker-%E9%85%8D%E7%BD%AE"><span class="nav-number">7.5.</span> <span class="nav-text">Docker 配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#docker-compose"><span class="nav-number">7.6.</span> <span class="nav-text">Docker Compose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F"><span class="nav-number">7.7.</span> <span class="nav-text">监控系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC"><span class="nav-number">7.8.</span> <span class="nav-text">部署脚本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qa-%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">8.</span> <span class="nav-text">❓ Q&amp;A:
前沿技术与实战应用常见问题</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
