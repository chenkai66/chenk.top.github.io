<!DOCTYPE html>



<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            自然语言处理（十）—— RAG与知识增强系统 |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"zh-CN","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">自然语言处理（十）—— RAG与知识增强系统</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-02-28 00:00:00</span>
        <span class="mobile">2025-02-28 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/RAG/">RAG</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>5k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>21 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>随着大语言模型的广泛应用，一个核心问题逐渐凸显：如何让模型访问和利用外部知识？传统的预训练模型虽然拥有海量参数和强大的语言理解能力，但其知识被"冻结"在训练时的数据中，无法获取最新信息，也无法访问私有知识库。检索增强生成（Retrieval-Augmented
Generation, RAG）技术应运而生，通过将信息检索与生成模型相结合，让 LLM
能够动态地访问外部知识，从而生成更准确、更相关的回答。</p>
<p>RAG 系统的核心包括向量数据库的选择、Embedding
模型的优化、检索策略的设计、重排序技术的应用，以及查询重写与扩展等关键技术。一个优秀的
RAG
系统不仅需要高效的检索机制，还需要精心设计的查询优化和结果融合策略。本文将深入探讨
RAG
系统的各个组件，从基础架构到高级优化技术，并通过实战案例展示如何构建企业级
RAG 系统。</p>
<span id="more"></span>
<h2 id="rag-基础与架构">RAG 基础与架构</h2>
<h3 id="rag-核心思想">RAG 核心思想</h3>
<p>RAG
的基本思想是：在生成答案之前，先从外部知识库中检索相关信息，然后将检索到的信息与用户查询一起输入到生成模型中。</p>
<p><strong>数学表示</strong>：</p>
<p>对于查询 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span>，RAG
系统的输出为：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.779ex;" xmlns="http://www.w3.org/2000/svg" width="29.01ex" height="4.928ex" role="img" focusable="false" viewbox="0 -950 12822.3 2178.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1630,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1908,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(2368,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3034.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="munder" transform="translate(4090.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(66.2,-1100) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(465,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/></g><g data-mml-node="mi" transform="translate(1132,0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"/></g></g></g><g data-mml-node="mi" transform="translate(5701.2,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(6452.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(6841.2,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(7306.2,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(7584.2,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(8044.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(8655.4,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="mi" transform="translate(9155.7,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(9906.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(10295.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(10785.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(11063.7,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(11528.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(11973.3,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(12433.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p>
<p>其中： - <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.636ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 723 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"/></g></g></g></svg></mjx-container></span> 是检索到的文档集合 -
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.181ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2732 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(1605,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(2343,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> 是检索概率（文档 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g></g></g></svg></mjx-container></span> 与查询 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 的相关性） - <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.296ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 3666.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1630,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1908,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(2373,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2817.7,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3277.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> 是生成概率（基于检索文档
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g></g></g></svg></mjx-container></span> 和查询 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 生成答案 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewbox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span>）</p>
<h3 id="rag-架构流程">RAG 架构流程</h3>
<p>典型的 RAG 系统包含以下步骤：</p>
<ol type="1">
<li><strong>文档处理</strong>：将原始文档切分、向量化并存储到向量数据库</li>
<li><strong>查询处理</strong>：将用户查询转换为向量表示</li>
<li><strong>检索</strong>：在向量数据库中检索相关文档</li>
<li><strong>重排序</strong>：对检索结果进行精排</li>
<li><strong>生成</strong>：将检索到的文档和查询输入生成模型</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载文档</span></span><br><span class="line">loader = TextLoader(<span class="string">"documents.txt"</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 文档切分</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span></span><br><span class="line">)</span><br><span class="line">chunks = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建向量数据库</span></span><br><span class="line">embeddings = HuggingFaceEmbeddings(</span><br><span class="line">    model_name=<span class="string">"sentence-transformers/all-MiniLM-L6-v2"</span></span><br><span class="line">)</span><br><span class="line">vectorstore = FAISS.from_documents(chunks, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 创建检索器</span></span><br><span class="line">retriever = vectorstore.as_retriever(search_kwargs={<span class="string">"k"</span>: <span class="number">5</span>})</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 创建 RAG 链</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">"stuff"</span>,</span><br><span class="line">    retriever=retriever,</span><br><span class="line">    return_source_documents=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 查询</span></span><br><span class="line">query = <span class="string">"什么是机器学习？"</span></span><br><span class="line">result = qa_chain({<span class="string">"query"</span>: query})</span><br><span class="line"><span class="built_in">print</span>(result[<span class="string">"result"</span>])</span><br><span class="line"><span class="built_in">print</span>(result[<span class="string">"source_documents"</span>])</span><br></pre></td></tr></table></figure>
<h3 id="rag-vs-微调">RAG vs 微调</h3>
<table>
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 33%">
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>优势</th>
<th>劣势</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>RAG</td>
<td>可动态更新知识、成本低、可解释性强</td>
<td>依赖检索质量、可能有幻觉</td>
<td>知识库频繁更新、多领域知识</td>
</tr>
<tr>
<td>微调</td>
<td>模型完全适应任务、性能可能更好</td>
<td>成本高、难以更新、可能遗忘</td>
<td>特定任务、知识相对稳定</td>
</tr>
</tbody>
</table>
<h2 id="向量数据库选择">向量数据库选择</h2>
<p>向量数据库是 RAG
系统的核心组件，负责存储和检索文档向量。不同的向量数据库有不同的特点和适用场景。</p>
<h3 id="faissfacebook-ai-similarity-search">FAISS（Facebook AI
Similarity Search）</h3>
<p>FAISS 是 Facebook 开源的向量相似度搜索库，支持 CPU 和 GPU 加速。</p>
<p><strong>特点</strong>： - 高性能：支持多种索引算法（IVF、HNSW、LSH）
- 内存高效：支持内存映射和量化 - 易于集成：Python API 简单易用</p>
<p><strong>适用场景</strong>： - 中小规模数据集（百万级向量） -
需要快速原型开发 - 本地部署</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line">dimension = <span class="number">768</span>  <span class="comment"># 向量维度</span></span><br><span class="line">index = faiss.IndexFlatL2(dimension)  <span class="comment"># L2 距离</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加向量</span></span><br><span class="line">vectors = np.random.random((<span class="number">10000</span>, dimension)).astype(<span class="string">'float32'</span>)</span><br><span class="line">index.add(vectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索</span></span><br><span class="line">query_vector = np.random.random((<span class="number">1</span>, dimension)).astype(<span class="string">'float32'</span>)</span><br><span class="line">k = <span class="number">5</span>  <span class="comment"># 返回 top-k</span></span><br><span class="line">distances, indices = index.search(query_vector, k)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Top <span class="subst">{k}</span> similar vectors:"</span>)</span><br><span class="line"><span class="keyword">for</span> i, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(indices[<span class="number">0</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Rank <span class="subst">{i+<span class="number">1</span>}</span>: Index <span class="subst">{idx}</span>, Distance <span class="subst">{distances[<span class="number">0</span>][i]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 IVF 索引（更快，但需要训练）</span></span><br><span class="line">nlist = <span class="number">100</span>  <span class="comment"># 聚类中心数</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(dimension)</span><br><span class="line">index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)</span><br><span class="line">index_ivf.train(vectors)  <span class="comment"># 训练索引</span></span><br><span class="line">index_ivf.add(vectors)</span><br><span class="line">index_ivf.nprobe = <span class="number">10</span>  <span class="comment"># 搜索时检查的聚类数</span></span><br></pre></td></tr></table></figure>
<h3 id="milvus">Milvus</h3>
<p>Milvus 是云原生的向量数据库，支持分布式部署和水平扩展。</p>
<p><strong>特点</strong>： - 分布式架构：支持集群部署 -
高可用性：支持数据复制和故障恢复 -
丰富的功能：支持标量过滤、时间序列等</p>
<p><strong>适用场景</strong>： - 大规模数据集（千万级以上） -
生产环境部署 - 需要高可用性和可扩展性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymilvus <span class="keyword">import</span> connections, Collection, FieldSchema, CollectionSchema, DataType</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接 Milvus</span></span><br><span class="line">connections.connect(</span><br><span class="line">    alias=<span class="string">"default"</span>,</span><br><span class="line">    host=<span class="string">"localhost"</span>,</span><br><span class="line">    port=<span class="string">"19530"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 schema</span></span><br><span class="line">fields = [</span><br><span class="line">    FieldSchema(name=<span class="string">"id"</span>, dtype=DataType.INT64, is_primary=<span class="literal">True</span>, auto_id=<span class="literal">True</span>),</span><br><span class="line">    FieldSchema(name=<span class="string">"text"</span>, dtype=DataType.VARCHAR, max_length=<span class="number">1000</span>),</span><br><span class="line">    FieldSchema(name=<span class="string">"embedding"</span>, dtype=DataType.FLOAT_VECTOR, dim=<span class="number">768</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = CollectionSchema(fields, <span class="string">"RAG collection"</span>)</span><br><span class="line">collection = Collection(<span class="string">"rag_collection"</span>, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line">index_params = {</span><br><span class="line">    <span class="string">"metric_type"</span>: <span class="string">"L2"</span>,</span><br><span class="line">    <span class="string">"index_type"</span>: <span class="string">"IVF_FLAT"</span>,</span><br><span class="line">    <span class="string">"params"</span>: {<span class="string">"nlist"</span>: <span class="number">1024</span>}</span><br><span class="line">}</span><br><span class="line">collection.create_index(<span class="string">"embedding"</span>, index_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line">data = [</span><br><span class="line">    [<span class="string">"Document 1"</span>, <span class="string">"Document 2"</span>, <span class="string">"Document 3"</span>],</span><br><span class="line">    [[<span class="number">0.1</span>] * <span class="number">768</span>, [<span class="number">0.2</span>] * <span class="number">768</span>, [<span class="number">0.3</span>] * <span class="number">768</span>]  <span class="comment"># 示例向量</span></span><br><span class="line">]</span><br><span class="line">collection.insert(data)</span><br><span class="line">collection.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索</span></span><br><span class="line">search_params = {<span class="string">"metric_type"</span>: <span class="string">"L2"</span>, <span class="string">"params"</span>: {<span class="string">"nprobe"</span>: <span class="number">10</span>}}</span><br><span class="line">results = collection.search(</span><br><span class="line">    data=[[<span class="number">0.15</span>] * <span class="number">768</span>],  <span class="comment"># 查询向量</span></span><br><span class="line">    anns_field=<span class="string">"embedding"</span>,</span><br><span class="line">    param=search_params,</span><br><span class="line">    limit=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="pinecone">Pinecone</h3>
<p>Pinecone 是托管的向量数据库服务，无需管理基础设施。</p>
<p><strong>特点</strong>： - 完全托管：无需管理服务器 -
自动扩展：根据负载自动调整 - 简单易用：RESTful API</p>
<p><strong>适用场景</strong>： - 快速上线 - 中小规模应用 -
不想管理基础设施</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pinecone</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">pinecone.init(api_key=<span class="string">"your-api-key"</span>, environment=<span class="string">"us-west1-gcp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line">index_name = <span class="string">"rag-index"</span></span><br><span class="line"><span class="keyword">if</span> index_name <span class="keyword">not</span> <span class="keyword">in</span> pinecone.list_indexes():</span><br><span class="line">    pinecone.create_index(</span><br><span class="line">        index_name,</span><br><span class="line">        dimension=<span class="number">768</span>,</span><br><span class="line">        metric=<span class="string">"cosine"</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接索引</span></span><br><span class="line">index = pinecone.Index(index_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入向量</span></span><br><span class="line">vectors = [</span><br><span class="line">    (<span class="string">"vec1"</span>, [<span class="number">0.1</span>] * <span class="number">768</span>, {<span class="string">"text"</span>: <span class="string">"Document 1"</span>}),</span><br><span class="line">    (<span class="string">"vec2"</span>, [<span class="number">0.2</span>] * <span class="number">768</span>, {<span class="string">"text"</span>: <span class="string">"Document 2"</span>})</span><br><span class="line">]</span><br><span class="line">index.upsert(vectors=vectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索</span></span><br><span class="line">query_vector = [<span class="number">0.15</span>] * <span class="number">768</span></span><br><span class="line">results = index.query(</span><br><span class="line">    vector=query_vector,</span><br><span class="line">    top_k=<span class="number">5</span>,</span><br><span class="line">    include_metadata=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="chroma">Chroma</h3>
<p>Chroma 是轻量级的向量数据库，专注于易用性和开发体验。</p>
<p><strong>特点</strong>： - 轻量级：资源占用小 - 易用性：API 设计简洁 -
灵活性：支持多种部署方式</p>
<p><strong>适用场景</strong>： - 开发测试 - 小规模应用 - 快速原型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="keyword">from</span> chromadb.config <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建客户端</span></span><br><span class="line">client = chromadb.Client(Settings(</span><br><span class="line">    chroma_db_impl=<span class="string">"duckdb+parquet"</span>,</span><br><span class="line">    persist_directory=<span class="string">"./chroma_db"</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建集合</span></span><br><span class="line">collection = client.create_collection(</span><br><span class="line">    name=<span class="string">"rag_collection"</span>,</span><br><span class="line">    metadata={<span class="string">"hnsw:space"</span>: <span class="string">"cosine"</span>}</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加文档</span></span><br><span class="line">collection.add(</span><br><span class="line">    documents=[<span class="string">"Document 1"</span>, <span class="string">"Document 2"</span>, <span class="string">"Document 3"</span>],</span><br><span class="line">    embeddings=[[<span class="number">0.1</span>] * <span class="number">768</span>, [<span class="number">0.2</span>] * <span class="number">768</span>, [<span class="number">0.3</span>] * <span class="number">768</span>],</span><br><span class="line">    ids=[<span class="string">"id1"</span>, <span class="string">"id2"</span>, <span class="string">"id3"</span>],</span><br><span class="line">    metadatas=[{<span class="string">"source"</span>: <span class="string">"doc1"</span>}, {<span class="string">"source"</span>: <span class="string">"doc2"</span>}, {<span class="string">"source"</span>: <span class="string">"doc3"</span>}]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">results = collection.query(</span><br><span class="line">    query_embeddings=[[<span class="number">0.15</span>] * <span class="number">768</span>],</span><br><span class="line">    n_results=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="向量数据库对比">向量数据库对比</h3>
<table>
<thead>
<tr>
<th>数据库</th>
<th>规模</th>
<th>部署方式</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>FAISS</td>
<td>百万级</td>
<td>本地</td>
<td>高性能、易用</td>
<td>开发、中小规模</td>
</tr>
<tr>
<td>Milvus</td>
<td>千万级+</td>
<td>分布式</td>
<td>可扩展、高可用</td>
<td>生产、大规模</td>
</tr>
<tr>
<td>Pinecone</td>
<td>百万级</td>
<td>托管</td>
<td>简单、无需运维</td>
<td>快速上线</td>
</tr>
<tr>
<td>Chroma</td>
<td>十万级</td>
<td>本地/云</td>
<td>轻量、易用</td>
<td>开发、小规模</td>
</tr>
</tbody>
</table>
<h2 id="embedding-模型对比">Embedding 模型对比</h2>
<p>Embedding
模型的质量直接影响检索效果。不同的模型有不同的特点和适用场景。</p>
<h3 id="通用-embedding-模型">通用 Embedding 模型</h3>
<p><strong>OpenAI text-embedding-ada-002</strong>： - 维度：1536 -
优势：性能优秀、多语言支持 - 劣势：需要 API 调用、有成本</p>
<p><strong>sentence-transformers</strong>： - 开源模型集合 -
优势：免费、可本地部署、性能好 - 常用模型： -
<code>all-MiniLM-L6-v2</code>：快速、轻量 -
<code>all-mpnet-base-v2</code>：性能更好 -
<code>multi-qa-mpnet-base</code>：针对问答优化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model = SentenceTransformer(<span class="string">'all-mpnet-base-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码文本</span></span><br><span class="line">texts = [</span><br><span class="line">    <span class="string">"机器学习是人工智能的一个分支"</span>,</span><br><span class="line">    <span class="string">"深度学习使用神经网络"</span>,</span><br><span class="line">    <span class="string">"自然语言处理处理文本数据"</span></span><br><span class="line">]</span><br><span class="line">embeddings = model.encode(texts)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Embedding shape: <span class="subst">{embeddings.shape}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Embedding dimension: <span class="subst">{embeddings.shape[<span class="number">1</span>]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相似度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">similarity_matrix = cosine_similarity(embeddings)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Similarity matrix:"</span>)</span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></table></figure>
<h3 id="领域特定-embedding">领域特定 Embedding</h3>
<p>对于特定领域，可以使用领域数据微调 Embedding 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, InputExample, losses</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载基础模型</span></span><br><span class="line">model = SentenceTransformer(<span class="string">'all-mpnet-base-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备训练数据</span></span><br><span class="line">train_examples = [</span><br><span class="line">    InputExample(texts=[<span class="string">"机器学习"</span>, <span class="string">"ML"</span>]),</span><br><span class="line">    InputExample(texts=[<span class="string">"深度学习"</span>, <span class="string">"DL"</span>]),</span><br><span class="line">    InputExample(texts=[<span class="string">"自然语言处理"</span>, <span class="string">"NLP"</span>])</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数（对比学习）</span></span><br><span class="line">train_dataloader = DataLoader(train_examples, shuffle=<span class="literal">True</span>, batch_size=<span class="number">16</span>)</span><br><span class="line">train_loss = losses.CosineSimilarityLoss(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 微调</span></span><br><span class="line">model.fit(</span><br><span class="line">    train_objectives=[(train_dataloader, train_loss)],</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    warmup_steps=<span class="number">100</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">'./domain-specific-embedding'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="embedding-模型选择指南">Embedding 模型选择指南</h3>
<table>
<thead>
<tr>
<th>模型类型</th>
<th>维度</th>
<th>速度</th>
<th>精度</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>text-embedding-ada-002</td>
<td>1536</td>
<td>中</td>
<td>高</td>
<td>生产环境、多语言</td>
</tr>
<tr>
<td>all-MiniLM-L6-v2</td>
<td>384</td>
<td>快</td>
<td>中</td>
<td>快速原型、资源受限</td>
</tr>
<tr>
<td>all-mpnet-base-v2</td>
<td>768</td>
<td>中</td>
<td>高</td>
<td>平衡性能和速度</td>
</tr>
<tr>
<td>multi-qa-mpnet-base</td>
<td>768</td>
<td>中</td>
<td>高</td>
<td>问答任务</td>
</tr>
</tbody>
</table>
<h2 id="检索策略优化">检索策略优化</h2>
<p>检索策略决定了如何从向量数据库中检索相关文档。不同的策略适用于不同的场景。</p>
<h3 id="dense-retrieval密集检索">Dense Retrieval（密集检索）</h3>
<p>Dense Retrieval 使用向量相似度进行检索，是最常用的方法。</p>
<p><strong>优势</strong>： - 语义理解能力强 - 能够处理同义词和语义相似 -
实现简单</p>
<p><strong>劣势</strong>： - 对关键词匹配不敏感 -
可能遗漏精确匹配的文档</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dense_retrieval</span>(<span class="params">query_embedding, vectorstore, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""密集检索"""</span></span><br><span class="line">    <span class="comment"># 计算相似度</span></span><br><span class="line">    similarities = vectorstore.similarity_search_with_score(</span><br><span class="line">        query_embedding,</span><br><span class="line">        k=top_k</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> similarities</span><br></pre></td></tr></table></figure>
<h3 id="sparse-retrieval稀疏检索">Sparse Retrieval（稀疏检索）</h3>
<p>Sparse Retrieval 使用关键词匹配（如 BM25）进行检索。</p>
<p><strong>优势</strong>： - 关键词匹配精确 - 对精确术语敏感 - 不需要
Embedding 模型</p>
<p><strong>劣势</strong>： - 无法理解语义 - 对同义词不敏感</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> rank_bm25 <span class="keyword">import</span> BM25Okapi</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sparse_retrieval</span>(<span class="params">query, documents, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""稀疏检索（BM25）"""</span></span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    tokenized_docs = [jieba.lcut(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> documents]</span><br><span class="line">    tokenized_query = jieba.lcut(query)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建 BM25 索引</span></span><br><span class="line">    bm25 = BM25Okapi(tokenized_docs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检索</span></span><br><span class="line">    scores = bm25.get_scores(tokenized_query)</span><br><span class="line">    top_indices = np.argsort(scores)[::-<span class="number">1</span>][:top_k]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [(documents[i], scores[i]) <span class="keyword">for</span> i <span class="keyword">in</span> top_indices]</span><br></pre></td></tr></table></figure>
<h3 id="hybrid-retrieval混合检索">Hybrid Retrieval（混合检索）</h3>
<p>Hybrid Retrieval 结合 Dense 和 Sparse 检索，取长补短。</p>
<p><strong>策略</strong>： 1. <strong>RRF（Reciprocal Rank
Fusion）</strong>：合并两种检索结果的排名 2.
<strong>加权融合</strong>：对两种检索结果加权求和 3.
<strong>重排序融合</strong>：先用两种方法检索，再用重排序模型融合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hybrid_retrieval</span>(<span class="params">query, query_embedding, vectorstore, documents, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""混合检索"""</span></span><br><span class="line">    <span class="comment"># Dense 检索</span></span><br><span class="line">    dense_results = dense_retrieval(query_embedding, vectorstore, top_k=top_k*<span class="number">2</span>)</span><br><span class="line">    dense_scores = {doc: score <span class="keyword">for</span> doc, score <span class="keyword">in</span> dense_results}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sparse 检索</span></span><br><span class="line">    sparse_results = sparse_retrieval(query, documents, top_k=top_k*<span class="number">2</span>)</span><br><span class="line">    sparse_scores = {doc: score <span class="keyword">for</span> doc, score <span class="keyword">in</span> sparse_results}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># RRF 融合</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rrf_score</span>(<span class="params">doc, rank, k=<span class="number">60</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (k + rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算 RRF 分数</span></span><br><span class="line">    combined_scores = {}</span><br><span class="line">    <span class="keyword">for</span> rank, (doc, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dense_results, <span class="number">1</span>):</span><br><span class="line">        combined_scores[doc] = combined_scores.get(doc, <span class="number">0</span>) + rrf_score(doc, rank)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> rank, (doc, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(sparse_results, <span class="number">1</span>):</span><br><span class="line">        combined_scores[doc] = combined_scores.get(doc, <span class="number">0</span>) + rrf_score(doc, rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 排序并返回 top-k</span></span><br><span class="line">    sorted_docs = <span class="built_in">sorted</span>(combined_scores.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_docs[:top_k]</span><br></pre></td></tr></table></figure>
<h2 id="reranking-技术">Reranking 技术</h2>
<p>Reranking 对初始检索结果进行精排，提高最终结果的质量。</p>
<h3 id="cross-encoder-reranking">Cross-Encoder Reranking</h3>
<p>Cross-Encoder 将查询和文档一起输入模型，计算相关性分数。</p>
<p><strong>优势</strong>： - 精度高 - 能够理解查询-文档交互</p>
<p><strong>劣势</strong>： - 计算成本高（不能预先计算） - 速度慢</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> CrossEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 Cross-Encoder 模型</span></span><br><span class="line">reranker = CrossEncoder(<span class="string">'cross-encoder/ms-marco-MiniLM-L-6-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rerank</span>(<span class="params">query, documents, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""重排序"""</span></span><br><span class="line">    <span class="comment"># 构建查询-文档对</span></span><br><span class="line">    pairs = [[query, doc] <span class="keyword">for</span> doc <span class="keyword">in</span> documents]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算相关性分数</span></span><br><span class="line">    scores = reranker.predict(pairs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 排序</span></span><br><span class="line">    ranked_indices = np.argsort(scores)[::-<span class="number">1</span>]</span><br><span class="line">    ranked_docs = [documents[i] <span class="keyword">for</span> i <span class="keyword">in</span> ranked_indices[:top_k]]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ranked_docs</span><br></pre></td></tr></table></figure>
<h3 id="多阶段检索">多阶段检索</h3>
<p>多阶段检索结合快速检索和精确重排序：</p>
<ol type="1">
<li><strong>第一阶段</strong>：使用快速方法（Dense/Sparse）检索大量候选（如
100 个）</li>
<li><strong>第二阶段</strong>：使用重排序模型对候选进行精排（如
top-5）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multi_stage_retrieval</span>(<span class="params">query, query_embedding, vectorstore, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""多阶段检索"""</span></span><br><span class="line">    <span class="comment"># 第一阶段：快速检索大量候选</span></span><br><span class="line">    candidates = dense_retrieval(query_embedding, vectorstore, top_k=<span class="number">100</span>)</span><br><span class="line">    candidate_docs = [doc <span class="keyword">for</span> doc, _ <span class="keyword">in</span> candidates]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二阶段：重排序</span></span><br><span class="line">    reranked_docs = rerank(query, candidate_docs, top_k=top_k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> reranked_docs</span><br></pre></td></tr></table></figure>
<h2 id="query-重写与扩展">Query 重写与扩展</h2>
<p>查询优化可以提高检索效果，包括查询重写、查询扩展和查询分解。</p>
<h3 id="query-rewriting查询重写">Query Rewriting（查询重写）</h3>
<p>查询重写将用户查询转换为更适合检索的形式。</p>
<p><strong>方法</strong>： 1. <strong>同义词扩展</strong>：添加同义词 2.
<strong>查询补全</strong>：补全不完整的查询 3.
<strong>查询简化</strong>：去除冗余词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rewrite_query</span>(<span class="params">original_query, llm</span>):</span><br><span class="line">    <span class="string">"""查询重写"""</span></span><br><span class="line">    prompt = PromptTemplate(</span><br><span class="line">        input_variables=[<span class="string">"query"</span>],</span><br><span class="line">        template=<span class="string">"""</span></span><br><span class="line"><span class="string">        将以下查询重写为更适合信息检索的形式，保持原意但使用更精确的术语：</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        原始查询：{query}</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        重写后的查询：</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    rewritten = llm(prompt.<span class="built_in">format</span>(query=original_query))</span><br><span class="line">    <span class="keyword">return</span> rewritten.strip()</span><br></pre></td></tr></table></figure>
<h3 id="query-expansion查询扩展">Query Expansion（查询扩展）</h3>
<p>查询扩展添加相关术语和概念。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">expand_query</span>(<span class="params">query, llm</span>):</span><br><span class="line">    <span class="string">"""查询扩展"""</span></span><br><span class="line">    prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">    为以下查询生成相关的关键词和同义词，用于信息检索：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    查询：<span class="subst">{query}</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    相关关键词（用逗号分隔）：</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    expanded_terms = llm(prompt).strip().split(<span class="string">','</span>)</span><br><span class="line">    expanded_query = query + <span class="string">" "</span> + <span class="string">" "</span>.join(expanded_terms)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> expanded_query</span><br></pre></td></tr></table></figure>
<h3 id="query-decomposition查询分解">Query
Decomposition（查询分解）</h3>
<p>对于复杂查询，可以分解为多个子查询。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decompose_query</span>(<span class="params">query, llm</span>):</span><br><span class="line">    <span class="string">"""查询分解"""</span></span><br><span class="line">    prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">    将以下复杂查询分解为多个简单的子查询：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    复杂查询：<span class="subst">{query}</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    子查询（每行一个）：</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    subqueries = llm(prompt).strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [q.strip() <span class="keyword">for</span> q <span class="keyword">in</span> subqueries <span class="keyword">if</span> q.strip()]</span><br></pre></td></tr></table></figure>
<h2 id="实战构建企业级-rag-系统">实战：构建企业级 RAG 系统</h2>
<h3 id="使用-langchain-构建-rag">使用 LangChain 构建 RAG</h3>
<p>LangChain 提供了完整的 RAG 工具链。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> DirectoryLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载文档</span></span><br><span class="line">loader = DirectoryLoader(<span class="string">"./documents"</span>, glob=<span class="string">"*.txt"</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 文档切分</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span></span><br><span class="line">)</span><br><span class="line">chunks = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建 Embedding</span></span><br><span class="line">embeddings = HuggingFaceEmbeddings(</span><br><span class="line">    model_name=<span class="string">"sentence-transformers/all-mpnet-base-v2"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 创建向量数据库</span></span><br><span class="line">vectorstore = Chroma.from_documents(</span><br><span class="line">    documents=chunks,</span><br><span class="line">    embedding=embeddings,</span><br><span class="line">    persist_directory=<span class="string">"./chroma_db"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 自定义 Prompt</span></span><br><span class="line">prompt_template = <span class="string">"""使用以下上下文回答问题。如果你不知道答案，就说不知道，不要编造答案。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">上下文：{context}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问题：{question}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">答案："""</span></span><br><span class="line"></span><br><span class="line">PROMPT = PromptTemplate(</span><br><span class="line">    template=prompt_template,</span><br><span class="line">    input_variables=[<span class="string">"context"</span>, <span class="string">"question"</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 创建 RAG 链</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">"stuff"</span>,</span><br><span class="line">    retriever=vectorstore.as_retriever(search_kwargs={<span class="string">"k"</span>: <span class="number">5</span>}),</span><br><span class="line">    chain_type_kwargs={<span class="string">"prompt"</span>: PROMPT},</span><br><span class="line">    return_source_documents=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 查询</span></span><br><span class="line">query = <span class="string">"什么是 RAG？"</span></span><br><span class="line">result = qa_chain({<span class="string">"query"</span>: query})</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"答案：<span class="subst">{result[<span class="string">'result'</span>]}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"来源文档：<span class="subst">{result[<span class="string">'source_documents'</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="使用-llamaindex-构建-rag">使用 LlamaIndex 构建 RAG</h3>
<p>LlamaIndex 专注于 LLM 应用的数据层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader, ServiceContext</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings <span class="keyword">import</span> HuggingFaceEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载文档</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">"./documents"</span>).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建服务上下文</span></span><br><span class="line">embed_model = HuggingFaceEmbedding(</span><br><span class="line">    model_name=<span class="string">"sentence-transformers/all-mpnet-base-v2"</span></span><br><span class="line">)</span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">service_context = ServiceContext.from_defaults(</span><br><span class="line">    llm=llm,</span><br><span class="line">    embed_model=embed_model,</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建索引</span></span><br><span class="line">index = VectorStoreIndex.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 创建查询引擎</span></span><br><span class="line">query_engine = index.as_query_engine(</span><br><span class="line">    similarity_top_k=<span class="number">5</span>,</span><br><span class="line">    response_mode=<span class="string">"compact"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 查询</span></span><br><span class="line">response = query_engine.query(<span class="string">"什么是 RAG？"</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="built_in">print</span>(response.source_nodes)</span><br></pre></td></tr></table></figure>
<h3 id="高级-rag-模式">高级 RAG 模式</h3>
<p><strong>Parent-Child Retrieval</strong>： -
存储时：将文档切分为小块（子块） -
检索时：检索子块，但返回父块（包含更多上下文）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建父子文档分割器</span></span><br><span class="line">parent_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">2000</span>)</span><br><span class="line">child_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割文档</span></span><br><span class="line">parent_docs = parent_splitter.split_documents(documents)</span><br><span class="line">child_docs = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> parent_docs:</span><br><span class="line">    children = child_splitter.split_documents([parent])</span><br><span class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> children:</span><br><span class="line">        child.metadata[<span class="string">"parent_id"</span>] = parent.metadata.get(<span class="string">"id"</span>)</span><br><span class="line">    child_docs.extend(children)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储子块（用于检索）</span></span><br><span class="line">vectorstore = Chroma.from_documents(child_docs, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索时返回父块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retrieve_with_parent</span>(<span class="params">query, vectorstore</span>):</span><br><span class="line">    <span class="comment"># 检索子块</span></span><br><span class="line">    child_results = vectorstore.similarity_search(query, k=<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取父块</span></span><br><span class="line">    parent_ids = <span class="built_in">set</span>([r.metadata[<span class="string">"parent_id"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> child_results])</span><br><span class="line">    parent_docs = [doc <span class="keyword">for</span> doc <span class="keyword">in</span> parent_docs <span class="keyword">if</span> doc.metadata.get(<span class="string">"id"</span>) <span class="keyword">in</span> parent_ids]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parent_docs</span><br></pre></td></tr></table></figure>
<p><strong>Self-RAG</strong>： - 使用 LLM 判断是否需要检索 -
对检索结果进行批判性评估 - 根据评估结果决定是否使用检索信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_rag</span>(<span class="params">query, llm, vectorstore</span>):</span><br><span class="line">    <span class="string">"""Self-RAG 实现"""</span></span><br><span class="line">    <span class="comment"># 1. 判断是否需要检索</span></span><br><span class="line">    need_retrieval_prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">    判断以下查询是否需要检索外部知识库：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    查询：<span class="subst">{query}</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    如果需要检索，回答"是"，否则回答"否"。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    need_retrieval = llm(need_retrieval_prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="string">"是"</span> <span class="keyword">in</span> need_retrieval <span class="keyword">or</span> <span class="string">"yes"</span> <span class="keyword">in</span> need_retrieval.lower():</span><br><span class="line">        <span class="comment"># 2. 检索</span></span><br><span class="line">        docs = vectorstore.similarity_search(query, k=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 评估检索结果</span></span><br><span class="line">        evaluation_prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">        评估以下检索结果与查询的相关性：</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        查询：<span class="subst">{query}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        检索结果：</span></span><br><span class="line"><span class="string">        <span class="subst">{<span class="built_in">chr</span>(<span class="number">10</span>).join([<span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{doc.page_content[:<span class="number">200</span>]}</span>"</span> <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs)])}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        对每个结果给出相关性评分（1-5），并说明原因。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        evaluation = llm(evaluation_prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. 生成答案</span></span><br><span class="line">        answer_prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">        基于以下检索结果回答问题：</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        查询：<span class="subst">{query}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        检索结果：</span></span><br><span class="line"><span class="string">        <span class="subst">{<span class="built_in">chr</span>(<span class="number">10</span>).join([doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs])}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        相关性评估：</span></span><br><span class="line"><span class="string">        <span class="subst">{evaluation}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        答案：</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        answer = llm(answer_prompt)</span><br><span class="line">        <span class="keyword">return</span> answer, docs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 直接生成（不需要检索）</span></span><br><span class="line">        answer = llm(<span class="string">f"回答以下问题：<span class="subst">{query}</span>"</span>)</span><br><span class="line">        <span class="keyword">return</span> answer, []</span><br></pre></td></tr></table></figure>
<h2 id="qa-rag常见问题">❓ Q&amp;A: RAG常见问题</h2>
<h3 id="q1-rag-和微调有什么区别什么时候用-rag">Q1: RAG
和微调有什么区别？什么时候用 RAG？</h3>
<p><strong>A</strong>: -
<strong>RAG</strong>：动态检索外部知识，适合知识库频繁更新、需要访问最新信息、多领域知识的场景
-
<strong>微调</strong>：将知识编码到模型参数中，适合特定任务、知识相对稳定、需要极致性能的场景
- <strong>选择</strong>：如果知识需要频繁更新或涉及私有数据，选择
RAG；如果任务固定且性能要求高，考虑微调</p>
<h3 id="q2-如何选择向量数据库">Q2: 如何选择向量数据库？</h3>
<p><strong>A</strong>: 选择取决于： -
<strong>数据规模</strong>：百万级用 FAISS，千万级+用 Milvus -
<strong>部署方式</strong>：本地用 FAISS/Chroma，云部署用 Milvus/Pinecone
- <strong>运维能力</strong>：不想管理基础设施用 Pinecone，有运维团队用
Milvus - <strong>开发阶段</strong>：快速原型用 FAISS/Chroma，生产环境用
Milvus</p>
<h3 id="q3-embedding-模型如何选择">Q3: Embedding 模型如何选择？</h3>
<p><strong>A</strong>: -
<strong>通用场景</strong>：<code>all-mpnet-base-v2</code> 或
<code>text-embedding-ada-002</code> -
<strong>资源受限</strong>：<code>all-MiniLM-L6-v2</code>（速度快、维度低）
- <strong>问答任务</strong>：<code>multi-qa-mpnet-base</code> -
<strong>多语言</strong>：<code>paraphrase-multilingual-mpnet-base-v2</code>
- <strong>领域特定</strong>：使用领域数据微调通用模型</p>
<h3 id="q4-dense-retrieval-和-sparse-retrieval-如何选择">Q4: Dense
Retrieval 和 Sparse Retrieval 如何选择？</h3>
<p><strong>A</strong>: - <strong>Dense
Retrieval</strong>：适合语义理解、同义词匹配、概念检索 - <strong>Sparse
Retrieval</strong>：适合精确关键词匹配、术语检索 -
<strong>推荐</strong>：使用 Hybrid Retrieval，结合两者优势</p>
<h3 id="q5-如何提高检索精度">Q5: 如何提高检索精度？</h3>
<p><strong>A</strong>: 多管齐下： 1. <strong>优化
Embedding</strong>：使用更好的模型或领域微调 2.
<strong>改进切分策略</strong>：根据文档特点选择合适的切分方法 3.
<strong>使用 Reranking</strong>：Cross-Encoder 重排序 4.
<strong>查询优化</strong>：查询重写、扩展、分解 5.
<strong>多阶段检索</strong>：先粗排再精排</p>
<h3 id="q6-rag-系统出现幻觉怎么办">Q6: RAG 系统出现幻觉怎么办？</h3>
<p><strong>A</strong>: 1.
<strong>提高检索质量</strong>：确保检索到的文档与查询相关 2.
<strong>Prompt
设计</strong>：明确要求模型基于检索内容回答，不知道就说不知道 3.
<strong>结果验证</strong>：对关键信息进行事实核查 4. <strong>使用
Self-RAG</strong>：让模型评估检索结果的相关性 5.
<strong>置信度评分</strong>：对生成结果给出置信度，低置信度时提示用户</p>
<h3 id="q7-如何处理长文档">Q7: 如何处理长文档？</h3>
<p><strong>A</strong>: 1. <strong>Parent-Child
Retrieval</strong>：检索小块，返回大块 2.
<strong>滑动窗口</strong>：检索时包含相邻块 3.
<strong>文档摘要</strong>：对长文档生成摘要，检索摘要 4.
<strong>层次检索</strong>：先检索章节，再检索具体内容</p>
<h3 id="q8-rag-系统的延迟如何优化">Q8: RAG 系统的延迟如何优化？</h3>
<p><strong>A</strong>: 1. <strong>异步检索</strong>：并行检索多个查询 2.
<strong>缓存</strong>：缓存常见查询的结果 3.
<strong>索引优化</strong>：使用更快的索引（如 HNSW） 4.
<strong>批量处理</strong>：批量处理多个查询 5.
<strong>模型优化</strong>：使用更快的 Embedding 和生成模型</p>
<h3 id="q9-如何评估-rag-系统性能">Q9: 如何评估 RAG 系统性能？</h3>
<p><strong>A</strong>: 评估指标： -
<strong>检索指标</strong>：Recall@K、MRR（Mean Reciprocal Rank）、NDCG -
<strong>生成指标</strong>：BLEU、ROUGE、BERTScore、人工评估 -
<strong>端到端指标</strong>：答案准确性、相关性、完整性 -
<strong>系统指标</strong>：延迟、吞吐量、成本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_rag_system</span>(<span class="params">qa_chain, test_set</span>):</span><br><span class="line">    <span class="string">"""评估 RAG 系统"""</span></span><br><span class="line">    results = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> query, ground_truth <span class="keyword">in</span> test_set:</span><br><span class="line">        <span class="comment"># 生成答案</span></span><br><span class="line">        result = qa_chain({<span class="string">"query"</span>: query})</span><br><span class="line">        answer = result[<span class="string">"result"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算指标</span></span><br><span class="line">        <span class="keyword">from</span> rouge_score <span class="keyword">import</span> rouge_scorer</span><br><span class="line">        scorer = rouge_scorer.RougeScorer([<span class="string">'rouge1'</span>, <span class="string">'rougeL'</span>], use_stemmer=<span class="literal">True</span>)</span><br><span class="line">        scores = scorer.score(ground_truth, answer)</span><br><span class="line">        </span><br><span class="line">        results.append({</span><br><span class="line">            <span class="string">"query"</span>: query,</span><br><span class="line">            <span class="string">"answer"</span>: answer,</span><br><span class="line">            <span class="string">"ground_truth"</span>: ground_truth,</span><br><span class="line">            <span class="string">"rouge1"</span>: scores[<span class="string">"rouge1"</span>].fmeasure,</span><br><span class="line">            <span class="string">"rougeL"</span>: scores[<span class="string">"rougeL"</span>].fmeasure</span><br><span class="line">        })</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算平均分数</span></span><br><span class="line">    avg_rouge1 = <span class="built_in">sum</span>([r[<span class="string">"rouge1"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results]) / <span class="built_in">len</span>(results)</span><br><span class="line">    avg_rougeL = <span class="built_in">sum</span>([r[<span class="string">"rougeL"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results]) / <span class="built_in">len</span>(results)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> {</span><br><span class="line">        <span class="string">"avg_rouge1"</span>: avg_rouge1,</span><br><span class="line">        <span class="string">"avg_rougeL"</span>: avg_rougeL,</span><br><span class="line">        <span class="string">"results"</span>: results</span><br><span class="line">    }</span><br></pre></td></tr></table></figure>
<h3 id="q10-如何构建多轮对话的-rag-系统">Q10: 如何构建多轮对话的 RAG
系统？</h3>
<p><strong>A</strong>: 1. <strong>上下文管理</strong>：维护对话历史 2.
<strong>查询重写</strong>：将当前查询与历史结合 3.
<strong>上下文检索</strong>：检索时考虑对话上下文 4.
<strong>记忆机制</strong>：区分短期记忆（当前对话）和长期记忆（知识库）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConversationalRAG</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, qa_chain</span>):</span><br><span class="line">        self.qa_chain = qa_chain</span><br><span class="line">        self.conversation_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="comment"># 1. 构建带上下文的查询</span></span><br><span class="line">        context_query = self._build_contextual_query(query)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 检索和生成</span></span><br><span class="line">        result = self.qa_chain({<span class="string">"query"</span>: context_query})</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 更新历史</span></span><br><span class="line">        self.conversation_history.append({</span><br><span class="line">            <span class="string">"user"</span>: query,</span><br><span class="line">            <span class="string">"assistant"</span>: result[<span class="string">"result"</span>]</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result[<span class="string">"result"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_contextual_query</span>(<span class="params">self, current_query</span>):</span><br><span class="line">        <span class="string">"""构建带上下文的查询"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.conversation_history:</span><br><span class="line">            <span class="keyword">return</span> current_query</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加最近几轮对话</span></span><br><span class="line">        recent_history = self.conversation_history[-<span class="number">3</span>:]</span><br><span class="line">        context = <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"用户：<span class="subst">{h[<span class="string">'user'</span>]}</span>\n助手：<span class="subst">{h[<span class="string">'assistant'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> recent_history</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"""</span></span><br><span class="line"><span class="string">        对话历史：</span></span><br><span class="line"><span class="string">        <span class="subst">{context}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        当前问题：<span class="subst">{current_query}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        请基于对话历史回答当前问题。</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></table></figure>
<hr>
<p>RAG 技术为大语言模型提供了访问外部知识的能力，是构建知识增强 AI
系统的关键技术。一个优秀的 RAG
系统需要精心设计各个组件，从向量数据库选择到检索策略优化，从查询处理到结果生成。在实际应用中，需要根据具体需求选择合适的组件和技术，不断优化和迭代，才能构建出高效、准确的
RAG 系统。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>本文标题：自然语言处理（十）—— RAG与知识增强系统</li>
        <li>本文作者：Chen Kai</li>
        <li>创建时间：2025-02-28 00:00:00</li>
        <li>
            本文链接：https://www.chenk.top/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94-RAG%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E7%B3%BB%E7%BB%9F/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/RAG/">#RAG</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94-%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8EIn-Context-Learning/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">自然语言处理（七）—— 提示工程与In-Context Learning</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94-%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">自然语言处理（十二）—— 前沿技术与实战应用</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- 由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#rag-%E5%9F%BA%E7%A1%80%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">RAG 基础与架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">1.1.</span> <span class="nav-text">RAG 核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-%E6%9E%B6%E6%9E%84%E6%B5%81%E7%A8%8B"><span class="nav-number">1.2.</span> <span class="nav-text">RAG 架构流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-vs-%E5%BE%AE%E8%B0%83"><span class="nav-number">1.3.</span> <span class="nav-text">RAG vs 微调</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%89%E6%8B%A9"><span class="nav-number">2.</span> <span class="nav-text">向量数据库选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#faissfacebook-ai-similarity-search"><span class="nav-number">2.1.</span> <span class="nav-text">FAISS（Facebook AI
Similarity Search）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#milvus"><span class="nav-number">2.2.</span> <span class="nav-text">Milvus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pinecone"><span class="nav-number">2.3.</span> <span class="nav-text">Pinecone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chroma"><span class="nav-number">2.4.</span> <span class="nav-text">Chroma</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E6%AF%94"><span class="nav-number">2.5.</span> <span class="nav-text">向量数据库对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#embedding-%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94"><span class="nav-number">3.</span> <span class="nav-text">Embedding 模型对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8-embedding-%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">通用 Embedding 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%86%E5%9F%9F%E7%89%B9%E5%AE%9A-embedding"><span class="nav-number">3.2.</span> <span class="nav-text">领域特定 Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E6%8C%87%E5%8D%97"><span class="nav-number">3.3.</span> <span class="nav-text">Embedding 模型选择指南</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="nav-number">4.</span> <span class="nav-text">检索策略优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dense-retrieval%E5%AF%86%E9%9B%86%E6%A3%80%E7%B4%A2"><span class="nav-number">4.1.</span> <span class="nav-text">Dense Retrieval（密集检索）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparse-retrieval%E7%A8%80%E7%96%8F%E6%A3%80%E7%B4%A2"><span class="nav-number">4.2.</span> <span class="nav-text">Sparse Retrieval（稀疏检索）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hybrid-retrieval%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2"><span class="nav-number">4.3.</span> <span class="nav-text">Hybrid Retrieval（混合检索）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reranking-%E6%8A%80%E6%9C%AF"><span class="nav-number">5.</span> <span class="nav-text">Reranking 技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cross-encoder-reranking"><span class="nav-number">5.1.</span> <span class="nav-text">Cross-Encoder Reranking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E9%98%B6%E6%AE%B5%E6%A3%80%E7%B4%A2"><span class="nav-number">5.2.</span> <span class="nav-text">多阶段检索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#query-%E9%87%8D%E5%86%99%E4%B8%8E%E6%89%A9%E5%B1%95"><span class="nav-number">6.</span> <span class="nav-text">Query 重写与扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#query-rewriting%E6%9F%A5%E8%AF%A2%E9%87%8D%E5%86%99"><span class="nav-number">6.1.</span> <span class="nav-text">Query Rewriting（查询重写）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#query-expansion%E6%9F%A5%E8%AF%A2%E6%89%A9%E5%B1%95"><span class="nav-number">6.2.</span> <span class="nav-text">Query Expansion（查询扩展）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#query-decomposition%E6%9F%A5%E8%AF%A2%E5%88%86%E8%A7%A3"><span class="nav-number">6.3.</span> <span class="nav-text">Query
Decomposition（查询分解）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98%E6%9E%84%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7-rag-%E7%B3%BB%E7%BB%9F"><span class="nav-number">7.</span> <span class="nav-text">实战：构建企业级 RAG 系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-langchain-%E6%9E%84%E5%BB%BA-rag"><span class="nav-number">7.1.</span> <span class="nav-text">使用 LangChain 构建 RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-llamaindex-%E6%9E%84%E5%BB%BA-rag"><span class="nav-number">7.2.</span> <span class="nav-text">使用 LlamaIndex 构建 RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7-rag-%E6%A8%A1%E5%BC%8F"><span class="nav-number">7.3.</span> <span class="nav-text">高级 RAG 模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qa-rag%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">8.</span> <span class="nav-text">❓ Q&amp;A: RAG常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-rag-%E5%92%8C%E5%BE%AE%E8%B0%83%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%94%A8-rag"><span class="nav-number">8.1.</span> <span class="nav-text">Q1: RAG
和微调有什么区别？什么时候用 RAG？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">8.2.</span> <span class="nav-text">Q2: 如何选择向量数据库？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-embedding-%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="nav-number">8.3.</span> <span class="nav-text">Q3: Embedding 模型如何选择？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-dense-retrieval-%E5%92%8C-sparse-retrieval-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="nav-number">8.4.</span> <span class="nav-text">Q4: Dense
Retrieval 和 Sparse Retrieval 如何选择？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%A3%80%E7%B4%A2%E7%B2%BE%E5%BA%A6"><span class="nav-number">8.5.</span> <span class="nav-text">Q5: 如何提高检索精度？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-rag-%E7%B3%BB%E7%BB%9F%E5%87%BA%E7%8E%B0%E5%B9%BB%E8%A7%89%E6%80%8E%E4%B9%88%E5%8A%9E"><span class="nav-number">8.6.</span> <span class="nav-text">Q6: RAG 系统出现幻觉怎么办？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q7-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%95%BF%E6%96%87%E6%A1%A3"><span class="nav-number">8.7.</span> <span class="nav-text">Q7: 如何处理长文档？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q8-rag-%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BB%B6%E8%BF%9F%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96"><span class="nav-number">8.8.</span> <span class="nav-text">Q8: RAG 系统的延迟如何优化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q9-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0-rag-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD"><span class="nav-number">8.9.</span> <span class="nav-text">Q9: 如何评估 RAG 系统性能？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q10-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E7%9A%84-rag-%E7%B3%BB%E7%BB%9F"><span class="nav-number">8.10.</span> <span class="nav-text">Q10: 如何构建多轮对话的 RAG
系统？</span></a></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
