<!DOCTYPE html>



<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            自然语言处理（一）—— NLP入门与文本预处理 |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"zh-CN","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">自然语言处理（一）—— NLP入门与文本预处理</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2024-09-05 00:00:00</span>
        <span class="mobile">2024-09-05 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>17.4k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>66 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>人类每天产生的文本数据量惊人：社交媒体上的帖子、搜索引擎的查询、客服系统的对话、新闻报道、学术论文……这些文字背后蕴藏着巨大的价值，但计算机天生不理解人类语言。自然语言处理（Natural
Language
Processing，NLP）就是教会机器"读懂"文字的技术，让它们能从海量文本中提取信息、理解意图、生成回复，甚至进行创作。</p>
<p>本文从零开始介绍 NLP 的核心概念和第一步：文本预处理。你将了解 NLP
如何从符号规则走向深度学习，为什么需要对文本进行分词、去噪、标准化，以及如何用
Python
工具实现中英文预处理流程。最后通过一个完整的文本分类实战案例，把理论和代码串联起来。</p>
<span id="more"></span>
<h2 id="nlp-是什么从符号主义到大模型的演进之路">NLP
是什么？从符号主义到大模型的演进之路</h2>
<h3 id="自然语言处理的本质">自然语言处理的本质</h3>
<p>人类用语言交流时，一个简单的句子"我在银行取钱"包含多重信息：</p>
<ul>
<li><strong>词汇语义</strong>："银行"指金融机构还是河岸？</li>
<li><strong>句法结构</strong>："我"是主语，"取钱"是动作</li>
<li><strong>语用意图</strong>：陈述事实还是请求帮助？</li>
</ul>
<p>NLP
的核心任务是让计算机理解这些层次的信息，并在此基础上完成具体任务：</p>
<ul>
<li><strong>文本分类</strong>：判断邮件是否是垃圾邮件</li>
<li><strong>信息抽取</strong>：从新闻中提取人名、地点、时间</li>
<li><strong>机器翻译</strong>：把英文文档翻译成中文</li>
<li><strong>问答系统</strong>：回答"特斯拉的创始人是谁？"</li>
<li><strong>文本生成</strong>：写诗、摘要、对话回复</li>
</ul>
<h3 id="nlp-发展的四个阶段">NLP 发展的四个阶段</h3>
<h4 id="第一阶段符号主义1950s-1980s">第一阶段：符号主义（1950s-1980s）</h4>
<p>早期研究者认为语言可以用规则描述。比如：</p>
<ul>
<li>句子 = 主语 + 谓语 + 宾语</li>
<li>如果句子包含"不"、"没有"，则情感为负面</li>
</ul>
<p>这种方法依赖专家手工编写语法规则和词典。典型代表是
ELIZA（1966年的聊天机器人），它用模式匹配把用户的话改写后抛回去：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">用户："我很难过"</span><br><span class="line">ELIZA："为什么你很难过？"（直接套用模板）</span><br></pre></td></tr></table></figure>
<p><strong>局限性</strong>：语言现象太复杂，规则无法穷尽。"银行"的歧义、"真香"的反讽、"雨我无瓜"的网络用语……人工规则很快就力不从心。</p>
<h4 id="第二阶段统计方法1990s-2010s">第二阶段：统计方法（1990s-2010s）</h4>
<p>统计 NLP 不再依赖专家，而是让机器从大量文本中学习规律。核心思想：</p>
<ul>
<li><strong>数据驱动</strong>：给模型看 10
万篇新闻，让它统计哪些词经常一起出现</li>
<li><strong>概率建模</strong>：用 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.247ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 4529 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mtext" transform="translate(1140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">序</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">列</text></g><g data-mml-node="mo" transform="translate(4140,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> 表示句子的合理性</li>
</ul>
<p>标志性成果：</p>
<ul>
<li><strong>N-gram 语言模型</strong>：根据前 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewbox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container></span> 个词预测下一个词 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="44.019ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 19456.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(2183,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(2461,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(3613.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(4058.2,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"/></g><g data-mml-node="mo" transform="translate(5396.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(5841.5,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(7788.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(8454.9,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"/></g><g data-mml-node="mi" transform="translate(9510.7,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(10261.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(10650.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(11693.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(11971.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(1123,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1723,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(2501,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(14892.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(15337.3,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"/></g><g data-mml-node="mo" transform="translate(16676,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(17120.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(19067.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></li>
<li><strong>隐马尔可夫模型（HMM）</strong>：用于词性标注</li>
<li><strong>朴素贝叶斯分类器</strong>：用于垃圾邮件过滤</li>
</ul>
<p>这一时期的模型需要<strong>特征工程</strong>：人工设计输入特征（如词频、是否包含特定词、句子长度等），然后训练分类器。</p>
<h4 id="第三阶段深度学习2013-2020">第三阶段：深度学习（2013-2020）</h4>
<p>神经网络的兴起改变了 NLP 格局。关键技术：</p>
<ul>
<li><strong>Word2Vec（2013）</strong>：把词映射为稠密向量，语义相似的词向量接近
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.466ex;" xmlns="http://www.w3.org/2000/svg" width="29.793ex" height="2.036ex" role="img" focusable="false" viewbox="0 -694 13168.4 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="6B" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T97 124T98 167T98 217T98 272T98 329Q98 366 98 407T98 482T98 542T97 586T97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V463L180 233L240 287Q300 341 304 347Q310 356 310 364Q310 383 289 385H284V431H293Q308 428 412 428Q475 428 484 431H489V385H476Q407 380 360 341Q286 278 286 274Q286 273 349 181T420 79Q434 60 451 53T500 46H511V0H505Q496 3 418 3Q322 3 307 0H299V46H306Q330 48 330 65Q330 72 326 79Q323 84 276 153T228 222L176 176V120V84Q176 65 178 59T189 49Q210 46 238 46H254V0H246Q231 3 137 3T28 0H20V46H36Z"/><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(528,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(806,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(1362,0)"/></g><g data-mml-node="mo" transform="translate(2084.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mtext" transform="translate(3084.4,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)"/></g><g data-mml-node="mo" transform="translate(5195.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mtext" transform="translate(6195.9,0)"><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(722,0)"/><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1222,0)"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2055,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2555,0)"/></g><g data-mml-node="mo" transform="translate(9584.7,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"/></g><g data-mml-node="mtext" transform="translate(10640.4,0)"><path data-c="71" d="M33 218Q33 308 95 374T236 441H246Q330 441 381 372L387 364Q388 364 404 403L420 442H457V156Q457 -132 458 -134Q462 -142 470 -145Q491 -148 519 -148H535V-194H527L504 -193Q480 -192 453 -192T415 -191Q312 -191 303 -194H295V-148H311Q339 -148 360 -145Q369 -141 371 -135T373 -106V-41V49Q313 -11 236 -11Q154 -11 94 53T33 218ZM376 300Q346 389 278 401Q275 401 269 401T261 402Q211 400 171 350T131 214Q131 137 165 82T253 27Q296 27 328 54T376 118V300Z"/><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(528,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1084,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1528,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1972,0)"/></g></g></g></svg></mjx-container></span></li>
<li><strong>循环神经网络（RNN/LSTM）</strong>：处理变长序列，捕捉上下文关系</li>
<li><strong>注意力机制（Attention）</strong>：让模型关注输入的关键部分</li>
<li><strong>Transformer（2017）</strong>：并行化训练，成为现代 NLP
的基石</li>
</ul>
<p>深度学习模型可以<strong>端到端</strong>训练：直接输入原始文本，输出任务结果，不需要手工设计特征。</p>
<h4 id="第四阶段预训练大模型2018至今">第四阶段：预训练大模型（2018至今）</h4>
<p>2018 年 BERT 横空出世，开启了"预训练-微调"范式：</p>
<ol type="1">
<li><strong>预训练</strong>：在海量无标注文本上训练通用语言模型</li>
<li><strong>微调</strong>：在少量标注数据上针对具体任务调整</li>
</ol>
<p>随后 GPT-3（2020，1750
亿参数）、ChatGPT（2022）、GPT-4（2023）不断刷新能力上限。现在的大模型具备：</p>
<ul>
<li><strong>少样本学习</strong>：给几个例子就能理解新任务</li>
<li><strong>指令跟随</strong>：理解"用正式语气写一封道歉信"</li>
<li><strong>多模态能力</strong>：同时处理文本、图像、语音</li>
</ul>
<h2 id="nlp-的现实应用场景">NLP 的现实应用场景</h2>
<h3 id="搜索引擎">搜索引擎</h3>
<p>当你在 Google 搜索"苹果新品发布会"时，NLP 帮助：</p>
<ul>
<li><strong>查询理解</strong>：识别"苹果"指公司而非水果</li>
<li><strong>相关性排序</strong>：把最相关的新闻排在前面</li>
<li><strong>摘要生成</strong>：在搜索结果下方显示关键信息</li>
</ul>
<h3 id="机器翻译">机器翻译</h3>
<p>Google 翻译、DeepL 用神经机器翻译（NMT）实现：</p>
<ul>
<li><strong>Encoder-Decoder
架构</strong>：把源语言编码为语义表示，再解码为目标语言</li>
<li><strong>注意力机制</strong>：翻译长句时关注对应的源词</li>
</ul>
<h3 id="智能客服">智能客服</h3>
<p>电商平台的客服机器人能：</p>
<ul>
<li><strong>意图识别</strong>：判断用户是要退货、查物流还是咨询尺码</li>
<li><strong>槽位填充</strong>：提取订单号、商品名等关键信息</li>
<li><strong>对话管理</strong>：多轮交互引导用户解决问题</li>
</ul>
<h3 id="推荐系统">推荐系统</h3>
<p>新闻 App 推荐你可能感兴趣的文章：</p>
<ul>
<li><strong>文本表示</strong>：用词向量或 BERT 把文章编码为向量</li>
<li><strong>相似度计算</strong>：推荐与你已读文章相似的内容</li>
<li><strong>用户画像</strong>：根据历史阅读构建兴趣模型</li>
</ul>
<h3 id="情感分析">情感分析</h3>
<p>分析商品评论的情感倾向：</p>
<ul>
<li><strong>正面</strong>："质量很好，物流快！"</li>
<li><strong>负面</strong>："颜色和描述不符，申请退货"</li>
<li><strong>中性</strong>："包装完好无损"</li>
</ul>
<p>企业可以据此监控品牌口碑、发现产品问题。</p>
<h2 id="为什么需要文本预处理">为什么需要文本预处理？</h2>
<p>假设你要训练一个模型判断电影评论的情感。原始数据可能是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"This movie is GREAT!!! 😊"</span><br><span class="line">"i loved it, best film ever."</span><br><span class="line">"Terrible... worst experience :("</span><br></pre></td></tr></table></figure>
<p>直接把这些文本喂给模型会遇到问题：</p>
<ol type="1">
<li><strong>大小写不统一</strong>："GREAT" 和 "great"
被当作不同的词</li>
<li><strong>标点和表情</strong>："!!!" 和 "😊" 干扰了核心词汇</li>
<li><strong>拼写和形态</strong>："loved" 和 "love"
语义相同但形式不同</li>
<li><strong>噪音词</strong>："this"、"is"、"it"
等高频词对情感判断贡献不大</li>
</ol>
<p>文本预处理就是<strong>清洗和标准化</strong>原始文本，提取有效信息，让模型更容易学习。</p>
<h2 id="文本预处理的核心流程">文本预处理的核心流程</h2>
<h3 id="标准化normalization">标准化（Normalization）</h3>
<p>把文本转为统一格式：</p>
<ul>
<li><strong>小写化</strong>：<code>"GREAT"</code> →
<code>"great"</code></li>
<li><strong>去除标点</strong>：<code>"loved it!!!"</code> →
<code>"loved it"</code></li>
<li><strong>统一编码</strong>：处理全角半角、繁简体转换</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This movie is GREAT!!! 😊"</span></span><br><span class="line"><span class="comment"># 小写化</span></span><br><span class="line">text = text.lower()</span><br><span class="line"><span class="comment"># 去除标点和表情</span></span><br><span class="line">text = re.sub(<span class="string">r'[^\w\s]'</span>, <span class="string">''</span>, text)</span><br><span class="line"><span class="built_in">print</span>(text)  <span class="comment"># "this movie is great"</span></span><br></pre></td></tr></table></figure>
<h3 id="分词tokenization">分词（Tokenization）</h3>
<p>把句子切分为词或子词单元。英文用空格分词较简单，中文则需要专门算法。</p>
<p><strong>英文分词</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"I love machine learning"</span></span><br><span class="line">tokens = text.split()</span><br><span class="line"><span class="built_in">print</span>(tokens)  <span class="comment"># ['I', 'love', 'machine', 'learning']</span></span><br></pre></td></tr></table></figure></p>
<p><strong>中文分词</strong>（稍后详述）： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">text = <span class="string">"我喜欢自然语言处理"</span></span><br><span class="line">tokens = jieba.lcut(text)</span><br><span class="line"><span class="built_in">print</span>(tokens)  <span class="comment"># ['我', '喜欢', '自然语言处理']</span></span><br></pre></td></tr></table></figure></p>
<h3 id="去除停用词stop-words-removal">去除停用词（Stop Words
Removal）</h3>
<p>停用词是高频但语义贡献小的词，如"的"、"了"、"is"、"the"。去掉它们可以：</p>
<ul>
<li>减少特征维度</li>
<li>突出关键词</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">stop_words = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">tokens = [<span class="string">'i'</span>, <span class="string">'love'</span>, <span class="string">'machine'</span>, <span class="string">'learning'</span>]</span><br><span class="line">filtered = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"><span class="built_in">print</span>(filtered)  <span class="comment"># ['love', 'machine', 'learning']</span></span><br></pre></td></tr></table></figure>
<h3 id="词干化与词形还原">词干化与词形还原</h3>
<p>把词的不同形态归为统一形式：</p>
<ul>
<li><strong>词干化（Stemming）</strong>：粗暴地截取词根
<ul>
<li><code>running</code> → <code>run</code></li>
<li><code>happily</code> → <code>happi</code>（不是真实单词）</li>
</ul></li>
<li><strong>词形还原（Lemmatization）</strong>：基于词典还原为基础形式
<ul>
<li><code>running</code> → <code>run</code></li>
<li><code>better</code> → <code>good</code></li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer, WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(stemmer.stem(<span class="string">"running"</span>))  <span class="comment"># "run"</span></span><br><span class="line"><span class="built_in">print</span>(lemmatizer.lemmatize(<span class="string">"running"</span>, pos=<span class="string">'v'</span>))  <span class="comment"># "run"</span></span><br><span class="line"><span class="built_in">print</span>(lemmatizer.lemmatize(<span class="string">"better"</span>, pos=<span class="string">'a'</span>))  <span class="comment"># "good"</span></span><br></pre></td></tr></table></figure>
<p>词形还原更准确但速度慢，词干化快但有时产生不存在的词。实际中根据任务选择。</p>
<h2 id="中文分词挑战与工具">中文分词：挑战与工具</h2>
<h3 id="中文分词的特殊性">中文分词的特殊性</h3>
<p>英文单词间有空格，中文则是连续的字符流。"我爱自然语言处理"可以有多种切分方式：</p>
<ul>
<li><code>["我", "爱", "自然", "语言", "处理"]</code>（字级别）</li>
<li><code>["我", "爱", "自然语言", "处理"]</code></li>
<li><code>["我", "爱", "自然语言处理"]</code>（词级别）</li>
</ul>
<p>正确的分词需要理解语义。考虑歧义："结婚的和尚未结婚的"</p>
<ul>
<li>错误分词：<code>["结婚", "的", "和", "尚未", "结婚", "的"]</code></li>
<li>正确分词：<code>["结婚", "的", "和", "尚", "未", "结婚", "的"]</code></li>
</ul>
<h3 id="主流中文分词工具">主流中文分词工具</h3>
<h4 id="jieba结巴分词">jieba（结巴分词）</h4>
<p>最流行的 Python 中文分词库，基于前缀词典和动态规划。</p>
<p><strong>安装</strong>： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></p>
<p><strong>基本用法</strong>：</p>
<p>jieba分词提供了三种切分模式，每种模式适用于不同的场景。理解这些模式的区别对于选择合适的分词策略至关重要。</p>
<p><strong>模式对比</strong>：精确模式追求准确性，每个词只出现一次，适合文本分析和分类任务；全模式输出所有可能的词组合，适合关键词提取和词云生成；搜索引擎模式在精确模式基础上对长词进行细粒度切分，提高召回率，适合搜索场景。</p>
<p><strong>设计原理</strong>：jieba基于前缀词典和动态规划算法，通过计算最大概率路径来确定最佳分词结果。精确模式选择概率最大的路径，全模式保留所有可能的路径，搜索引擎模式则对长词进行二次切分以增加匹配机会。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例文本：包含复合词"清华大学"（这是一个专有名词）</span></span><br><span class="line">text = <span class="string">"我来到北京清华大学"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 模式1：精确模式（默认，推荐） ==========</span></span><br><span class="line"><span class="comment"># cut_all=False: 精确模式，只输出概率最大的分词结果</span></span><br><span class="line"><span class="comment"># 特点：</span></span><br><span class="line"><span class="comment">#   - 每个词只出现一次，无冗余</span></span><br><span class="line"><span class="comment">#   - 分词结果最准确，适合大多数NLP任务</span></span><br><span class="line"><span class="comment">#   - 速度较快</span></span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 返回：生成器对象（惰性计算，节省内存）</span></span><br><span class="line"><span class="comment"># 注意：jieba.cut()返回生成器，jieba.lcut()返回列表</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"精确模式:"</span>, <span class="string">"/"</span>.join(seg_list))  </span><br><span class="line"><span class="comment"># 输出："我/来到/北京/清华大学"</span></span><br><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment">#   - "清华大学"被识别为一个整体（专有名词）</span></span><br><span class="line"><span class="comment">#   - 没有冗余切分（如"清华"+"大学"）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 模式2：全模式 ==========</span></span><br><span class="line"><span class="comment"># cut_all=True: 全模式，输出所有可能的词组合</span></span><br><span class="line"><span class="comment"># 特点：</span></span><br><span class="line"><span class="comment">#   - 包含所有可能的切分方式，有冗余</span></span><br><span class="line"><span class="comment">#   - 适合关键词提取、词云生成等需要更多候选词的场景</span></span><br><span class="line"><span class="comment">#   - 速度较慢，结果可能包含不存在的词</span></span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"全模式:"</span>, <span class="string">"/"</span>.join(seg_list))</span><br><span class="line"><span class="comment"># 输出："我/来到/北京/清华/清华大学/华大/大学"</span></span><br><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment">#   - "清华大学"被切分为多种组合："清华"+"大学"+"清华大学"</span></span><br><span class="line"><span class="comment">#   - "华大"是"清华"+"大学"的重叠部分，可能不是真实词</span></span><br><span class="line"><span class="comment">#   - 适合需要更多候选词的场景（如关键词提取）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 模式3：搜索引擎模式 ==========</span></span><br><span class="line"><span class="comment"># cut_for_search(): 搜索引擎模式，对长词进行细粒度切分</span></span><br><span class="line"><span class="comment"># 特点：</span></span><br><span class="line"><span class="comment">#   - 在精确模式基础上，对长词（3字及以上）进行二次切分</span></span><br><span class="line"><span class="comment">#   - 提高召回率：用户搜索"清华"也能匹配到"清华大学"</span></span><br><span class="line"><span class="comment">#   - 适合搜索引擎、信息检索等场景</span></span><br><span class="line">seg_list = jieba.cut_for_search(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"搜索引擎模式:"</span>, <span class="string">"/"</span>.join(seg_list))</span><br><span class="line"><span class="comment"># 输出："我/来到/北京/清华/华大/大学/清华大学"</span></span><br><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment">#   - "清华大学"被切分为："清华"+"大学"+"清华大学"</span></span><br><span class="line"><span class="comment">#   - 这样用户搜索"清华"或"大学"都能匹配到包含"清华大学"的文档</span></span><br><span class="line"><span class="comment">#   - 比全模式更精确（不会切分短词）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 性能对比 ==========</span></span><br><span class="line"><span class="comment"># 精确模式：速度最快，结果最准确，推荐用于大多数任务</span></span><br><span class="line"><span class="comment"># 全模式：速度较慢，结果有冗余，适合关键词提取</span></span><br><span class="line"><span class="comment"># 搜索引擎模式：速度中等，结果平衡，适合搜索场景</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 使用建议 ==========</span></span><br><span class="line"><span class="comment"># 1. 文本分类、情感分析：使用精确模式（cut_all=False）</span></span><br><span class="line"><span class="comment"># 2. 关键词提取、词云：使用全模式（cut_all=True）或搜索引擎模式</span></span><br><span class="line"><span class="comment"># 3. 搜索引擎、信息检索：使用搜索引擎模式（cut_for_search）</span></span><br><span class="line"><span class="comment"># 4. 不确定时：使用精确模式，这是最安全和通用的选择</span></span><br></pre></td></tr></table></figure>
<p><strong>深入解读：jieba分词的原理与优化</strong></p>
<p>jieba分词看似简单，但背后有复杂的算法和设计考虑：</p>
<p><strong>1. 分词算法原理</strong></p>
<p>jieba使用<strong>基于前缀词典的动态规划算法</strong>：</p>
<ul>
<li><strong>前缀词典</strong>：存储所有可能的词及其频率，用于计算分词路径的概率</li>
<li><strong>动态规划</strong>：计算从句子开头到当前位置的最大概率路径</li>
<li><strong>HMM模型</strong>：处理未登录词（OOV），识别新词</li>
</ul>
<p><strong>算法流程</strong>： 1.
构建有向无环图（DAG），每个节点表示可能的词 2.
使用动态规划计算最大概率路径 3. 对未登录词使用HMM模型识别</p>
<p><strong>2. 三种模式的技术细节</strong></p>
<table>
<thead>
<tr>
<th>模式</th>
<th>算法差异</th>
<th>时间复杂度</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>精确模式</strong></td>
<td>选择概率最大的单一路径</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.832ex" height="2.452ex" role="img" focusable="false" viewbox="0 -833.9 2577.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></td>
<td>文本分类、NER</td>
</tr>
<tr>
<td><strong>全模式</strong></td>
<td>保留所有可能的路径</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.584ex" height="2.452ex" role="img" focusable="false" viewbox="0 -833.9 4678 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(2410.8,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mi" transform="translate(3411,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(4289,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></td>
<td>关键词提取</td>
</tr>
<tr>
<td><strong>搜索引擎模式</strong></td>
<td>精确模式 + 长词二次切分</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.776ex" height="2.452ex" role="img" focusable="false" viewbox="0 -833.9 4321 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(2410.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(3411,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g><g data-mml-node="mo" transform="translate(3932,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></td>
<td>信息检索</td>
</tr>
</tbody>
</table>
<p>其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span> 是句子长度，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 878 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span> 是可能的词数，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewbox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></g></g></g></svg></mjx-container></span> 是长词数量。</p>
<p><strong>3. 自定义词典的使用</strong></p>
<p>jieba允许添加自定义词典，这对领域特定文本很重要：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：添加单个词</span></span><br><span class="line">jieba.add_word(<span class="string">"自然语言处理"</span>, freq=<span class="number">1000</span>, tag=<span class="string">"n"</span>)</span><br><span class="line"><span class="comment"># freq: 词频（越高越可能被切分出来）</span></span><br><span class="line"><span class="comment"># tag: 词性标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：加载词典文件</span></span><br><span class="line">jieba.load_userdict(<span class="string">"userdict.txt"</span>)</span><br><span class="line"><span class="comment"># 文件格式：每行一个词，格式为：词 词频 词性（可选）</span></span><br><span class="line"><span class="comment"># 示例：自然语言处理 1000 n</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法3：动态调整词频</span></span><br><span class="line">jieba.suggest_freq(<span class="string">"自然语言处理"</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 强制将"自然语言处理"作为一个词</span></span><br></pre></td></tr></table></figure>
<p><strong>4. 分词准确率的提升</strong></p>
<p>jieba的默认准确率约95%，可以通过以下方式提升：</p>
<ul>
<li><strong>添加领域词典</strong>：针对特定领域（如医疗、法律）添加专业术语</li>
<li><strong>调整词频</strong>：对于歧义切分，提高正确切分的词频</li>
<li><strong>使用pkuseg</strong>：对于特定领域，pkuseg的准确率可能更高</li>
<li><strong>后处理规则</strong>：添加规则处理特定模式（如日期、数字）</li>
</ul>
<p><strong>5. 性能优化</strong></p>
<p>对于大规模文本处理，可以优化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：并行分词（jieba支持多进程）</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">jieba.enable_parallel(<span class="number">4</span>)  <span class="comment"># 开启4进程并行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：批量处理</span></span><br><span class="line">texts = [<span class="string">"文本1"</span>, <span class="string">"文本2"</span>, ...]</span><br><span class="line">results = [<span class="built_in">list</span>(jieba.cut(text)) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法3：使用lcut()而非cut()（如果不需要生成器）</span></span><br><span class="line"><span class="comment"># lcut()返回列表，适合小文本；cut()返回生成器，适合大文本</span></span><br></pre></td></tr></table></figure>
<p><strong>6. 常见问题与解决方案</strong></p>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>专有名词被拆分</td>
<td>词典中未包含该词</td>
<td>使用add_word()添加</td>
</tr>
<tr>
<td>分词结果不一致</td>
<td>词典加载顺序或词频变化</td>
<td>固定词典和参数</td>
</tr>
<tr>
<td>新词无法识别</td>
<td>HMM模型未训练</td>
<td>使用更大的语料训练或手动添加</td>
</tr>
<tr>
<td>处理速度慢</td>
<td>文本过长或未并行</td>
<td>开启并行处理或分段处理</td>
</tr>
<tr>
<td>歧义切分错误</td>
<td>词频设置不当</td>
<td>调整词频或使用规则</td>
</tr>
</tbody>
</table>
<p><strong>7. 与其他分词工具的对比</strong></p>
<table>
<thead>
<tr>
<th>工具</th>
<th>速度</th>
<th>准确率</th>
<th>领域适应性</th>
<th>推荐场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>jieba</strong></td>
<td>快</td>
<td>中等（~95%）</td>
<td>一般</td>
<td>通用文本、快速原型</td>
</tr>
<tr>
<td><strong>pkuseg</strong></td>
<td>中等</td>
<td>高（~97%）</td>
<td>好（有领域模型）</td>
<td>专业领域文本</td>
</tr>
<tr>
<td><strong>THULAC</strong></td>
<td>快</td>
<td>高</td>
<td>一般</td>
<td>学术研究</td>
</tr>
<tr>
<td><strong>LAC</strong></td>
<td>中等</td>
<td>高</td>
<td>一般</td>
<td>需要词性和NER</td>
</tr>
</tbody>
</table>
<p><strong>8. 实际应用建议</strong></p>
<ol type="1">
<li><strong>默认使用精确模式</strong>：除非有特殊需求，否则使用<code>cut_all=False</code></li>
<li><strong>添加领域词典</strong>：针对特定领域添加专业术语词典</li>
<li><strong>处理歧义</strong>：对于已知的歧义切分，使用<code>add_word()</code>或<code>suggest_freq()</code>调整</li>
<li><strong>性能监控</strong>：记录分词时间和准确率，根据需求选择工具</li>
<li><strong>版本控制</strong>：jieba的版本更新可能改变分词结果，固定版本号</li>
</ol>
<p>理解jieba分词的原理和使用技巧，是中文NLP的基础。在实际项目中，根据任务需求选择合适的模式和参数，能够显著提升系统性能。</p>
<p><strong>添加自定义词典</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jieba.add_word(<span class="string">"自然语言处理"</span>)</span><br><span class="line">text = <span class="string">"我在学习自然语言处理"</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"/"</span>.join(jieba.cut(text)))</span><br><span class="line"><span class="comment"># "我/在/学习/自然语言处理"</span></span><br></pre></td></tr></table></figure></p>
<h4 id="pkuseg">pkuseg</h4>
<p>北京大学开源的分词工具，针对不同领域训练了专用模型（新闻、医疗、旅游等），准确率更高但速度慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pkuseg</span><br><span class="line"></span><br><span class="line">seg = pkuseg.pkuseg()  <span class="comment"># 使用默认模型</span></span><br><span class="line">text = <span class="string">"我爱自然语言处理"</span></span><br><span class="line"><span class="built_in">print</span>(seg.cut(text))  <span class="comment"># ['我', '爱', '自然语言处理']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用领域模型</span></span><br><span class="line">seg_med = pkuseg.pkuseg(model_name=<span class="string">'medicine'</span>)</span><br><span class="line">text_med = <span class="string">"患者出现高血压症状"</span></span><br><span class="line"><span class="built_in">print</span>(seg_med.cut(text_med))  <span class="comment"># ['患者', '出现', '高血压', '症状']</span></span><br></pre></td></tr></table></figure>
<h4 id="laclexical-analysis-of-chinese">LAC（Lexical Analysis of
Chinese）</h4>
<p>百度开源的词法分析工具，不仅分词还能进行词性标注和命名实体识别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> LAC <span class="keyword">import</span> LAC</span><br><span class="line"></span><br><span class="line">lac = LAC(mode=<span class="string">'seg'</span>)  <span class="comment"># 只分词</span></span><br><span class="line">text = <span class="string">"百度是一家高科技公司"</span></span><br><span class="line"><span class="built_in">print</span>(lac.run(text))  <span class="comment"># ['百度', '是', '一家', '高科技', '公司']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词 + 词性标注</span></span><br><span class="line">lac = LAC(mode=<span class="string">'lac'</span>)</span><br><span class="line">seg_result, pos_result = lac.run(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">zip</span>(seg_result, pos_result)))</span><br><span class="line"><span class="comment"># [('百度', 'ORG'), ('是', 'v'), ('一家', 'm'), ('高科技', 'n'), ('公司', 'n')]</span></span><br></pre></td></tr></table></figure>
<h3 id="中文预处理完整流程">中文预处理完整流程</h3>
<p>中文文本预处理面临独特的挑战：没有天然的词边界，需要先分词才能进行后续处理。与英文不同，中文的标点、数字、英文单词可能混在一起，需要统一处理。此外，中文停用词（如"的"、"了"）虽然高频但语义贡献小，去除它们可以突出关键词，减少特征维度。</p>
<p>本代码实现了一个完整的中文预处理流程，包含四个核心步骤：文本清洗、分词、停用词过滤和单字过滤。每个步骤都有其设计考虑：正则表达式保留中英数字和空格，jieba分词处理中文特有的歧义问题，停用词表过滤高频低义词，单字过滤则进一步精简输出。这个流程平衡了信息保留和噪声去除，适用于大多数中文NLP任务。</p>
<p><strong>设计思路</strong>：采用管道式设计，每个步骤的输出作为下一步的输入，便于调试和扩展。停用词使用集合（set）而非列表，查找时间复杂度从
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1752,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> 降到 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mn" transform="translate(1152,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(1652,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>，提升处理速度。单字过滤是可选的，因为单字词（如"我"、"你"）在某些任务中可能有用，但在关键词提取、主题建模等任务中通常噪声较大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_chinese</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    中文文本预处理函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    功能：将原始中文文本转换为清洗后的词列表，去除标点、停用词和单字词</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        text (str): 原始中文文本，可能包含标点、数字、英文等混合内容</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        list[str]: 预处理后的词列表，每个元素是一个词（长度&gt;=2）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    处理流程：</span></span><br><span class="line"><span class="string">        1. 文本清洗：去除标点和特殊字符，保留中文、英文、数字和空格</span></span><br><span class="line"><span class="string">        2. 分词：使用jieba将文本切分为词序列</span></span><br><span class="line"><span class="string">        3. 停用词过滤：去除高频低义词和空白词</span></span><br><span class="line"><span class="string">        4. 单字过滤：去除长度小于2的词（可选步骤）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    示例：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; text = "我在北京的清华大学学习自然语言处理，这是一门很有趣的课程！"</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; preprocess_chinese(text)</span></span><br><span class="line"><span class="string">        ['北京', '清华大学', '学习', '自然语言处理', '一门', '有趣', '课程']</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 步骤1：去除标点和特殊字符</span></span><br><span class="line">    <span class="comment"># 正则表达式 [^\u4e00-\u9fa5a-zA-Z0-9\s] 的含义：</span></span><br><span class="line">    <span class="comment">#   - ^ 表示取反（不匹配后面的字符集）</span></span><br><span class="line">    <span class="comment">#   - \u4e00-\u9fa5 是中文字符的Unicode范围（CJK统一汉字）</span></span><br><span class="line">    <span class="comment">#   - a-zA-Z 匹配所有英文字母（大小写）</span></span><br><span class="line">    <span class="comment">#   - 0-9 匹配数字</span></span><br><span class="line">    <span class="comment">#   - \s 匹配空白字符（空格、制表符等）</span></span><br><span class="line">    <span class="comment">#   整体含义：保留中英数字和空格，删除其他所有字符（包括标点、emoji等）</span></span><br><span class="line">    text = re.sub(<span class="string">r'[^\u4e00-\u9fa5a-zA-Z0-9\s]'</span>, <span class="string">''</span>, text)</span><br><span class="line">    <span class="comment"># 结果示例："我在北京的清华大学学习自然语言处理这是一门很有趣的课程"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤2：中文分词</span></span><br><span class="line">    <span class="comment"># jieba.lcut() 返回词列表，使用精确模式（默认）</span></span><br><span class="line">    <span class="comment"># 分词算法：基于前缀词典和动态规划，处理歧义切分</span></span><br><span class="line">    <span class="comment"># 例如："自然语言处理"会被识别为一个整体，而不是"自然"+"语言"+"处理"</span></span><br><span class="line">    tokens = jieba.lcut(text)</span><br><span class="line">    <span class="comment"># 结果示例：['我', '在', '北京', '的', '清华大学', '学习', '自然语言处理', '这', '是', '一门', '很', '有趣', '的', '课程']</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤3：去除停用词和空白词</span></span><br><span class="line">    <span class="comment"># 停用词（stopwords）：高频但语义贡献小的词，如"的"、"了"、"在"等</span></span><br><span class="line">    <span class="comment"># 使用集合（set）而非列表，查找时间复杂度从O(n)降到O(1)</span></span><br><span class="line">    <span class="comment"># w.strip() 检查词是否为空字符串（去除首尾空白后）</span></span><br><span class="line">    stopwords = <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, <span class="string">'都'</span>])</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> w.strip()]</span><br><span class="line">    <span class="comment"># 过滤后：['北京', '清华大学', '学习', '自然语言处理', '这', '一门', '很', '有趣', '课程']</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤4：过滤单字词（可选步骤）</span></span><br><span class="line">    <span class="comment"># 单字词（如"这"、"很"）通常对语义贡献较小，过滤后可以：</span></span><br><span class="line">    <span class="comment">#   - 减少特征维度</span></span><br><span class="line">    <span class="comment">#   - 突出多字词（通常语义更明确）</span></span><br><span class="line">    <span class="comment"># 注意：某些任务（如情感分析）可能需要保留单字词，此时应注释掉这一步</span></span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt; <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 最终结果：['北京', '清华大学', '学习', '自然语言处理', '一门', '有趣', '课程']</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">text = <span class="string">"我在北京的清华大学学习自然语言处理，这是一门很有趣的课程！"</span></span><br><span class="line"><span class="built_in">print</span>(preprocess_chinese(text))</span><br><span class="line"><span class="comment"># 输出：['北京', '清华大学', '学习', '自然语言处理', '一门', '有趣', '课程']</span></span><br></pre></td></tr></table></figure>
<p><strong>深入解读：设计权衡与常见问题</strong></p>
<p>这个预处理流程看似简单，但每个步骤都涉及重要的设计决策。让我们深入分析：</p>
<p><strong>1. 正则表达式的选择</strong></p>
<p>正则 <code>[^\u4e00-\u9fa5a-zA-Z0-9\s]</code>
看似全面，但实际应用中可能遇到问题：</p>
<ul>
<li><strong>问题1：保留英文和数字是否必要？</strong>
如果任务是纯中文文本分析，可以改为
<code>[^\u4e00-\u9fa5\s]</code>，但现代中文文本常混有英文术语（如"NLP"、"AI"），保留它们可能更有价值。</li>
<li><strong>问题2：如何处理URL和邮箱？</strong>
当前正则会保留URL中的字符，导致"https://example.com"被切分为多个词。更好的做法是先识别并替换为特殊标记（如<code>&lt;URL&gt;</code>），再清洗。</li>
<li><strong>问题3：emoji和特殊符号</strong>
当前正则会删除emoji，但情感分析任务中emoji可能包含重要信息。需要根据任务调整。</li>
</ul>
<p><strong>2. 分词工具的选择</strong></p>
<p>jieba是最流行的中文分词工具，但并非唯一选择：</p>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 35%">
</colgroup>
<thead>
<tr>
<th>工具</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>jieba</td>
<td>速度快、易用、社区活跃</td>
<td>准确率中等，领域适应性一般</td>
<td>通用文本、快速原型</td>
</tr>
<tr>
<td>pkuseg</td>
<td>准确率高，有领域模型</td>
<td>速度较慢，需要下载模型</td>
<td>专业领域（医疗、新闻）</td>
</tr>
<tr>
<td>LAC</td>
<td>支持词性标注和NER</td>
<td>依赖百度生态</td>
<td>需要词性信息的任务</td>
</tr>
</tbody>
</table>
<p><strong>3. 停用词表的构建</strong></p>
<p>代码中使用了硬编码的停用词表，但实际应用中应该：</p>
<ul>
<li><strong>加载外部停用词表</strong>：使用标准停用词库（如哈工大停用词表、百度停用词表），包含数百到数千个词。</li>
<li><strong>任务相关调整</strong>：情感分析任务中，"不"、"没有"等否定词不应被过滤；问答任务中，"谁"、"什么"等疑问词应保留。</li>
<li><strong>动态构建</strong>：根据语料统计词频，自动识别高频低义词加入停用词表。</li>
</ul>
<p><strong>4. 单字过滤的权衡</strong></p>
<p>过滤单字词是一把双刃剑：</p>
<ul>
<li><strong>优点</strong>：减少噪声，突出多字词（通常语义更明确）</li>
<li><strong>缺点</strong>：可能丢失重要信息，如"好"、"坏"在情感分析中很重要</li>
</ul>
<p><strong>建议</strong>：对于关键词提取、主题建模等任务，过滤单字词；对于情感分析、文本分类等任务，保留单字词或使用更精细的过滤规则。</p>
<p><strong>5. 性能优化</strong></p>
<p>当前实现每次调用都创建停用词集合，可以优化为模块级变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模块级停用词集合（只创建一次）</span></span><br><span class="line">_STOPWORDS = <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, <span class="string">'都'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_chinese</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># ... 其他代码 ...</span></span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> _STOPWORDS <span class="keyword">and</span> w.strip()]</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p><strong>6. 常见问题排查</strong></p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 45%">
</colgroup>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>分词结果不理想</td>
<td>jieba词典未包含领域术语</td>
<td>使用<code>jieba.add_word()</code>添加自定义词</td>
</tr>
<tr>
<td>停用词过滤过度</td>
<td>停用词表包含任务相关词</td>
<td>根据任务调整停用词表</td>
</tr>
<tr>
<td>处理速度慢</td>
<td>文本过长或停用词表过大</td>
<td>使用生成器、批量处理、优化数据结构</td>
</tr>
<tr>
<td>单字词丢失</td>
<td>单字过滤过于激进</td>
<td>根据任务决定是否过滤单字词</td>
</tr>
</tbody>
</table>
<p><strong>7. 扩展建议</strong></p>
<p>实际项目中，可以考虑以下扩展：</p>
<ul>
<li><strong>文本规范化</strong>：繁体转简体（使用<code>opencc</code>库）、全角转半角</li>
<li><strong>数字处理</strong>：统一数字表示（如"100"、"一百"都转为"100"）</li>
<li><strong>英文处理</strong>：英文词转小写、词形还原</li>
<li><strong>新词识别</strong>：使用更先进的分词工具（如基于BERT的分词）识别新词</li>
<li><strong>错误处理</strong>：添加异常处理，处理空输入、None值等边界情况</li>
</ul>
<p>这个预处理流程是中文NLP的基础，理解每个步骤的设计考虑和权衡，有助于在实际项目中做出正确的选择。</p>
<h2 id="英文文本预处理nltk-与-spacy">英文文本预处理：NLTK 与 spaCy</h2>
<h3 id="nltknatural-language-toolkit">NLTK（Natural Language
Toolkit）</h3>
<p>经典的 NLP 工具包，适合学习和原型开发。</p>
<p><strong>安装与下载资源</strong>： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install nltk</span><br><span class="line">python -c <span class="string">"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"</span></span><br></pre></td></tr></table></figure></p>
<p><strong>完整预处理流程</strong>：</p>
<p>英文文本预处理相比中文更直接，因为单词之间有天然的空格分隔。但英文也有其复杂性：大小写、标点、缩写（如"I'm"）、词形变化（如"running"→"run"）等都需要处理。本代码实现了一个标准的英文预处理流程，使用NLTK库完成分词、停用词过滤和词形还原。</p>
<p><strong>核心步骤解析</strong>：小写化统一格式，word_tokenize处理缩写和标点，isalnum()过滤非字母数字字符，停用词过滤去除高频低义词，词形还原将词的不同形态统一为基础形式。这个流程适用于大多数英文NLP任务，但需要根据具体任务调整：情感分析可能需要保留标点（如"!!!"表示强调），命名实体识别不能小写化（"Apple"公司和"apple"水果不同）。</p>
<p><strong>设计考虑</strong>：使用词形还原而非词干化，因为词形还原返回真实单词，更适合下游任务。词性标注（pos='v'）确保动词正确还原，但实际应用中可能需要更复杂的词性标注流程。停用词使用集合而非列表，提升查找效率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_english</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    英文文本预处理函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    功能：将原始英文文本转换为清洗后的词列表，统一大小写、去除标点、停用词和词形还原</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        text (str): 原始英文文本，可能包含大小写混合、标点、缩写等</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        list[str]: 预处理后的词列表，每个元素是词形还原后的词</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    处理流程：</span></span><br><span class="line"><span class="string">        1. 小写化：统一转换为小写，消除大小写差异</span></span><br><span class="line"><span class="string">        2. 分词：使用NLTK的word_tokenize处理缩写和标点</span></span><br><span class="line"><span class="string">        3. 去除标点：只保留字母数字字符</span></span><br><span class="line"><span class="string">        4. 停用词过滤：去除高频低义词（如"the"、"is"、"a"）</span></span><br><span class="line"><span class="string">        5. 词形还原：将词的不同形态还原为基础形式（如"running"→"run"）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    示例：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; text = "I'm learning Natural Language Processing! It's amazing."</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; preprocess_english(text)</span></span><br><span class="line"><span class="string">        ['learning', 'natural', 'language', 'processing', 'amazing']</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    注意：</span></span><br><span class="line"><span class="string">        - 词形还原需要词性信息，这里简化为动词（pos='v'）</span></span><br><span class="line"><span class="string">        - 实际应用中应使用词性标注器（如nltk.pos_tag）获取准确词性</span></span><br><span class="line"><span class="string">        - 某些任务（如情感分析）可能需要保留标点和大小写</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 步骤1：小写化</span></span><br><span class="line">    <span class="comment"># 将所有字符转换为小写，统一格式</span></span><br><span class="line">    <span class="comment"># 优点：消除大小写差异（"Apple"和"apple"被视为同一词）</span></span><br><span class="line">    <span class="comment"># 缺点：丢失专有名词信息（"Apple"公司和"apple"水果无法区分）</span></span><br><span class="line">    <span class="comment"># 适用场景：文本分类、信息检索等不需要区分专有名词的任务</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    <span class="comment"># 结果示例："i'm learning natural language processing! it's amazing."</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤2：分词（Tokenization）</span></span><br><span class="line">    <span class="comment"># word_tokenize() 是NLTK的智能分词器，能够：</span></span><br><span class="line">    <span class="comment">#   - 处理缩写："I'm" → ["I", "'m"] 或 ["I'm"]（取决于版本）</span></span><br><span class="line">    <span class="comment">#   - 分离标点："amazing." → ["amazing", "."]</span></span><br><span class="line">    <span class="comment">#   - 处理数字和特殊字符</span></span><br><span class="line">    <span class="comment"># 相比简单的split()，word_tokenize更准确但速度较慢</span></span><br><span class="line">    tokens = word_tokenize(text)</span><br><span class="line">    <span class="comment"># 结果示例：["i", "'m", "learning", "natural", "language", "processing", "!", "it", "'s", "amazing", "."]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤3：去除标点和特殊字符</span></span><br><span class="line">    <span class="comment"># isalnum() 检查字符串是否只包含字母和数字</span></span><br><span class="line">    <span class="comment"># 过滤掉标点符号（如"!", ".", "'"）和纯标点token</span></span><br><span class="line">    <span class="comment"># 注意：这也会过滤掉纯数字（如"2024"），某些任务可能需要保留</span></span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w.isalnum()]</span><br><span class="line">    <span class="comment"># 过滤后：["i", "m", "learning", "natural", "language", "processing", "it", "s", "amazing"]</span></span><br><span class="line">    <span class="comment"># 注意："'m"被拆分为"m"，"'s"被拆分为"s"，这些会在停用词过滤时去除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤4：去除停用词（Stop Words Removal）</span></span><br><span class="line">    <span class="comment"># stopwords.words('english') 返回NLTK内置的英文停用词列表（约179个词）</span></span><br><span class="line">    <span class="comment"># 停用词包括：冠词（"a", "the"）、代词（"I", "you"）、助动词（"is", "are"）等</span></span><br><span class="line">    <span class="comment"># 使用集合（set）而非列表，查找时间复杂度从O(n)降到O(1)</span></span><br><span class="line">    <span class="comment"># 注意：某些任务（如情感分析）中，"not"、"no"等否定词不应被过滤</span></span><br><span class="line">    stop_words = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">    <span class="comment"># 过滤后：["learning", "natural", "language", "processing", "amazing"]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤5：词形还原（Lemmatization）</span></span><br><span class="line">    <span class="comment"># WordNetLemmatizer 将词的不同形态还原为词典形式（lemma）</span></span><br><span class="line">    <span class="comment"># 例如："running" → "run", "better" → "good"（需要指定pos='a'）</span></span><br><span class="line">    <span class="comment"># pos='v' 指定词性为动词，但这里简化处理，实际应用中应使用词性标注</span></span><br><span class="line">    <span class="comment"># 词形还原 vs 词干化：</span></span><br><span class="line">    <span class="comment">#   - 词形还原：返回真实单词，更准确但需要词性信息</span></span><br><span class="line">    <span class="comment">#   - 词干化：快速但可能返回不存在的词（如"studies" → "studi"）</span></span><br><span class="line">    lemmatizer = WordNetLemmatizer()</span><br><span class="line">    <span class="comment"># 注意：这里假设所有词都是动词，实际应用中应使用nltk.pos_tag()获取词性</span></span><br><span class="line">    tokens = [lemmatizer.lemmatize(w, pos=<span class="string">'v'</span>) <span class="keyword">for</span> w <span class="keyword">in</span> tokens]</span><br><span class="line">    <span class="comment"># 结果：["learn", "natural", "language", "process", "amaze"]</span></span><br><span class="line">    <span class="comment"># 注意："natural"和"language"是名词，pos='v'不会改变它们</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">text = <span class="string">"I'm learning Natural Language Processing! It's amazing."</span></span><br><span class="line"><span class="built_in">print</span>(preprocess_english(text))</span><br><span class="line"><span class="comment"># 输出：['learn', 'natural', 'language', 'process', 'amaze']</span></span><br><span class="line"><span class="comment"># 注意：实际输出可能因NLTK版本和词性标注不同而略有差异</span></span><br></pre></td></tr></table></figure>
<p><strong>深入解读：英文预处理的细节与优化</strong></p>
<p>英文预处理看似简单，但每个步骤都有其复杂性和权衡：</p>
<p><strong>1. 小写化的权衡</strong></p>
<p>小写化是最常见的预处理步骤，但并非总是合适：</p>
<ul>
<li><strong>应该小写化</strong>：文本分类、信息检索、主题建模等任务，其中"Apple"和"apple"应被视为相关</li>
<li><strong>不应小写化</strong>：命名实体识别（NER）、情感分析（"LOVE"和"love"语气不同）、某些专有名词相关的任务</li>
</ul>
<p><strong>改进方案</strong>：根据任务选择性地小写化，或使用更智能的方法（如保留首字母大写的词作为候选专有名词）。</p>
<p><strong>2. 分词器的选择</strong></p>
<p><code>word_tokenize()</code> 是NLTK的标准分词器，但还有其他选择：</p>
<table>
<thead>
<tr>
<th>分词器</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>word_tokenize</td>
<td>处理缩写和标点准确</td>
<td>速度较慢</td>
<td>通用文本</td>
</tr>
<tr>
<td>WhitespaceTokenizer</td>
<td>速度快</td>
<td>不处理缩写</td>
<td>简单文本</td>
</tr>
<tr>
<td>TweetTokenizer</td>
<td>处理社交媒体文本</td>
<td>需要单独导入</td>
<td>Twitter、微博等</td>
</tr>
<tr>
<td>spaCy tokenizer</td>
<td>速度快、准确</td>
<td>需要安装spaCy</td>
<td>生产环境</td>
</tr>
</tbody>
</table>
<p><strong>3. 标点处理的复杂性</strong></p>
<p>代码中使用 <code>isalnum()</code>
过滤标点，但这可能丢失重要信息：</p>
<ul>
<li><strong>问题1：情感标点</strong> "amazing!!!"
中的多个感叹号表达强烈情感，删除后丢失信息</li>
<li><strong>问题2：URL和邮箱</strong> "visit https://example.com"
中的URL会被拆分</li>
<li><strong>问题3：数字和单位</strong> "<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="17.195ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 7600 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1500,0)"><g data-mml-node="mo"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"/><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"/></g></g></g><g data-mml-node="mi" transform="translate(2050,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">会</text></g><g data-mml-node="mi" transform="translate(3050,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">被</text></g><g data-mml-node="mi" transform="translate(4050,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">拆</text></g><g data-mml-node="mi" transform="translate(5050,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">分</text></g><g data-mml-node="mi" transform="translate(6050,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">为</text></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7050,0)"><g data-mml-node="mo"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"/><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"/></g></g></g></g></g></svg></mjx-container></span>" 和
"100"</li>
</ul>
<p><strong>改进方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更精细的标点处理</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留URL和邮箱</span></span><br><span class="line">text = re.sub(<span class="string">r'http\S+|www\.\S+'</span>, <span class="string">'&lt;URL&gt;'</span>, text)</span><br><span class="line">text = re.sub(<span class="string">r'\S+@\S+'</span>, <span class="string">'&lt;EMAIL&gt;'</span>, text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留货币符号和数字</span></span><br><span class="line">tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w.isalnum() <span class="keyword">or</span> re.<span class="keyword">match</span>(<span class="string">r'^[\$€£¥]\d+'</span>, w)]</span><br></pre></td></tr></table></figure></p>
<p><strong>4. 停用词表的定制</strong></p>
<p>NLTK的默认停用词表包含179个词，但可能需要调整：</p>
<ul>
<li><strong>情感分析</strong>：保留否定词（"not", "no",
"never"）和程度词（"very", "extremely"）</li>
<li><strong>问答系统</strong>：保留疑问词（"what", "who", "where",
"when", "why", "how"）</li>
<li><strong>领域特定</strong>：某些领域的高频词可能不是通用停用词</li>
</ul>
<p><strong>改进方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义停用词表</span></span><br><span class="line">base_stopwords = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line"><span class="comment"># 移除不应过滤的词</span></span><br><span class="line">custom_stopwords = base_stopwords - {<span class="string">'not'</span>, <span class="string">'no'</span>, <span class="string">'never'</span>, <span class="string">'very'</span>}</span><br><span class="line"><span class="comment"># 添加领域特定停用词</span></span><br><span class="line">custom_stopwords.update({<span class="string">'said'</span>, <span class="string">'according'</span>, <span class="string">'reported'</span>})</span><br></pre></td></tr></table></figure></p>
<p><strong>5. 词形还原的准确性</strong></p>
<p>代码中简化了词性标注，但词形还原的准确性很大程度上依赖于词性：</p>
<ul>
<li><strong>问题</strong>：<code>lemmatize(w, pos='v')</code>
假设所有词都是动词，导致名词和形容词无法正确还原</li>
<li><strong>示例</strong>："better" 作为形容词应还原为 "good"，但
<code>pos='v'</code> 不会改变它</li>
</ul>
<p><strong>改进方案</strong>：使用词性标注器获取准确词性：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> pos_tag</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_wordnet_pos</span>(<span class="params">treebank_tag</span>):</span><br><span class="line">    <span class="string">"""将NLTK词性标签转换为WordNet词性标签"""</span></span><br><span class="line">    <span class="keyword">if</span> treebank_tag.startswith(<span class="string">'J'</span>):</span><br><span class="line">        <span class="keyword">return</span> wordnet.ADJ</span><br><span class="line">    <span class="keyword">elif</span> treebank_tag.startswith(<span class="string">'V'</span>):</span><br><span class="line">        <span class="keyword">return</span> wordnet.VERB</span><br><span class="line">    <span class="keyword">elif</span> treebank_tag.startswith(<span class="string">'N'</span>):</span><br><span class="line">        <span class="keyword">return</span> wordnet.NOUN</span><br><span class="line">    <span class="keyword">elif</span> treebank_tag.startswith(<span class="string">'R'</span>):</span><br><span class="line">        <span class="keyword">return</span> wordnet.ADV</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> wordnet.NOUN  <span class="comment"># 默认</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改进的词形还原</span></span><br><span class="line">pos_tags = pos_tag(tokens)</span><br><span class="line">tokens = [lemmatizer.lemmatize(w, pos=get_wordnet_pos(pos)) </span><br><span class="line">          <span class="keyword">for</span> w, pos <span class="keyword">in</span> pos_tags]</span><br></pre></td></tr></table></figure></p>
<p><strong>6. 性能优化</strong></p>
<p>当前实现在每次调用时都创建停用词集合和词形还原器，可以优化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模块级初始化（只执行一次）</span></span><br><span class="line">_STOPWORDS = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">_LEMMATIZER = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_english</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># ... 其他代码 ...</span></span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> _STOPWORDS]</span><br><span class="line">    tokens = [_LEMMATIZER.lemmatize(w, pos=<span class="string">'v'</span>) <span class="keyword">for</span> w <span class="keyword">in</span> tokens]</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p><strong>7. 常见问题与解决方案</strong></p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 45%">
</colgroup>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>缩写处理不当</td>
<td>word_tokenize可能将"I'm"拆分为["I", "'m"]</td>
<td>使用TweetTokenizer或自定义规则</td>
</tr>
<tr>
<td>词形还原不准确</td>
<td>未使用正确的词性</td>
<td>使用pos_tag()获取词性后再还原</td>
</tr>
<tr>
<td>专有名词丢失</td>
<td>小写化导致专有名词信息丢失</td>
<td>根据任务决定是否小写化，或使用NER识别专有名词</td>
</tr>
<tr>
<td>数字被过滤</td>
<td>isalnum()可能过滤某些数字格式</td>
<td>使用更精细的正则表达式</td>
</tr>
<tr>
<td>处理速度慢</td>
<td>词形还原和词性标注耗时</td>
<td>考虑使用词干化（更快）或批量处理</td>
</tr>
</tbody>
</table>
<p><strong>8. 与spaCy的对比</strong></p>
<p>spaCy是另一个流行的NLP库，预处理流程更简洁：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_with_spacy</span>(<span class="params">text</span>):</span><br><span class="line">    doc = nlp(text.lower())</span><br><span class="line">    tokens = [token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> doc </span><br><span class="line">              <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop <span class="keyword">and</span> <span class="keyword">not</span> token.is_punct <span class="keyword">and</span> token.is_alpha]</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure>
<p><strong>对比</strong>： -
<strong>NLTK</strong>：更灵活，适合学习和研究，但需要手动组合多个步骤 -
<strong>spaCy</strong>：更快速，一体化处理，但定制性稍弱</p>
<p>选择哪个取决于项目需求：快速原型用spaCy，需要精细控制用NLTK。</p>
<p><strong>9. 实际应用建议</strong></p>
<p>在实际项目中，预处理流程应该：</p>
<ol type="1">
<li><strong>根据任务调整</strong>：没有"一刀切"的预处理流程，必须根据具体任务定制</li>
<li><strong>保留中间结果</strong>：保存原始文本和每个步骤的结果，便于调试和回滚</li>
<li><strong>版本控制</strong>：记录预处理参数和版本，确保实验可复现</li>
<li><strong>性能监控</strong>：记录处理时间和内存使用，优化瓶颈步骤</li>
<li><strong>错误处理</strong>：处理空输入、None值、编码错误等边界情况</li>
</ol>
<p>这个预处理流程是英文NLP的基础，理解每个步骤的原理和权衡，能够帮助你在实际项目中做出正确的选择。</p>
<h3 id="spacy">spaCy</h3>
<p>工业级 NLP 库，速度快、功能强、支持多语言。</p>
<p><strong>安装与下载模型</strong>： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install spacy</span><br><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure></p>
<p><strong>使用 spaCy 预处理</strong>：</p>
<p>spaCy是工业级的NLP库，相比NLTK，它提供了更一体化、更快速的预处理流程。spaCy的核心优势在于将分词、词性标注、词形还原、命名实体识别等功能集成在一个管道中，通过一次处理完成所有任务。</p>
<p><strong>设计理念</strong>：spaCy采用管道（pipeline）架构，每个组件（tokenizer、tagger、parser、ner等）按顺序处理文档，中间结果自动传递。这种设计避免了NLTK中需要手动组合多个步骤的复杂性，同时通过Cython优化实现了更高的处理速度。</p>
<p><strong>核心优势</strong>：1）速度优势：spaCy用Cython编写，比纯Python的NLTK快10-100倍；2）一体化处理：一次调用完成分词、词性标注、词形还原等；3）丰富的语言模型：支持60+种语言，包括中文；4）生产就绪：API设计简洁，适合生产环境部署。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载英文模型</span></span><br><span class="line"><span class="comment"># en_core_web_sm: 小型英文模型（约12MB，速度快但准确率稍低）</span></span><br><span class="line"><span class="comment"># 其他选择：</span></span><br><span class="line"><span class="comment">#   - en_core_web_md: 中型模型（约40MB，包含词向量）</span></span><br><span class="line"><span class="comment">#   - en_core_web_lg: 大型模型（约560MB，最准确但速度慢）</span></span><br><span class="line">nlp = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line"><span class="comment"># 注意：首次使用需要先下载模型：python -m spacy download en_core_web_sm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_with_spacy</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用spaCy进行文本预处理</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    功能：一体化完成分词、词性标注、词形还原、停用词过滤</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        text (str): 原始文本</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        list[str]: 预处理后的词列表（词形还原后的词）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    处理流程：</span></span><br><span class="line"><span class="string">        1. nlp()处理文本，返回Doc对象（包含所有NLP信息）</span></span><br><span class="line"><span class="string">        2. 遍历每个token（词），提取词形还原形式</span></span><br><span class="line"><span class="string">        3. 过滤停用词、标点和非字母字符</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    spaCy的token属性：</span></span><br><span class="line"><span class="string">        - token.text: 原始文本</span></span><br><span class="line"><span class="string">        - token.lemma_: 词形还原形式</span></span><br><span class="line"><span class="string">        - token.pos_: 词性标签（如NOUN, VERB）</span></span><br><span class="line"><span class="string">        - token.is_stop: 是否为停用词</span></span><br><span class="line"><span class="string">        - token.is_punct: 是否为标点</span></span><br><span class="line"><span class="string">        - token.is_alpha: 是否为字母字符</span></span><br><span class="line"><span class="string">        - token.ent_type_: 命名实体类型（如果有）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 小写化文本（spaCy保留原始大小写，需要手动处理）</span></span><br><span class="line">    text_lower = text.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># nlp()处理文本，返回Doc对象</span></span><br><span class="line">    <span class="comment"># Doc对象包含：tokens（词列表）、sentences（句子列表）、ents（命名实体）等</span></span><br><span class="line">    <span class="comment"># 处理过程包括：分词、词性标注、依存句法分析、命名实体识别等</span></span><br><span class="line">    doc = nlp(text_lower)</span><br><span class="line">    <span class="comment"># doc是Doc对象，可以迭代获取每个token</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 列表推导式：提取词形还原形式，过滤停用词、标点和非字母字符</span></span><br><span class="line">    tokens = [</span><br><span class="line">        token.lemma_  <span class="comment"># 词形还原形式（如"learning" → "learn"）</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> doc  <span class="comment"># 遍历文档中的每个token</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop      <span class="comment"># 不是停用词</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> token.is_punct    <span class="comment"># 不是标点符号</span></span><br><span class="line">        <span class="keyword">and</span> token.is_alpha        <span class="comment"># 是字母字符（过滤数字等）</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 结果：['learn', 'natural', 'language', 'process', 'amaze']</span></span><br><span class="line">    <span class="comment"># 注意："processing"被还原为"process"，"amazing"被还原为"amaze"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">text = <span class="string">"I'm learning Natural Language Processing! It's amazing."</span></span><br><span class="line">result = preprocess_with_spacy(text)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># 输出：['learn', 'natural', 'language', 'process', 'amaze']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== spaCy的高级功能 ==========</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 词性标注（Part-of-Speech Tagging）</span></span><br><span class="line">doc = nlp(<span class="string">"I'm learning Natural Language Processing!"</span>)</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"<span class="subst">{token.text:<span class="number">15</span>}</span> <span class="subst">{token.pos_:<span class="number">10</span>}</span> <span class="subst">{token.tag_:<span class="number">10</span>}</span> <span class="subst">{token.lemma_:<span class="number">15</span>}</span>"</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># I               PRON       PRP        I               </span></span><br><span class="line"><span class="comment"># 'm              AUX        VBP        be              </span></span><br><span class="line"><span class="comment"># learning        VERB       VBG        learn           </span></span><br><span class="line"><span class="comment"># Natural         ADJ        JJ         natural         </span></span><br><span class="line"><span class="comment"># Language        NOUN       NN         language        </span></span><br><span class="line"><span class="comment"># Processing      NOUN       NN         processing      </span></span><br><span class="line"><span class="comment"># !               PUNCT      .          !               </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 命名实体识别（Named Entity Recognition）</span></span><br><span class="line">text = <span class="string">"Apple is looking at buying U.K. startup for $1 billion"</span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n命名实体识别:"</span>)</span><br><span class="line"><span class="keyword">for</span> ent <span class="keyword">in</span> doc.ents:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"<span class="subst">{ent.text:<span class="number">15</span>}</span> <span class="subst">{ent.label_:<span class="number">10</span>}</span> <span class="subst">{spacy.explain(ent.label_)}</span>"</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># Apple           ORG        Companies, agencies, institutions</span></span><br><span class="line"><span class="comment"># U.K.            GPE        Countries, cities, states</span></span><br><span class="line"><span class="comment"># $1 billion      MONEY      Monetary values</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 依存句法分析（Dependency Parsing）</span></span><br><span class="line">doc = nlp(<span class="string">"The cat sat on the mat"</span>)</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"<span class="subst">{token.text:<span class="number">10</span>}</span> <span class="subst">{token.dep_:<span class="number">10</span>}</span> <span class="subst">{token.head.text}</span>"</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># The        det        cat</span></span><br><span class="line"><span class="comment"># cat        nsubj      sat</span></span><br><span class="line"><span class="comment"># sat        ROOT       sat</span></span><br><span class="line"><span class="comment"># on         prep       sat</span></span><br><span class="line"><span class="comment"># the        det        mat</span></span><br><span class="line"><span class="comment"># mat        pobj       on</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 批量处理（提高效率）</span></span><br><span class="line">texts = [</span><br><span class="line">    <span class="string">"I'm learning NLP"</span>,</span><br><span class="line">    <span class="string">"Natural language processing is amazing"</span>,</span><br><span class="line">    <span class="string">"spaCy is fast and efficient"</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># nlp.pipe()返回生成器，批量处理文本</span></span><br><span class="line"><span class="comment"># batch_size: 每批处理的文本数量</span></span><br><span class="line"><span class="comment"># n_process: 并行进程数（需要先调用nlp.disable_pipes()禁用某些组件）</span></span><br><span class="line">docs = <span class="built_in">list</span>(nlp.pipe(texts, batch_size=<span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">    tokens = [token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> doc <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop <span class="keyword">and</span> token.is_alpha]</span><br><span class="line">    <span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure>
<p><strong>深入解读：spaCy的设计哲学与性能优化</strong></p>
<p>spaCy的设计哲学是"生产优先"，这体现在其架构和API设计的各个方面：</p>
<p><strong>1. 管道架构的优势</strong></p>
<p>spaCy使用管道（pipeline）架构，组件按顺序处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前管道的组件</span></span><br><span class="line"><span class="built_in">print</span>(nlp.pipe_names)</span><br><span class="line"><span class="comment"># 输出：['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用某些组件以提高速度（如果不需要）</span></span><br><span class="line">nlp = spacy.load(<span class="string">"en_core_web_sm"</span>, disable=[<span class="string">'parser'</span>, <span class="string">'ner'</span>])</span><br><span class="line"><span class="comment"># 只保留分词、词性标注和词形还原，速度提升2-3倍</span></span><br></pre></td></tr></table></figure>
<p><strong>2. 模型选择策略</strong></p>
<p>spaCy提供不同大小的模型，需要权衡速度和准确率：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>大小</th>
<th>速度</th>
<th>准确率</th>
<th>包含内容</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>sm</strong></td>
<td>~12MB</td>
<td>最快</td>
<td>中等</td>
<td>基础NLP组件</td>
<td>快速原型、大规模处理</td>
</tr>
<tr>
<td><strong>md</strong></td>
<td>~40MB</td>
<td>中等</td>
<td>较高</td>
<td>sm + 词向量</td>
<td>需要词向量的任务</td>
</tr>
<tr>
<td><strong>lg</strong></td>
<td>~560MB</td>
<td>较慢</td>
<td>最高</td>
<td>md + 更大词向量</td>
<td>追求最佳性能</td>
</tr>
</tbody>
</table>
<p><strong>3. 性能优化技巧</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 技巧1：批量处理（比循环快10-100倍）</span></span><br><span class="line">texts = [<span class="string">"文本1"</span>, <span class="string">"文本2"</span>, ...]</span><br><span class="line">docs = <span class="built_in">list</span>(nlp.pipe(texts, batch_size=<span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 技巧2：禁用不需要的组件</span></span><br><span class="line">nlp = spacy.load(<span class="string">"en_core_web_sm"</span>, disable=[<span class="string">'parser'</span>, <span class="string">'ner'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 技巧3：使用多进程（需要先禁用某些组件）</span></span><br><span class="line">docs = <span class="built_in">list</span>(nlp.pipe(texts, n_process=<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 技巧4：只处理需要的属性（减少内存）</span></span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> nlp.pipe(texts):</span><br><span class="line">    <span class="comment"># 只访问需要的属性，避免加载所有信息</span></span><br><span class="line">    tokens = [token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc]</span><br></pre></td></tr></table></figure>
<p><strong>4. 中文支持</strong></p>
<p>spaCy支持中文，但需要下载中文模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载中文模型</span></span><br><span class="line"><span class="comment"># python -m spacy download zh_core_web_sm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">"zh_core_web_sm"</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"我在北京的清华大学学习自然语言处理"</span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文分词</span></span><br><span class="line">tokens = [token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc]</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br><span class="line"><span class="comment"># 输出：['我', '在', '北京', '的', '清华大学', '学习', '自然语言处理']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词性标注</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"<span class="subst">{token.text}</span> <span class="subst">{token.pos_}</span>"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>5. 与NLTK的详细对比</strong></p>
<table>
<thead>
<tr>
<th>特性</th>
<th>NLTK</th>
<th>spaCy</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>速度</strong></td>
<td>慢（纯Python）</td>
<td>快（Cython优化）</td>
</tr>
<tr>
<td><strong>API设计</strong></td>
<td>函数式，需要手动组合</td>
<td>面向对象，一体化</td>
</tr>
<tr>
<td><strong>模型</strong></td>
<td>需要单独下载数据包</td>
<td>模型包含所有组件</td>
</tr>
<tr>
<td><strong>定制性</strong></td>
<td>高（可以精细控制）</td>
<td>中等（通过配置）</td>
</tr>
<tr>
<td><strong>学习曲线</strong></td>
<td>陡峭（需要理解多个模块）</td>
<td>平缓（API简洁）</td>
</tr>
<tr>
<td><strong>生产部署</strong></td>
<td>需要较多配置</td>
<td>开箱即用</td>
</tr>
<tr>
<td><strong>社区支持</strong></td>
<td>学术导向</td>
<td>工业导向</td>
</tr>
</tbody>
</table>
<p><strong>6. 常见问题与解决方案</strong></p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 45%">
</colgroup>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型下载失败</td>
<td>网络问题或权限问题</td>
<td>使用镜像源或手动下载</td>
</tr>
<tr>
<td>内存占用大</td>
<td>加载了大型模型</td>
<td>使用sm模型或禁用不需要的组件</td>
</tr>
<tr>
<td>处理速度慢</td>
<td>未使用批量处理</td>
<td>使用nlp.pipe()批量处理</td>
</tr>
<tr>
<td>中文分词不准</td>
<td>中文模型较小</td>
<td>使用jieba或pkuseg进行分词，spaCy处理其他任务</td>
</tr>
<tr>
<td>自定义规则</td>
<td>默认规则不满足需求</td>
<td>使用Matcher或PhraseMatcher添加规则</td>
</tr>
</tbody>
</table>
<p><strong>7. 实际应用建议</strong></p>
<ol type="1">
<li><strong>模型选择</strong>：快速原型用sm，生产环境根据需求选择md或lg</li>
<li><strong>批量处理</strong>：始终使用<code>nlp.pipe()</code>而非循环调用<code>nlp()</code></li>
<li><strong>组件禁用</strong>：不需要的功能（如NER、parser）应禁用以提升速度</li>
<li><strong>内存管理</strong>：处理大规模数据时，使用生成器而非列表</li>
<li><strong>错误处理</strong>：spaCy可能对某些文本抛出异常，需要try-except处理</li>
</ol>
<p><strong>8. 进阶用法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义分词规则</span></span><br><span class="line"><span class="keyword">from</span> spacy.lang.en <span class="keyword">import</span> English</span><br><span class="line"><span class="keyword">from</span> spacy.tokenizer <span class="keyword">import</span> Tokenizer</span><br><span class="line"></span><br><span class="line">nlp = English()</span><br><span class="line"><span class="comment"># 自定义tokenizer，不拆分URL</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">custom_tokenizer</span>(<span class="params">nlp</span>):</span><br><span class="line">    <span class="keyword">return</span> Tokenizer(nlp.vocab, rules={})</span><br><span class="line"></span><br><span class="line">nlp.tokenizer = custom_tokenizer(nlp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Matcher提取模式</span></span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"></span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line">pattern = [{<span class="string">"LOWER"</span>: <span class="string">"natural"</span>}, {<span class="string">"LOWER"</span>: <span class="string">"language"</span>}, {<span class="string">"LOWER"</span>: <span class="string">"processing"</span>}]</span><br><span class="line">matcher.add(<span class="string">"NLP"</span>, [pattern])</span><br><span class="line"></span><br><span class="line">doc = nlp(<span class="string">"I'm learning Natural Language Processing"</span>)</span><br><span class="line">matches = matcher(doc)</span><br><span class="line"><span class="keyword">for</span> match_id, start, end <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="built_in">print</span>(doc[start:end])</span><br></pre></td></tr></table></figure>
<p>spaCy是生产环境NLP的首选工具，理解其设计理念和使用技巧，能够显著提升开发效率和系统性能。</p>
<h2 id="文本表示从词袋到-tf-idf">文本表示：从词袋到 TF-IDF</h2>
<p>预处理后的文本是词的列表，但机器学习模型需要数值输入。如何把文本转为向量？</p>
<h3 id="词袋模型bag-of-wordsbow">词袋模型（Bag of Words，BoW）</h3>
<p>把文本表示为词频向量，忽略语序。</p>
<p><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">文档1: "我 喜欢 机器学习"</span><br><span class="line">文档2: "我 喜欢 深度学习"</span><br><span class="line">文档3: "机器学习 和 深度学习 很有趣"</span><br><span class="line"></span><br><span class="line">词汇表: ["我", "喜欢", "机器学习", "深度学习", "和", "很有趣"]</span><br><span class="line"></span><br><span class="line">向量表示:</span><br><span class="line">文档1: [1, 1, 1, 0, 0, 0]</span><br><span class="line">文档2: [1, 1, 0, 1, 0, 0]</span><br><span class="line">文档3: [0, 0, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure></p>
<p><strong>Python 实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">"我 喜欢 机器学习"</span>,</span><br><span class="line">    <span class="string">"我 喜欢 深度学习"</span>,</span><br><span class="line">    <span class="string">"机器学习 和 深度学习 很有趣"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">X = vectorizer.fit_transform(corpus)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"词汇表:"</span>, vectorizer.get_feature_names_out())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"向量表示:\n"</span>, X.toarray())</span><br></pre></td></tr></table></figure></p>
<p><strong>局限性</strong>：</p>
<ul>
<li>高频常见词（"的"、"是"）权重过高</li>
<li>无法区分重要词和普通词</li>
</ul>
<h3 id="tf-idfterm-frequency---inverse-document-frequency">TF-IDF（Term
Frequency - Inverse Document Frequency）</h3>
<p>TF-IDF 根据词的重要性加权：</p>
<ul>
<li><p><strong>TF（词频）</strong>：词在文档中出现的次数 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="34.238ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 15133.2 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(1375,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1764,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2125,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2569.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3089.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3756.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(4812.2,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text><path data-c="A0" d="" transform="translate(1000,0)"/></g><g data-mml-node="mi" transform="translate(1250,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mtext" transform="translate(1611,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">在</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><path data-c="A0" d="" transform="translate(3250,0)"/></g><g data-mml-node="mi" transform="translate(5111,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mtext" transform="translate(5631,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">中</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">次</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g><g data-mml-node="mrow" transform="translate(1650.5,-710)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><path data-c="A0" d="" transform="translate(2000,0)"/></g><g data-mml-node="mi" transform="translate(2250,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mtext" transform="translate(2770,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g><rect width="10081" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p></li>
<li><p><strong>IDF（逆文档频率）</strong>：词在所有文档中的稀有程度
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="35.563ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 15718.7 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(1778,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(2167,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2528,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3194.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(4250.6,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(5528.6,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(5695.2,0)"><g data-mml-node="mtext" transform="translate(3011.7,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">包</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">含</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><path data-c="A0" d="" transform="translate(3000,0)"/></g><g data-mml-node="mi" transform="translate(3250,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mtext" transform="translate(3611,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(8083.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(9083.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><rect width="9783.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p></li>
<li><p><strong>TF-IDF</strong>： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="32.899ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 14541.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/><path data-c="2D" d="M11 179V252H277V179H11Z" transform="translate(1375,0)"/><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z" transform="translate(1708,0)"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(2069,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(2833,0)"/></g><g data-mml-node="mo" transform="translate(3486,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(3875,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(4236,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(4680.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(5200.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(5867.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mtext" transform="translate(6923.2,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(8298.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(8687.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(9048.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(9492.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(10012.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(10624.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mtext" transform="translate(11624.3,0)"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(13402.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(13791.3,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(14152.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p></li>
</ul>
<p><strong>直觉解释</strong>：</p>
<ul>
<li>如果词在某篇文档中频繁出现（高 TF），且在其他文档中很少见（高
IDF），则它对该文档很重要</li>
<li>"的"、"是"在所有文档中都常见（低 IDF），所以权重低</li>
<li>"量子计算"只在少数文档出现（高 IDF），权重高</li>
</ul>
<p><strong>Python 实现</strong>：</p>
<p>TF-IDF是文本特征提取的核心方法，它将文档转换为数值向量，其中每个维度对应一个词，值表示该词对文档的重要性。scikit-learn的TfidfVectorizer封装了完整的TF-IDF计算流程，包括词频统计、IDF计算、归一化等步骤。</p>
<p><strong>核心原理回顾</strong>：TF（词频）衡量词在文档中的重要性，IDF（逆文档频率）衡量词的区分度。TF-IDF
= TF ×
IDF，同时考虑局部重要性和全局稀有性。scikit-learn的实现还包含L2归一化，确保不同长度的文档可以公平比较。</p>
<p><strong>设计考虑</strong>：TfidfVectorizer默认使用平滑的IDF计算（避免除零），支持n-gram特征（捕捉短语），可以限制特征数量（max_features）控制维度。稀疏矩阵存储（CSR格式）节省内存，适合大规模语料。fit_transform()方法先学习词汇表和IDF值，再转换文档，这是典型的sklearn模式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例语料库（文档集合）</span></span><br><span class="line"><span class="comment"># 注意：文本已经分词并用空格分隔（这是TfidfVectorizer的输入格式要求）</span></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">"我 喜欢 机器学习"</span>,      <span class="comment"># 文档1</span></span><br><span class="line">    <span class="string">"我 喜欢 深度学习"</span>,      <span class="comment"># 文档2</span></span><br><span class="line">    <span class="string">"机器学习 和 深度学习 很有趣"</span>  <span class="comment"># 文档3</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TF-IDF向量化器</span></span><br><span class="line"><span class="comment"># TfidfVectorizer的参数说明：</span></span><br><span class="line"><span class="comment">#   - token_pattern: 默认r"(?u)\b\w\w+\b"，匹配2个或更多字符的词</span></span><br><span class="line"><span class="comment">#   - max_features: 限制特征数量，None表示使用所有词</span></span><br><span class="line"><span class="comment">#   - min_df: 词的最小文档频率，低于此值的词被忽略（可以是整数或比例）</span></span><br><span class="line"><span class="comment">#   - max_df: 词的最大文档频率，高于此值的词被忽略（用于过滤停用词）</span></span><br><span class="line"><span class="comment">#   - ngram_range: n-gram范围，默认(1,1)只使用单词，可以设置为(1,2)使用单词和双词</span></span><br><span class="line"><span class="comment">#   - norm: 归一化方式，默认'l2'（L2范数归一化）</span></span><br><span class="line"><span class="comment">#   - smooth_idf: 是否平滑IDF，默认True（避免除零，IDF = log((N+1)/(df+1)) + 1）</span></span><br><span class="line">vectorizer = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤1：fit - 学习词汇表和IDF值</span></span><br><span class="line"><span class="comment"># 这一步会：</span></span><br><span class="line"><span class="comment">#   1. 构建词汇表（所有唯一的词）</span></span><br><span class="line"><span class="comment">#   2. 统计每个词在每个文档中的出现次数（TF）</span></span><br><span class="line"><span class="comment">#   3. 统计每个词出现在多少个文档中（文档频率DF）</span></span><br><span class="line"><span class="comment">#   4. 计算每个词的IDF值：IDF = log((N+1)/(df+1)) + 1（smooth_idf=True时）</span></span><br><span class="line"><span class="comment">#   其中N是文档总数，df是包含该词的文档数</span></span><br><span class="line">vectorizer.fit(corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤2：transform - 将文档转换为TF-IDF向量</span></span><br><span class="line"><span class="comment"># 这一步会：</span></span><br><span class="line"><span class="comment">#   1. 对每个文档，计算每个词的TF（词频）</span></span><br><span class="line"><span class="comment">#   2. 将TF与IDF相乘得到TF-IDF值</span></span><br><span class="line"><span class="comment">#   3. 对每个文档的向量进行L2归一化：v_norm = v / ||v||_2</span></span><br><span class="line"><span class="comment">#   4. 返回稀疏矩阵（CSR格式），形状为(文档数, 词汇表大小)</span></span><br><span class="line">X = vectorizer.fit_transform(corpus)</span><br><span class="line"><span class="comment"># X是稀疏矩阵，形状：(3, 6) - 3个文档，6个词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看TF-IDF矩阵（转换为密集矩阵，小数据集可以这样做）</span></span><br><span class="line"><span class="comment"># toarray()将稀疏矩阵转换为numpy数组</span></span><br><span class="line"><span class="comment"># 每一行是一个文档的TF-IDF向量，每一列对应一个词</span></span><br><span class="line">tfidf_matrix = X.toarray()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"TF-IDF 矩阵:\n"</span>, tfidf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n特征名（词汇表）:"</span>, vectorizer.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出示例：</span></span><br><span class="line"><span class="comment"># TF-IDF 矩阵:</span></span><br><span class="line"><span class="comment"># [[0.57735027 0.57735027 0.57735027 0.         0.         0.        ]  # 文档1</span></span><br><span class="line"><span class="comment">#  [0.57735027 0.57735027 0.         0.57735027 0.         0.        ]  # 文档2</span></span><br><span class="line"><span class="comment">#  [0.         0.         0.40824829 0.40824829 0.57735027 0.57735027]]  # 文档3</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 特征名: ['我', '喜欢', '机器学习', '深度学习', '和', '很有趣']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解释矩阵的含义：</span></span><br><span class="line"><span class="comment"># - 文档1（"我 喜欢 机器学习"）：</span></span><br><span class="line"><span class="comment">#   * "我"、"喜欢"、"机器学习"的TF-IDF值都是0.577（约等于1/√3）</span></span><br><span class="line"><span class="comment">#   * 这是因为L2归一化：三个词权重相等，归一化后每个都是1/√3</span></span><br><span class="line"><span class="comment"># - 文档2（"我 喜欢 深度学习"）：</span></span><br><span class="line"><span class="comment">#   * "我"、"喜欢"、"深度学习"的TF-IDF值都是0.577</span></span><br><span class="line"><span class="comment"># - 文档3（"机器学习 和 深度学习 很有趣"）：</span></span><br><span class="line"><span class="comment">#   * "机器学习"和"深度学习"的TF-IDF值较低（0.408），因为它们在多个文档中出现（IDF较低）</span></span><br><span class="line"><span class="comment">#   * "和"和"很有趣"的TF-IDF值较高（0.577），因为它们只在文档3中出现（IDF较高）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看IDF值（逆文档频率）</span></span><br><span class="line"><span class="comment"># idf_属性存储每个词的IDF值，形状为(词汇表大小,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\nIDF值:"</span>)</span><br><span class="line"><span class="keyword">for</span> word, idf <span class="keyword">in</span> <span class="built_in">zip</span>(vectorizer.get_feature_names_out(), vectorizer.idf_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"  <span class="subst">{word}</span>: <span class="subst">{idf:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看词汇表到索引的映射</span></span><br><span class="line"><span class="comment"># vocabulary_是一个字典，键是词，值是在特征矩阵中的列索引</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n词汇表索引映射:"</span>, vectorizer.vocabulary_)</span><br><span class="line"><span class="comment"># 输出：{'我': 0, '喜欢': 1, '机器学习': 2, '深度学习': 3, '和': 4, '很有趣': 5}</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对新文档进行转换（使用已学习的向量化器）</span></span><br><span class="line">new_doc = [<span class="string">"我 喜欢 自然语言处理"</span>]</span><br><span class="line">new_vector = vectorizer.transform(new_doc)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n新文档的TF-IDF向量:"</span>, new_vector.toarray())</span><br><span class="line"><span class="comment"># 注意：新文档中的"自然语言处理"不在词汇表中，会被忽略</span></span><br><span class="line"><span class="comment"># 输出：[[0.70710678 0.70710678 0.         0.         0.         0.        ]]</span></span><br><span class="line"><span class="comment"># 只有"我"和"喜欢"有值，且经过L2归一化</span></span><br></pre></td></tr></table></figure>
<p><strong>深入解读：TF-IDF的数学原理与实现细节</strong></p>
<p>TF-IDF看似简单，但scikit-learn的实现包含许多细节和优化：</p>
<p><strong>1. TF-IDF公式的变体</strong></p>
<p>scikit-learn使用的TF-IDF公式与标准公式略有不同：</p>
<p><strong>标准公式</strong>： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="35.09ex" height="5.247ex" role="img" focusable="false" viewbox="0 -1359 15510 2319"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/><path data-c="2D" d="M11 179V252H277V179H11Z" transform="translate(1375,0)"/><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z" transform="translate(1708,0)"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(2069,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(2833,0)"/></g><g data-mml-node="mo" transform="translate(3486,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(3875,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(4236,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(4680.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(5200.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(5867.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mtext" transform="translate(6923.2,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(8298.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(8687.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(9048.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(9492.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(10012.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(10624.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mi" transform="translate(11624.3,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(12902.3,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(13069,0)"><g data-mml-node="mi" transform="translate(776.5,676)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mtext"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z"/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(556,0)"/></g><g data-mml-node="mo" transform="translate(862,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1251,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(1612,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><rect width="2201" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p>
<p><strong>scikit-learn公式</strong>（smooth_idf=True时）： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="46.215ex" height="5.451ex" role="img" focusable="false" viewbox="0 -1449.5 20426.9 2409.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/><path data-c="2D" d="M11 179V252H277V179H11Z" transform="translate(1375,0)"/><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z" transform="translate(1708,0)"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(2069,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(2833,0)"/></g><g data-mml-node="mo" transform="translate(3486,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(3875,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(4236,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(4680.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(5200.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(5867.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mtext" transform="translate(6923.2,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(8298.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(8687.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(9048.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(9492.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(10012.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(10624.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mrow" transform="translate(11624.3,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"/></g><g data-mml-node="mi" transform="translate(736,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(2014,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(2180.7,0)"><g data-mml-node="mrow" transform="translate(776.5,676)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mo" transform="translate(1110.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(2110.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mtext"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z"/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(556,0)"/></g><g data-mml-node="mo" transform="translate(862,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1251,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(1612,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(2223.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(3223.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><rect width="3923.4" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(6566.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(7566.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(8066.6,0) translate(0 -0.5)"><path data-c="29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"/></g></g></g></g></svg></mjx-container></span></p>
<p><strong>区别</strong>： - <strong>平滑处理</strong>：<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.906ex" height="1.731ex" role="img" focusable="false" viewbox="0 -683 2610.4 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mo" transform="translate(1110.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(2110.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container></span> 和 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.424ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 3723.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z"/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(556,0)"/></g><g data-mml-node="mo" transform="translate(862,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1251,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(1612,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(2223.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(3223.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container></span> 避免除零错误 -
<strong>+1项</strong>：确保IDF值始终为正，即使词出现在所有文档中</p>
<p><strong>2. TF的计算方式</strong></p>
<p>scikit-learn默认使用<strong>子线性TF缩放</strong>（sublinear_tf=False时使用原始词频）：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="36.645ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 16197.2 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(1375,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1764,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2125,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2569.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3089.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3756.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(4812.2,0)"><g data-mml-node="mtext" transform="translate(220,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1000,0)"/><text data-variant="normal" transform="translate(1389,0) scale(1,-1)" font-size="884px" font-family="serif">在</text><text data-variant="normal" transform="translate(2389,0) scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(3389,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(4389,0)"/><text data-variant="normal" transform="translate(4945,0) scale(1,-1)" font-size="884px" font-family="serif">中</text><text data-variant="normal" transform="translate(5945,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(6945,0) scale(1,-1)" font-size="884px" font-family="serif">出</text><text data-variant="normal" transform="translate(7945,0) scale(1,-1)" font-size="884px" font-family="serif">现</text><text data-variant="normal" transform="translate(8945,0) scale(1,-1)" font-size="884px" font-family="serif">次</text><text data-variant="normal" transform="translate(9945,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mtext" transform="translate(2414.5,-710)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(2000,0)"/><text data-variant="normal" transform="translate(2556,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(3556,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(4556,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(5556,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><rect width="11145" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p>
<p>如果设置<code>sublinear_tf=True</code>，则使用： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="31.831ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 14069.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(1375,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1764,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2125,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2569.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3089.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3756.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(4812.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(5534.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(6534.7,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(7812.7,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(7812.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mtext" transform="translate(8201.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">原</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">始</text><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" transform="translate(2000,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(2722,0)"/></g><g data-mml-node="mo" transform="translate(11576.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(11965.7,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(12326.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(12771.3,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(13291.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(13680.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p>
<p>这可以降低高频词的影响，突出稀有词的重要性。</p>
<p><strong>3. L2归一化的作用</strong></p>
<p>默认情况下，TfidfVectorizer会对每个文档的向量进行L2归一化：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -4.118ex;" xmlns="http://www.w3.org/2000/svg" width="25.757ex" height="6.652ex" role="img" focusable="false" viewbox="0 -1120 11384.7 2940"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(556,0)"/><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1056,0)"/><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1448,0)"/></g></g></g><g data-mml-node="mo" transform="translate(2580.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(3636.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(938.3,676)"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mo"><path data-c="2225" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500,0)"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="msub" transform="translate(1107,0)"><g data-mml-node="mo"><path data-c="2225" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"/></g><g data-mml-node="mn" transform="translate(533,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g><rect width="2243.6" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(6397.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(7453.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1662.1,676)"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="msqrt" transform="translate(220,-1249)"><g transform="translate(1020,0)"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="mi" transform="translate(1089,-285.4) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="msubsup" transform="translate(1549.6,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mn" transform="translate(518,353.6) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mi" transform="translate(518,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(0,79)"><path data-c="221A" d="M1001 1150Q1017 1150 1020 1132Q1020 1127 741 244L460 -643Q453 -650 436 -650H424Q423 -647 423 -645T421 -640T419 -631T415 -617T408 -594T399 -560T385 -512T367 -448T343 -364T312 -259L203 119L138 41L111 67L212 188L264 248L472 -474L983 1140Q988 1150 1001 1150Z"/></g><rect width="2471.2" height="60" x="1020" y="1169"/></g><rect width="3691.2" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p>
<p><strong>为什么归一化？</strong> -
<strong>公平比较</strong>：不同长度的文档可以公平比较（否则长文档的向量模长更大）
-
<strong>余弦相似度</strong>：归一化后，向量内积等于余弦相似度，便于计算文档相似度
- <strong>数值稳定</strong>：避免向量模长过大导致的数值问题</p>
<p><strong>4. 稀疏矩阵的优势</strong></p>
<p><code>fit_transform()</code>返回的是<strong>稀疏矩阵</strong>（CSR格式），而非密集矩阵：</p>
<p><strong>优势</strong>： -
<strong>内存效率</strong>：TF-IDF矩阵通常非常稀疏（大部分元素为0），稀疏矩阵只存储非零元素
- <strong>计算效率</strong>：矩阵运算可以跳过零元素，加速计算</p>
<p><strong>示例</strong>：假设有1000个文档，词汇表大小10000，密集矩阵需要1000×10000×8字节=80MB，而稀疏矩阵可能只需要几MB。</p>
<p><strong>5. 参数调优建议</strong></p>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 26%">
<col style="width: 33%">
<col style="width: 20%">
</colgroup>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>调优建议</th>
<th>影响</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>max_features</code></td>
<td>None</td>
<td>根据任务设置（如1000-10000）</td>
<td>控制特征维度，防止过拟合</td>
</tr>
<tr>
<td><code>min_df</code></td>
<td>1</td>
<td>设置为2或0.01（比例）过滤罕见词</td>
<td>减少噪声特征</td>
</tr>
<tr>
<td><code>max_df</code></td>
<td>1.0</td>
<td>设置为0.8-0.95过滤停用词</td>
<td>自动过滤高频低义词</td>
</tr>
<tr>
<td><code>ngram_range</code></td>
<td>(1,1)</td>
<td>(1,2)捕捉短语</td>
<td>提升表达能力但增加维度</td>
</tr>
<tr>
<td><code>sublinear_tf</code></td>
<td>False</td>
<td>True降低高频词影响</td>
<td>突出稀有词</td>
</tr>
<tr>
<td><code>norm</code></td>
<td>'l2'</td>
<td>'l1'或None根据任务选择</td>
<td>影响向量分布</td>
</tr>
</tbody>
</table>
<p><strong>6. 常见问题与解决方案</strong></p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 45%">
</colgroup>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存溢出</td>
<td>词汇表过大或文档过多</td>
<td>使用max_features限制特征数，或使用HashingVectorizer</td>
</tr>
<tr>
<td>新文档中的词被忽略</td>
<td>词汇表固定，新词不在其中</td>
<td>使用HashingVectorizer或定期重新fit</td>
</tr>
<tr>
<td>TF-IDF值全为0</td>
<td>文档中的所有词都不在词汇表中</td>
<td>检查预处理流程，确保分词正确</td>
</tr>
<tr>
<td>计算速度慢</td>
<td>文档数量或词汇表过大</td>
<td>使用HashingVectorizer或增量学习</td>
</tr>
<tr>
<td>维度灾难</td>
<td>词汇表过大导致特征维度爆炸</td>
<td>使用max_features、min_df、max_df限制</td>
</tr>
</tbody>
</table>
<p><strong>7. 与其他方法的对比</strong></p>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 35%">
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TF-IDF</strong></td>
<td>简单、可解释、无需训练</td>
<td>无法捕捉语义、维度高</td>
<td>文本分类、信息检索</td>
</tr>
<tr>
<td><strong>Word2Vec</strong></td>
<td>捕捉语义、维度低</td>
<td>需要预训练或训练时间</td>
<td>文本相似度、语义分析</td>
</tr>
<tr>
<td><strong>BERT</strong></td>
<td>上下文相关、性能强</td>
<td>计算开销大、需要GPU</td>
<td>复杂NLP任务</td>
</tr>
<tr>
<td><strong>HashingVectorizer</strong></td>
<td>内存效率高、支持新词</td>
<td>不可解释、可能冲突</td>
<td>大规模流式数据</td>
</tr>
</tbody>
</table>
<p><strong>8. 实际应用示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更完整的TF-IDF配置示例</span></span><br><span class="line">vectorizer = TfidfVectorizer(</span><br><span class="line">    max_features=<span class="number">5000</span>,        <span class="comment"># 限制特征数量</span></span><br><span class="line">    min_df=<span class="number">2</span>,                 <span class="comment"># 词至少出现在2个文档中</span></span><br><span class="line">    max_df=<span class="number">0.8</span>,               <span class="comment"># 词最多出现在80%的文档中（过滤停用词）</span></span><br><span class="line">    ngram_range=(<span class="number">1</span>, <span class="number">2</span>),       <span class="comment"># 使用单词和双词</span></span><br><span class="line">    sublinear_tf=<span class="literal">True</span>,        <span class="comment"># 使用子线性TF缩放</span></span><br><span class="line">    norm=<span class="string">'l2'</span>,                <span class="comment"># L2归一化</span></span><br><span class="line">    smooth_idf=<span class="literal">True</span>           <span class="comment"># 平滑IDF</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理大规模语料时的内存优化</span></span><br><span class="line"><span class="comment"># 方法1：使用HashingVectorizer（不需要存储词汇表）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> HashingVectorizer</span><br><span class="line">hasher = HashingVectorizer(n_features=<span class="number">10000</span>, norm=<span class="string">'l2'</span>)</span><br><span class="line">X = hasher.transform(corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：分批处理</span></span><br><span class="line">chunk_size = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(corpus), chunk_size):</span><br><span class="line">    chunk = corpus[i:i+chunk_size]</span><br><span class="line">    X_chunk = vectorizer.transform(chunk)</span><br><span class="line">    <span class="comment"># 处理X_chunk...</span></span><br></pre></td></tr></table></figure>
<p><strong>9. 性能优化技巧</strong></p>
<ol type="1">
<li><strong>预处理优化</strong>：在向量化前完成所有文本预处理，避免重复计算</li>
<li><strong>特征选择</strong>：使用<code>max_features</code>和<code>min_df</code>/<code>max_df</code>限制特征数量</li>
<li><strong>稀疏矩阵操作</strong>：使用scipy.sparse的矩阵运算，避免转换为密集矩阵</li>
<li><strong>并行处理</strong>：TfidfVectorizer支持n_jobs参数进行并行计算</li>
<li><strong>增量学习</strong>：对于流式数据，考虑使用<code>partial_fit()</code>方法</li>
</ol>
<p>TF-IDF是文本特征提取的经典方法，虽然简单但非常有效。理解其原理和实现细节，能够帮助你在实际项目中正确使用和调优。</p>
<h3 id="文本表示方法对比">文本表示方法对比</h3>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 35%">
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>词袋模型</td>
<td>简单、快速</td>
<td>丢失语序、稀疏、无法捕捉语义</td>
<td>文本分类、简单检索</td>
</tr>
<tr>
<td>TF-IDF</td>
<td>突出重要词</td>
<td>仍无法捕捉语义相似性</td>
<td>信息检索、关键词提取</td>
</tr>
<tr>
<td>Word2Vec</td>
<td>稠密向量、捕捉语义</td>
<td>无法处理一词多义</td>
<td>文本相似度、情感分析</td>
</tr>
<tr>
<td>BERT</td>
<td>上下文相关、语义理解强</td>
<td>计算开销大</td>
<td>复杂 NLP 任务、问答系统</td>
</tr>
</tbody>
</table>
<p>现代 NLP 任务更多使用 Word2Vec、BERT 等深度学习方法，但 TF-IDF
在简单任务中仍有用武之地。</p>
<h2 id="实战案例构建文本分类器">实战案例：构建文本分类器</h2>
<p>现在我们用前面学的预处理技术，构建一个完整的情感分类器。</p>
<h3 id="任务描述">任务描述</h3>
<p>判断电影评论的情感是正面还是负面。</p>
<h3 id="数据准备">数据准备</h3>
<p>使用 scikit-learn
自带的影评数据（实际项目中可替换为自己的数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的中文影评数据集</span></span><br><span class="line">reviews = [</span><br><span class="line">    <span class="string">"这部电影太精彩了，强烈推荐"</span>,</span><br><span class="line">    <span class="string">"剧情拖沓，演技尴尬，浪费时间"</span>,</span><br><span class="line">    <span class="string">"导演功力深厚，画面唯美，值得一看"</span>,</span><br><span class="line">    <span class="string">"烂片，看了十分钟就想离场"</span>,</span><br><span class="line">    <span class="string">"演员表现出色，剧情引人入胜"</span>,</span><br><span class="line">    <span class="string">"完全是烂俗套路，毫无新意"</span>,</span><br><span class="line">    <span class="string">"特效震撼，故事感人，五星好评"</span>,</span><br><span class="line">    <span class="string">"无聊至极，不推荐"</span>,</span><br><span class="line">    <span class="string">"这是我今年看过最好的电影"</span>,</span><br><span class="line">    <span class="string">"差评，根本不值票价"</span>,</span><br><span class="line">    <span class="string">"演技炸裂，每个镜头都是精心设计"</span>,</span><br><span class="line">    <span class="string">"剧情漏洞百出，逻辑混乱"</span>,</span><br><span class="line">    <span class="string">"音乐动人，情感真挚，催人泪下"</span>,</span><br><span class="line">    <span class="string">"浪费我两个小时，强烈不推荐"</span>,</span><br><span class="line">    <span class="string">"制作精良，诚意满满，良心之作"</span>,</span><br><span class="line">    <span class="string">"看完只想骂人，烂透了"</span>,</span><br><span class="line">    <span class="string">"笑点密集，轻松愉快，适合全家观看"</span>,</span><br><span class="line">    <span class="string">"尴尬癌都犯了，实在看不下去"</span>,</span><br><span class="line">    <span class="string">"细节丰富，伏笔精妙，值得二刷"</span>,</span><br><span class="line">    <span class="string">"毫无亮点，彻底失望"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 1=正面, 0=负面</span></span><br></pre></td></tr></table></figure>
<h3 id="预处理函数">预处理函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载停用词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_stopwords</span>():</span><br><span class="line">    stopwords = <span class="built_in">set</span>([</span><br><span class="line">        <span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, </span><br><span class="line">        <span class="string">'都'</span>, <span class="string">'一'</span>, <span class="string">'一个'</span>, <span class="string">'上'</span>, <span class="string">'也'</span>, <span class="string">'很'</span>, <span class="string">'到'</span>, <span class="string">'说'</span>, <span class="string">'要'</span>, <span class="string">'去'</span>,</span><br><span class="line">        <span class="string">'你'</span>, <span class="string">'会'</span>, <span class="string">'着'</span>, <span class="string">'没有'</span>, <span class="string">'看'</span>, <span class="string">'好'</span>, <span class="string">'自己'</span>, <span class="string">'这'</span></span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 去除标点</span></span><br><span class="line">    text = re.sub(<span class="string">r'[^\u4e00-\u9fa5a-zA-Z0-9]'</span>, <span class="string">' '</span>, text)</span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    tokens = jieba.lcut(text)</span><br><span class="line">    <span class="comment"># 去停用词</span></span><br><span class="line">    stopwords = load_stopwords()</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> <span class="built_in">len</span>(w) &gt; <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理所有评论</span></span><br><span class="line">processed_reviews = [preprocess(review) <span class="keyword">for</span> review <span class="keyword">in</span> reviews]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"预处理示例:"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"原始: <span class="subst">{reviews[<span class="number">0</span>]}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"处理后: <span class="subst">{processed_reviews[<span class="number">0</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="特征提取与模型训练">特征提取与模型训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    processed_reviews, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-IDF 向量化</span></span><br><span class="line">vectorizer = TfidfVectorizer(max_features=<span class="number">100</span>)</span><br><span class="line">X_train_tfidf = vectorizer.fit_transform(X_train)</span><br><span class="line">X_test_tfidf = vectorizer.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练朴素贝叶斯分类器</span></span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(X_train_tfidf, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = classifier.predict(X_test_tfidf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"准确率: <span class="subst">{accuracy_score(y_test, y_pred):<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n分类报告:"</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=[<span class="string">'负面'</span>, <span class="string">'正面'</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="预测新评论">预测新评论</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_sentiment</span>(<span class="params">text</span>):</span><br><span class="line">    processed = preprocess(text)</span><br><span class="line">    tfidf = vectorizer.transform([processed])</span><br><span class="line">    prediction = classifier.predict(tfidf)[<span class="number">0</span>]</span><br><span class="line">    proba = classifier.predict_proba(tfidf)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    sentiment = <span class="string">"正面"</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"负面"</span></span><br><span class="line">    confidence = proba[prediction] * <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"评论: <span class="subst">{text}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"预测: <span class="subst">{sentiment}</span> (置信度: <span class="subst">{confidence:<span class="number">.1</span>f}</span>%)"</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试新数据</span></span><br><span class="line">predict_sentiment(<span class="string">"这部电影非常精彩，推荐大家去看"</span>)</span><br><span class="line">predict_sentiment(<span class="string">"太烂了，完全是浪费时间"</span>)</span><br><span class="line">predict_sentiment(<span class="string">"还可以，中规中矩"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="完整代码整合">完整代码整合</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">reviews = [</span><br><span class="line">    <span class="string">"这部电影太精彩了，强烈推荐"</span>, <span class="string">"剧情拖沓，演技尴尬，浪费时间"</span>,</span><br><span class="line">    <span class="string">"导演功力深厚，画面唯美，值得一看"</span>, <span class="string">"烂片，看了十分钟就想离场"</span>,</span><br><span class="line">    <span class="string">"演员表现出色，剧情引人入胜"</span>, <span class="string">"完全是烂俗套路，毫无新意"</span>,</span><br><span class="line">    <span class="string">"特效震撼，故事感人，五星好评"</span>, <span class="string">"无聊至极，不推荐"</span>,</span><br><span class="line">    <span class="string">"这是我今年看过最好的电影"</span>, <span class="string">"差评，根本不值票价"</span>,</span><br><span class="line">    <span class="string">"演技炸裂，每个镜头都是精心设计"</span>, <span class="string">"剧情漏洞百出，逻辑混乱"</span>,</span><br><span class="line">    <span class="string">"音乐动人，情感真挚，催人泪下"</span>, <span class="string">"浪费我两个小时，强烈不推荐"</span>,</span><br><span class="line">    <span class="string">"制作精良，诚意满满，良心之作"</span>, <span class="string">"看完只想骂人，烂透了"</span>,</span><br><span class="line">    <span class="string">"笑点密集，轻松愉快，适合全家观看"</span>, <span class="string">"尴尬癌都犯了，实在看不下去"</span>,</span><br><span class="line">    <span class="string">"细节丰富，伏笔精妙，值得二刷"</span>, <span class="string">"毫无亮点，彻底失望"</span></span><br><span class="line">]</span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">stopwords = <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, <span class="string">'都'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text</span>):</span><br><span class="line">    text = re.sub(<span class="string">r'[^\u4e00-\u9fa5]'</span>, <span class="string">' '</span>, text)</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> jieba.lcut(text) <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> <span class="built_in">len</span>(w) &gt; <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tokens)</span><br><span class="line"></span><br><span class="line">processed = [preprocess(r) <span class="keyword">for</span> r <span class="keyword">in</span> reviews]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(processed, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">vectorizer = TfidfVectorizer()</span><br><span class="line">X_train_vec = vectorizer.fit_transform(X_train)</span><br><span class="line">X_test_vec = vectorizer.transform(X_test)</span><br><span class="line"></span><br><span class="line">clf = MultinomialNB()</span><br><span class="line">clf.fit(X_train_vec, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line">y_pred = clf.predict(X_test_vec)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"准确率: <span class="subst">{accuracy_score(y_test, y_pred):<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测新样本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">text</span>):</span><br><span class="line">    vec = vectorizer.transform([preprocess(text)])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"正面"</span> <span class="keyword">if</span> clf.predict(vec)[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"负面"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"电影拍得很棒，非常感动"</span>))  <span class="comment"># 正面</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"太差了，不想看"</span>))  <span class="comment"># 负面</span></span><br></pre></td></tr></table></figure>
<h3 id="模型改进方向">模型改进方向</h3>
<ol type="1">
<li><strong>扩充数据集</strong>：20
个样本太少，实际需要数千到数万样本</li>
<li><strong>调整停用词</strong>：根据任务定制停用词表</li>
<li><strong>尝试其他模型</strong>：逻辑回归、SVM、LSTM</li>
<li><strong>使用预训练模型</strong>：BERT 中文模型（如
<code>bert-base-chinese</code>）</li>
<li><strong>考虑否定词</strong>：处理"不好"、"不推荐"等否定结构</li>
</ol>
<h2 id="qa-nlp基础常见问题">❓ Q&amp;A: NLP基础常见问题</h2>
<h3 id="q1-中文分词和英文分词的本质区别是什么">Q1:
中文分词和英文分词的本质区别是什么？</h3>
<p><strong>A</strong>:
英文单词间有天然分隔符（空格），分词只需按空格切分（复杂情况才需处理缩写如
"don't"）。中文是连续字符流，没有明确边界，需要算法判断哪些字组成词：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">英文: "I love NLP" → 天然分隔 → ["I", "love", "NLP"]</span><br><span class="line">中文: "我爱NLP" → 需算法 → ["我", "爱", "NLP"] 或 ["我爱", "NLP"]</span><br></pre></td></tr></table></figure>
<p>中文分词的难点：</p>
<ul>
<li><strong>歧义消解</strong>："乒乓球拍卖" → "乒乓球/拍卖" 还是
"乒乓/球拍/卖"？</li>
<li><strong>新词识别</strong>："奥利给"、"yyds"
等网络词汇不在词典中</li>
<li><strong>领域适应</strong>：医疗、法律等领域有专门术语</li>
</ul>
<h3 id="q2-词干化和词形还原应该选哪个">Q2:
词干化和词形还原应该选哪个？</h3>
<p><strong>A</strong>: 看场景：</p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 45%">
<col style="width: 27%">
</colgroup>
<thead>
<tr>
<th>场景</th>
<th>推荐方法</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>信息检索、搜索引擎</td>
<td>词干化</td>
<td>快速，允许过匹配（"running" 和 "runner" 都匹配 "run"）</td>
</tr>
<tr>
<td>文本分类、情感分析</td>
<td>词形还原</td>
<td>准确，避免产生不存在的词</td>
</tr>
<tr>
<td>实时系统</td>
<td>词干化</td>
<td>速度优先</td>
</tr>
<tr>
<td>学术研究</td>
<td>词形还原</td>
<td>质量优先</td>
</tr>
</tbody>
</table>
<p><strong>实例对比</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer, WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line">words = [<span class="string">"studies"</span>, <span class="string">"studying"</span>, <span class="string">"better"</span>, <span class="string">"worse"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"<span class="subst">{w}</span>: stem=<span class="subst">{stemmer.stem(w)}</span>, lemma=<span class="subst">{lemmatizer.lemmatize(w, pos=<span class="string">'v'</span>)}</span>"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># studies: stem=studi, lemma=study</span></span><br><span class="line"><span class="comment"># studying: stem=studi, lemma=study</span></span><br><span class="line"><span class="comment"># better: stem=better, lemma=better (需要指定pos='a'才能还原为good)</span></span><br><span class="line"><span class="comment"># worse: stem=wors, lemma=worse</span></span><br></pre></td></tr></table></figure></p>
<h3 id="q3-tf-idf-的-idf-为什么要取对数">Q3: TF-IDF 的 IDF
为什么要取对数？</h3>
<p><strong>A</strong>: 三个原因：</p>
<ol type="1">
<li><p><strong>数值稳定</strong>：假设总文档数 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="10.682ex" height="1.731ex" role="img" focusable="false" viewbox="0 -683 4721.6 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(2000,0)"/></g></g></g></svg></mjx-container></span>，某词出现在 1 个文档中，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.984ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 8833.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(2055.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(3111.6,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3999.6,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mn" transform="translate(4499.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(5277.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(6333.1,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(2000,0)"/></g></g></g></svg></mjx-container></span>；另一词出现在
10 个文档中，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="11.565ex" height="1.731ex" role="img" focusable="false" viewbox="0 -683 5111.6 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(2055.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(3111.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/></g></g></g></svg></mjx-container></span>。差距过大会导致数值不稳定。</p></li>
<li><p><strong>符合人类直觉</strong>：词的重要性不应线性增长。出现在 1
篇 vs 2 篇文档的区别，比出现在 100 篇 vs 101 篇的区别更显著。</p></li>
<li><p><strong>匹配信息论</strong>：在信息论中，事件概率为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container></span> 的信息量是 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex;" xmlns="http://www.w3.org/2000/svg" width="6.544ex" height="2.036ex" role="img" focusable="false" viewbox="0 -694 2892.3 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(944.7,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(2222.7,0)"><path data-c="2061" d=""/></g><g data-mml-node="mi" transform="translate(2389.3,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container></span>。词出现在 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span> 篇文档的概率是 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.498ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 1988 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(600,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mi" transform="translate(1100,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g></svg></mjx-container></span>，信息量为： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.577ex;" xmlns="http://www.w3.org/2000/svg" width="17.7ex" height="4.652ex" role="img" focusable="false" viewbox="0 -1359 7823.6 2056"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(944.7,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(2222.7,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(2389.3,0)"><g data-mml-node="mi" transform="translate(364,676)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><rect width="1088" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(3995.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(5050.9,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(6328.9,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(6495.6,0)"><g data-mml-node="mi" transform="translate(220,676)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(364,-686)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><rect width="1088" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p></li>
</ol>
<p><strong>对比</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">N = <span class="number">10000</span>  <span class="comment"># 总文档数</span></span><br><span class="line">docs_contain = [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"不取对数:"</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> docs_contain:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"  出现在 <span class="subst">{n}</span> 篇: IDF = <span class="subst">{N/n}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n取对数:"</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> docs_contain:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"  出现在 <span class="subst">{n}</span> 篇: IDF = <span class="subst">{math.log(N/n):<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不取对数: 10000, 1000, 100, 10 (线性衰减)</span></span><br><span class="line"><span class="comment"># 取对数: 9.21, 6.91, 4.61, 2.30 (对数衰减，更平滑)</span></span><br></pre></td></tr></table></figure></p>
<h3 id="q4-为什么要去除停用词会不会丢失信息">Q4:
为什么要去除停用词？会不会丢失信息？</h3>
<p><strong>A</strong>: 去除停用词有明确好处：</p>
<ul>
<li><strong>降维</strong>：词汇表从 10 万降到 5 万，节省存储和计算</li>
<li><strong>提升效果</strong>：高频低义词（"的"、"了"）会掩盖关键词</li>
</ul>
<p>但确实可能丢失信息，尤其是：</p>
<ul>
<li><strong>情感分析</strong>："不好" 中的 "不" 是停用词但表否定</li>
<li><strong>问答系统</strong>："who"、"when" 是疑问词，不应删除</li>
<li><strong>短文本</strong>：微博、评论本来就短，再删停用词就没什么了</li>
</ul>
<p><strong>权衡策略</strong>：</p>
<ol type="1">
<li>任务相关停用词表（情感分析保留否定词）</li>
<li>对比实验（有/无停用词的模型效果）</li>
<li>深度学习模型可以不去停用词（模型自己学会忽略）</li>
</ol>
<h3 id="q5-词袋模型丢失了语序为什么还能工作">Q5:
词袋模型丢失了语序，为什么还能工作？</h3>
<p><strong>A</strong>: 对于某些任务，语序不是最关键的：</p>
<p><strong>文本分类示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正面评论: "电影很好，我喜欢"</span><br><span class="line">负面评论: "我喜欢好电影，但这部很差"</span><br></pre></td></tr></table></figure></p>
<p>词袋模型看到： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正面: {电影:1, 很:1, 好:1, 我:1, 喜欢:1}</span><br><span class="line">负面: {我:1, 喜欢:1, 好:1, 电影:1, 但:1, 这:1, 部:1, 很:1, 差:1}</span><br></pre></td></tr></table></figure></p>
<p>虽然丢失了语序，但"差"、"但"的出现本身就是强信号。</p>
<p><strong>但在这些任务中词袋模型会失效</strong>：</p>
<ul>
<li><strong>机器翻译</strong>："猫追老鼠" ≠ "老鼠追猫"</li>
<li><strong>问答系统</strong>："谁打了谁" 需要知道主宾关系</li>
<li><strong>文本生成</strong>：生成的句子需要符合语法</li>
</ul>
<p>现代 NLP 用 RNN、Transformer 等模型保留语序信息。</p>
<h3 id="q6-jieba-的精确模式全模式搜索引擎模式有什么区别">Q6: jieba
的精确模式、全模式、搜索引擎模式有什么区别？</h3>
<p><strong>A</strong>: 三种模式的切分粒度不同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">text = <span class="string">"我来到北京清华大学"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"精确模式:"</span>, jieba.lcut(text, cut_all=<span class="literal">False</span>))</span><br><span class="line"><span class="comment"># ['我', '来到', '北京', '清华大学']</span></span><br><span class="line"><span class="comment"># → 适合文本分析，每个词只出现一次</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"全模式:"</span>, jieba.lcut(text, cut_all=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># ['我', '来到', '北京', '清华', '清华大学', '华大', '大学']</span></span><br><span class="line"><span class="comment"># → 所有可能的词都输出，有冗余</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"搜索引擎模式:"</span>, jieba.lcut_for_search(text))</span><br><span class="line"><span class="comment"># ['我', '来到', '北京', '清华', '华大', '大学', '清华大学']</span></span><br><span class="line"><span class="comment"># → 在精确模式基础上，对长词再切分</span></span><br></pre></td></tr></table></figure>
<p><strong>使用场景</strong>：</p>
<ul>
<li><strong>精确模式</strong>：文本分类、情感分析（默认选择）</li>
<li><strong>全模式</strong>：关键词提取、词云（需要更多候选词）</li>
<li><strong>搜索引擎模式</strong>：搜索召回（"清华大学"被切为"清华"+"大学"+"清华大学"，用户搜"清华"也能匹配）</li>
</ul>
<h3 id="q7-bert-这么强大还需要学习传统预处理吗">Q7: BERT
这么强大，还需要学习传统预处理吗？</h3>
<p><strong>A</strong>: 必须学，原因有四：</p>
<ol type="1">
<li><strong>轻量级场景</strong>：嵌入式设备、实时系统跑不动
BERT（参数量上亿）</li>
<li><strong>数据稀缺时</strong>：标注数据少于 1000 条时，简单模型 +
TF-IDF 可能比 BERT 微调效果好</li>
<li><strong>可解释性</strong>：TF-IDF 可以告诉你哪些词重要，BERT
是黑盒</li>
<li><strong>成本考虑</strong>：训练一次 BERT 可能花费数千元 GPU
费用，朴素贝叶斯几秒钟</li>
</ol>
<p><strong>实际项目流程</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一步：用 TF-IDF + 逻辑回归建立 baseline（1小时）</span><br><span class="line">第二步：评估效果，如果满足需求就部署（节省成本）</span><br><span class="line">第三步：不满足才上 BERT（几天调参 + 数千元成本）</span><br></pre></td></tr></table></figure>
<h3 id="q8-中文预处理时要不要转繁体为简体">Q8:
中文预处理时要不要转繁体为简体？</h3>
<p><strong>A</strong>: 看数据来源：</p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 45%">
<col style="width: 27%">
</colgroup>
<thead>
<tr>
<th>场景</th>
<th>是否转换</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>用户输入（搜索、评论）</td>
<td>转换</td>
<td>用户可能混用，统一避免"臺灣"和"台湾"被当作不同词</td>
</tr>
<tr>
<td>历史文献、古籍</td>
<td>不转换</td>
<td>繁体字承载语义信息（"乾坤" ≠ "干坤"）</td>
</tr>
<tr>
<td>台湾、香港数据</td>
<td>看下游任务</td>
<td>如果和大陆数据混合训练，建议转换</td>
</tr>
</tbody>
</table>
<p><strong>转换工具</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> opencc <span class="keyword">import</span> OpenCC</span><br><span class="line"></span><br><span class="line">cc = OpenCC(<span class="string">'t2s'</span>)  <span class="comment"># 繁体转简体</span></span><br><span class="line">text_traditional = <span class="string">"我愛臺灣"</span></span><br><span class="line">text_simplified = cc.convert(text_traditional)</span><br><span class="line"><span class="built_in">print</span>(text_simplified)  <span class="comment"># "我爱台湾"</span></span><br></pre></td></tr></table></figure></p>
<p><strong>注意</strong>：转换可能有歧义，如"乾燥" → "干燥"，但 "乾隆" →
"乾隆"（不应转换）。</p>
<h3 id="q9-如何评估分词质量">Q9: 如何评估分词质量？</h3>
<p><strong>A</strong>: 三个指标（需要人工标注的标准答案）：</p>
<ol type="1">
<li><p><strong>准确率（Precision）</strong>：分出来的词有多少是对的
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="23.811ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 10524.6 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(1028.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(2084.6,0)"><g data-mml-node="mtext" transform="translate(720,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">确</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">切</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mtext" transform="translate(220,-710)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">模</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">型</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">切</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(7000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><rect width="8200" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p></li>
<li><p><strong>召回率（Recall）</strong>：标准答案中的词有多少被找到
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="23.829ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 10532.6 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g><g data-mml-node="mo" transform="translate(1036.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(2092.6,0)"><g data-mml-node="mtext" transform="translate(720,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">确</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">切</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mtext" transform="translate(220,-710)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">准</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">答</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">案</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(7000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><rect width="8200" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p></li>
<li><p><strong>F1 值</strong>：准确率和召回率的调和平均 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.738ex;" xmlns="http://www.w3.org/2000/svg" width="16.917ex" height="4.812ex" role="img" focusable="false" viewbox="0 -1359 7477.4 2127"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"/></g><g data-mml-node="mn" transform="translate(749,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(1526.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(2582.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mo" transform="translate(3304.8,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mfrac" transform="translate(4305,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(973.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mi" transform="translate(1973.4,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(973.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1973.4,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g></g><rect width="2932.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p></li>
</ol>
<p><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">原句: "我爱自然语言处理"</span><br><span class="line">标准答案: ["我", "爱", "自然语言处理"]</span><br><span class="line">模型输出: ["我", "爱", "自然", "语言", "处理"]</span><br><span class="line"></span><br><span class="line">正确切分: "我", "爱" (2个)</span><br><span class="line">模型总词数: 5</span><br><span class="line">标准答案总词数: 3</span><br><span class="line"></span><br><span class="line">准确率: 2/5 = 0.4</span><br><span class="line">召回率: 2/3 = 0.67</span><br><span class="line">F1: 2 × (0.4 × 0.67) / (0.4 + 0.67) = 0.5</span><br></pre></td></tr></table></figure></p>
<p>实际中更常用<strong>下游任务指标</strong>（如分类准确率）评估：分词好
→ 分类效果好。</p>
<h3 id="q10-实际项目中预处理流程应该怎么设计">Q10:
实际项目中预处理流程应该怎么设计？</h3>
<p><strong>A</strong>: 遵循这个检查清单：</p>
<p><strong>第一步：分析数据特征</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查数据长度分布</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame({<span class="string">'text'</span>: your_texts})</span><br><span class="line">df[<span class="string">'length'</span>] = df[<span class="string">'text'</span>].apply(<span class="built_in">len</span>)</span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">'length'</span>].describe())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否有特殊字符</span></span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">bool</span>(re.search(<span class="string">r'[^\u4e00-\u9fa5a-zA-Z0-9]'</span>, x))).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure></p>
<p><strong>第二步：确定预处理步骤</strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">短文本（&lt;50字）：保守去停用词（避免信息丢失）</span><br><span class="line">长文本（&gt;200字）：积极去停用词 + 过滤低频词</span><br><span class="line">含emoji/表情：看任务，情感分析可能需要保留</span><br><span class="line">含URL/邮箱：替换为特殊标记 &lt;URL&gt; &lt;EMAIL&gt;</span><br><span class="line">含数字：分类任务可删除，命名实体识别要保留</span><br></pre></td></tr></table></figure></p>
<p><strong>第三步：模块化实现</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextPreprocessor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, remove_stopwords=<span class="literal">True</span>, min_word_len=<span class="number">1</span></span>):</span><br><span class="line">        self.remove_stopwords = remove_stopwords</span><br><span class="line">        self.min_word_len = min_word_len</span><br><span class="line">        self.stopwords = self.load_stopwords()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_stopwords</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 加载停用词</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, ...])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clean</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># 清洗文本</span></span><br><span class="line">        text = re.sub(<span class="string">r'http\S+'</span>, <span class="string">'&lt;URL&gt;'</span>, text)</span><br><span class="line">        text = re.sub(<span class="string">r'\S+@\S+'</span>, <span class="string">'&lt;EMAIL&gt;'</span>, text)</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># 分词</span></span><br><span class="line">        <span class="keyword">return</span> jieba.lcut(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter_tokens</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="comment"># 过滤</span></span><br><span class="line">        <span class="keyword">if</span> self.remove_stopwords:</span><br><span class="line">            tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> self.stopwords]</span><br><span class="line">        tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt;= self.min_word_len]</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self, text</span>):</span><br><span class="line">        text = self.clean(text)</span><br><span class="line">        tokens = self.tokenize(text)</span><br><span class="line">        tokens = self.filter_tokens(tokens)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">preprocessor = TextPreprocessor(remove_stopwords=<span class="literal">True</span>, min_word_len=<span class="number">2</span>)</span><br><span class="line">processed = preprocessor.process(<span class="string">"我在北京的清华大学学习NLP"</span>)</span><br><span class="line"><span class="built_in">print</span>(processed)</span><br></pre></td></tr></table></figure></p>
<p><strong>第四步：A/B 测试</strong></p>
<p>对比不同预处理方案的下游效果： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">方案A: 去停用词 + 词形还原 → 准确率 85%</span><br><span class="line">方案B: 保留停用词 + 只小写化 → 准确率 87% ✓</span><br></pre></td></tr></table></figure></p>
<p>记住：<strong>预处理没有银弹，一切以下游任务效果为准</strong>。</p>
<h2 id="总结与展望">总结与展望</h2>
<p>文本预处理是 NLP
的基础设施，虽然深度学习降低了手工特征工程的需求，但理解这些经典方法仍然重要。你已经掌握了：</p>
<ul>
<li>NLP 的发展脉络和应用场景</li>
<li>中英文文本预处理的完整流程</li>
<li>jieba、NLTK、spaCy 等工具的使用</li>
<li>词袋模型和 TF-IDF 的原理与实现</li>
<li>端到端的文本分类实战案例</li>
</ul>
<p>下一篇文章将介绍<strong>词向量与语言模型</strong>，探讨
Word2Vec、GloVe、ELMo
如何把词表示为稠密向量，以及预训练语言模型的原理。敬请期待！</p>
<hr>
<p><strong>推荐资源</strong>：</p>
<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://github.com/fxsjy/jieba">jieba 官方文档<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://www.nltk.org/book/">NLTK
Book（免费在线书）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://spacy.io/usage">spaCy 使用指南<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction">scikit-learn
文本特征提取教程<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p><strong>代码仓库</strong>：本文完整代码已上传至 <a href="#">GitHub</a>（请根据实际情况替换链接）</p>
<p>如果觉得有帮助，欢迎点赞分享！有任何问题欢迎在评论区讨论。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>本文标题：自然语言处理（一）—— NLP入门与文本预处理</li>
        <li>本文作者：Chen Kai</li>
        <li>创建时间：2024-09-05 00:00:00</li>
        <li>
            本文链接：https://www.chenk.top/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94-NLP%E5%85%A5%E9%97%A8%E4%B8%8E%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Deep-Learning/">#Deep Learning</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/%E7%A5%9E%E7%BB%8F%E7%AE%97%E5%AD%90%E7%90%86%E8%AE%BA%EF%BC%9A%E5%AD%A6%E4%B9%A0%E6%97%A0%E9%99%90%E7%BB%B4%E6%98%A0%E5%B0%84/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">神经算子理论：学习无限维映射</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/%E7%89%A9%E7%90%86%E4%BF%A1%E6%81%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E4%BB%8ERitz%E6%96%B9%E6%B3%95%E5%88%B0%E7%8E%B0%E4%BB%A3%E6%B1%82%E8%A7%A3%E5%99%A8/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">物理信息神经网络：从Ritz方法到现代求解器</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- 由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#nlp-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%8E%E7%AC%A6%E5%8F%B7%E4%B8%BB%E4%B9%89%E5%88%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF"><span class="nav-number">1.</span> <span class="nav-text">NLP
是什么？从符号主义到大模型的演进之路</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-number">1.1.</span> <span class="nav-text">自然语言处理的本质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nlp-%E5%8F%91%E5%B1%95%E7%9A%84%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.</span> <span class="nav-text">NLP 发展的四个阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E7%AC%A6%E5%8F%B7%E4%B8%BB%E4%B9%891950s-1980s"><span class="nav-number">1.2.1.</span> <span class="nav-text">第一阶段：符号主义（1950s-1980s）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%951990s-2010s"><span class="nav-number">1.2.2.</span> <span class="nav-text">第二阶段：统计方法（1990s-2010s）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02013-2020"><span class="nav-number">1.2.3.</span> <span class="nav-text">第三阶段：深度学习（2013-2020）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E9%98%B6%E6%AE%B5%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B2018%E8%87%B3%E4%BB%8A"><span class="nav-number">1.2.4.</span> <span class="nav-text">第四阶段：预训练大模型（2018至今）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nlp-%E7%9A%84%E7%8E%B0%E5%AE%9E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">2.</span> <span class="nav-text">NLP 的现实应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E"><span class="nav-number">2.1.</span> <span class="nav-text">搜索引擎</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="nav-number">2.2.</span> <span class="nav-text">机器翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D"><span class="nav-number">2.3.</span> <span class="nav-text">智能客服</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">2.4.</span> <span class="nav-text">推荐系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="nav-number">2.5.</span> <span class="nav-text">情感分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">为什么需要文本预处理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B"><span class="nav-number">4.</span> <span class="nav-text">文本预处理的核心流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96normalization"><span class="nav-number">4.1.</span> <span class="nav-text">标准化（Normalization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E8%AF%8Dtokenization"><span class="nav-number">4.2.</span> <span class="nav-text">分词（Tokenization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8Dstop-words-removal"><span class="nav-number">4.3.</span> <span class="nav-text">去除停用词（Stop Words
Removal）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E5%B9%B2%E5%8C%96%E4%B8%8E%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F"><span class="nav-number">4.4.</span> <span class="nav-text">词干化与词形还原</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%8C%91%E6%88%98%E4%B8%8E%E5%B7%A5%E5%85%B7"><span class="nav-number">5.</span> <span class="nav-text">中文分词：挑战与工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E7%89%B9%E6%AE%8A%E6%80%A7"><span class="nav-number">5.1.</span> <span class="nav-text">中文分词的特殊性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%B5%81%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7"><span class="nav-number">5.2.</span> <span class="nav-text">主流中文分词工具</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jieba%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8D"><span class="nav-number">5.2.1.</span> <span class="nav-text">jieba（结巴分词）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pkuseg"><span class="nav-number">5.2.2.</span> <span class="nav-text">pkuseg</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#laclexical-analysis-of-chinese"><span class="nav-number">5.2.3.</span> <span class="nav-text">LAC（Lexical Analysis of
Chinese）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B"><span class="nav-number">5.3.</span> <span class="nav-text">中文预处理完整流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8B%B1%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86nltk-%E4%B8%8E-spacy"><span class="nav-number">6.</span> <span class="nav-text">英文文本预处理：NLTK 与 spaCy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nltknatural-language-toolkit"><span class="nav-number">6.1.</span> <span class="nav-text">NLTK（Natural Language
Toolkit）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spacy"><span class="nav-number">6.2.</span> <span class="nav-text">spaCy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E4%BB%8E%E8%AF%8D%E8%A2%8B%E5%88%B0-tf-idf"><span class="nav-number">7.</span> <span class="nav-text">文本表示：从词袋到 TF-IDF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8Bbag-of-wordsbow"><span class="nav-number">7.1.</span> <span class="nav-text">词袋模型（Bag of Words，BoW）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-idfterm-frequency---inverse-document-frequency"><span class="nav-number">7.2.</span> <span class="nav-text">TF-IDF（Term
Frequency - Inverse Document Frequency）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="nav-number">7.3.</span> <span class="nav-text">文本表示方法对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E6%9E%84%E5%BB%BA%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">8.</span> <span class="nav-text">实战案例：构建文本分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0"><span class="nav-number">8.1.</span> <span class="nav-text">任务描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">8.2.</span> <span class="nav-text">数据准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="nav-number">8.3.</span> <span class="nav-text">预处理函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">8.4.</span> <span class="nav-text">特征提取与模型训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E6%96%B0%E8%AF%84%E8%AE%BA"><span class="nav-number">8.5.</span> <span class="nav-text">预测新评论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88"><span class="nav-number">8.6.</span> <span class="nav-text">完整代码整合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B%E6%96%B9%E5%90%91"><span class="nav-number">8.7.</span> <span class="nav-text">模型改进方向</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qa-nlp%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">9.</span> <span class="nav-text">❓ Q&amp;A: NLP基础常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%92%8C%E8%8B%B1%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">9.1.</span> <span class="nav-text">Q1:
中文分词和英文分词的本质区别是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-%E8%AF%8D%E5%B9%B2%E5%8C%96%E5%92%8C%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%E5%BA%94%E8%AF%A5%E9%80%89%E5%93%AA%E4%B8%AA"><span class="nav-number">9.2.</span> <span class="nav-text">Q2:
词干化和词形还原应该选哪个？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-tf-idf-%E7%9A%84-idf-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8F%96%E5%AF%B9%E6%95%B0"><span class="nav-number">9.3.</span> <span class="nav-text">Q3: TF-IDF 的 IDF
为什么要取对数？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D%E4%BC%9A%E4%B8%8D%E4%BC%9A%E4%B8%A2%E5%A4%B1%E4%BF%A1%E6%81%AF"><span class="nav-number">9.4.</span> <span class="nav-text">Q4:
为什么要去除停用词？会不会丢失信息？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E4%B8%A2%E5%A4%B1%E4%BA%86%E8%AF%AD%E5%BA%8F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E5%B7%A5%E4%BD%9C"><span class="nav-number">9.5.</span> <span class="nav-text">Q5:
词袋模型丢失了语序，为什么还能工作？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-jieba-%E7%9A%84%E7%B2%BE%E7%A1%AE%E6%A8%A1%E5%BC%8F%E5%85%A8%E6%A8%A1%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%A8%A1%E5%BC%8F%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="nav-number">9.6.</span> <span class="nav-text">Q6: jieba
的精确模式、全模式、搜索引擎模式有什么区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q7-bert-%E8%BF%99%E4%B9%88%E5%BC%BA%E5%A4%A7%E8%BF%98%E9%9C%80%E8%A6%81%E5%AD%A6%E4%B9%A0%E4%BC%A0%E7%BB%9F%E9%A2%84%E5%A4%84%E7%90%86%E5%90%97"><span class="nav-number">9.7.</span> <span class="nav-text">Q7: BERT
这么强大，还需要学习传统预处理吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q8-%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E6%97%B6%E8%A6%81%E4%B8%8D%E8%A6%81%E8%BD%AC%E7%B9%81%E4%BD%93%E4%B8%BA%E7%AE%80%E4%BD%93"><span class="nav-number">9.8.</span> <span class="nav-text">Q8:
中文预处理时要不要转繁体为简体？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q9-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%E5%88%86%E8%AF%8D%E8%B4%A8%E9%87%8F"><span class="nav-number">9.9.</span> <span class="nav-text">Q9: 如何评估分词质量？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q10-%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE%E4%B8%AD%E9%A2%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1"><span class="nav-number">9.10.</span> <span class="nav-text">Q10:
实际项目中预处理流程应该怎么设计？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B"><span class="nav-number">10.</span> <span class="nav-text">总结与展望</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
