<!DOCTYPE html>



<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            自然语言处理（一）—— NLP入门与文本预处理 |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"zh-CN","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">自然语言处理（一）—— NLP入门与文本预处理</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-02-01 00:00:00</span>
        <span class="mobile">2025-02-01 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>7.2k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>28 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>人类每天产生的文本数据量惊人：社交媒体上的帖子、搜索引擎的查询、客服系统的对话、新闻报道、学术论文……这些文字背后蕴藏着巨大的价值，但计算机天生不理解人类语言。自然语言处理（Natural Language Processing，NLP）就是教会机器"读懂"文字的技术，让它们能从海量文本中提取信息、理解意图、生成回复，甚至进行创作。</p>
<p>本文从零开始介绍 NLP 的核心概念和第一步：文本预处理。你将了解 NLP 如何从符号规则走向深度学习，为什么需要对文本进行分词、去噪、标准化，以及如何用 Python 工具实现中英文预处理流程。最后通过一个完整的文本分类实战案例，把理论和代码串联起来。</p>
<span id="more"></span>
<h2 id="NLP-是什么？从符号主义到大模型的演进之路"><a class="header-anchor" href="#NLP-是什么？从符号主义到大模型的演进之路">¶</a>NLP 是什么？从符号主义到大模型的演进之路</h2>
<h3 id="自然语言处理的本质"><a class="header-anchor" href="#自然语言处理的本质">¶</a>自然语言处理的本质</h3>
<p>人类用语言交流时，一个简单的句子"我在银行取钱"包含多重信息：</p>
<ul>
<li><strong>词汇语义</strong>："银行"指金融机构还是河岸？</li>
<li><strong>句法结构</strong>："我"是主语，"取钱"是动作</li>
<li><strong>语用意图</strong>：陈述事实还是请求帮助？</li>
</ul>
<p>NLP 的核心任务是让计算机理解这些层次的信息，并在此基础上完成具体任务：</p>
<ul>
<li><strong>文本分类</strong>：判断邮件是否是垃圾邮件</li>
<li><strong>信息抽取</strong>：从新闻中提取人名、地点、时间</li>
<li><strong>机器翻译</strong>：把英文文档翻译成中文</li>
<li><strong>问答系统</strong>：回答"特斯拉的创始人是谁？"</li>
<li><strong>文本生成</strong>：写诗、摘要、对话回复</li>
</ul>
<h3 id="NLP-发展的四个阶段"><a class="header-anchor" href="#NLP-发展的四个阶段">¶</a>NLP 发展的四个阶段</h3>
<h4 id="第一阶段：符号主义（1950s-1980s）"><a class="header-anchor" href="#第一阶段：符号主义（1950s-1980s）">¶</a>第一阶段：符号主义（1950s-1980s）</h4>
<p>早期研究者认为语言可以用规则描述。比如：</p>
<ul>
<li>句子 = 主语 + 谓语 + 宾语</li>
<li>如果句子包含"不"、“没有”，则情感为负面</li>
</ul>
<p>这种方法依赖专家手工编写语法规则和词典。典型代表是 ELIZA（1966年的聊天机器人），它用模式匹配把用户的话改写后抛回去：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">用户："我很难过"</span><br><span class="line">ELIZA："为什么你很难过？"（直接套用模板）</span><br></pre></td></tr></table></figure>
<p><strong>局限性</strong>：语言现象太复杂，规则无法穷尽。"银行"的歧义、"真香"的反讽、"雨我无瓜"的网络用语……人工规则很快就力不从心。</p>
<h4 id="第二阶段：统计方法（1990s-2010s）"><a class="header-anchor" href="#第二阶段：统计方法（1990s-2010s）">¶</a>第二阶段：统计方法（1990s-2010s）</h4>
<p>统计 NLP 不再依赖专家，而是让机器从大量文本中学习规律。核心思想：</p>
<ul>
<li><strong>数据驱动</strong>：给模型看 10 万篇新闻，让它统计哪些词经常一起出现</li>
<li><strong>概率建模</strong>：用 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.247ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 4529 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mtext" transform="translate(1140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">序</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">列</text></g><g data-mml-node="mo" transform="translate(4140,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 表示句子的合理性</li>
</ul>
<p>标志性成果：</p>
<ul>
<li><strong>N-gram 语言模型</strong>：根据前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewbox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container> 个词预测下一个词<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="44.019ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 19456.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(2183,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(2461,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(3613.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(4058.2,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"/></g><g data-mml-node="mo" transform="translate(5396.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(5841.5,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(7788.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(8454.9,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"/></g><g data-mml-node="mi" transform="translate(9510.7,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(10261.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(10650.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(11693.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(11971.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(1123,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1723,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(2501,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(14892.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(15337.3,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"/></g><g data-mml-node="mo" transform="translate(16676,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(17120.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mo" transform="translate(19067.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></li>
<li><strong>隐马尔可夫模型（HMM）</strong>：用于词性标注</li>
<li><strong>朴素贝叶斯分类器</strong>：用于垃圾邮件过滤</li>
</ul>
<p>这一时期的模型需要<strong>特征工程</strong>：人工设计输入特征（如词频、是否包含特定词、句子长度等），然后训练分类器。</p>
<h4 id="第三阶段：深度学习（2013-2020）"><a class="header-anchor" href="#第三阶段：深度学习（2013-2020）">¶</a>第三阶段：深度学习（2013-2020）</h4>
<p>神经网络的兴起改变了 NLP 格局。关键技术：</p>
<ul>
<li><strong>Word2Vec（2013）</strong>：把词映射为稠密向量，语义相似的词向量接近<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.466ex;" xmlns="http://www.w3.org/2000/svg" width="29.793ex" height="2.036ex" role="img" focusable="false" viewbox="0 -694 13168.4 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="6B" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T97 124T98 167T98 217T98 272T98 329Q98 366 98 407T98 482T98 542T97 586T97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V463L180 233L240 287Q300 341 304 347Q310 356 310 364Q310 383 289 385H284V431H293Q308 428 412 428Q475 428 484 431H489V385H476Q407 380 360 341Q286 278 286 274Q286 273 349 181T420 79Q434 60 451 53T500 46H511V0H505Q496 3 418 3Q322 3 307 0H299V46H306Q330 48 330 65Q330 72 326 79Q323 84 276 153T228 222L176 176V120V84Q176 65 178 59T189 49Q210 46 238 46H254V0H246Q231 3 137 3T28 0H20V46H36Z"/><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(528,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(806,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(1362,0)"/></g><g data-mml-node="mo" transform="translate(2084.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mtext" transform="translate(3084.4,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)"/></g><g data-mml-node="mo" transform="translate(5195.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mtext" transform="translate(6195.9,0)"><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(722,0)"/><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1222,0)"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2055,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2555,0)"/></g><g data-mml-node="mo" transform="translate(9584.7,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"/></g><g data-mml-node="mtext" transform="translate(10640.4,0)"><path data-c="71" d="M33 218Q33 308 95 374T236 441H246Q330 441 381 372L387 364Q388 364 404 403L420 442H457V156Q457 -132 458 -134Q462 -142 470 -145Q491 -148 519 -148H535V-194H527L504 -193Q480 -192 453 -192T415 -191Q312 -191 303 -194H295V-148H311Q339 -148 360 -145Q369 -141 371 -135T373 -106V-41V49Q313 -11 236 -11Q154 -11 94 53T33 218ZM376 300Q346 389 278 401Q275 401 269 401T261 402Q211 400 171 350T131 214Q131 137 165 82T253 27Q296 27 328 54T376 118V300Z"/><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(528,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1084,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1528,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1972,0)"/></g></g></g></svg></mjx-container></li>
<li><strong>循环神经网络（RNN/LSTM）</strong>：处理变长序列，捕捉上下文关系</li>
<li><strong>注意力机制（Attention）</strong>：让模型关注输入的关键部分</li>
<li><strong>Transformer（2017）</strong>：并行化训练，成为现代 NLP 的基石</li>
</ul>
<p>深度学习模型可以<strong>端到端</strong>训练：直接输入原始文本，输出任务结果，不需要手工设计特征。</p>
<h4 id="第四阶段：预训练大模型（2018至今）"><a class="header-anchor" href="#第四阶段：预训练大模型（2018至今）">¶</a>第四阶段：预训练大模型（2018至今）</h4>
<p>2018 年 BERT 横空出世，开启了"预训练-微调"范式：</p>
<ol>
<li><strong>预训练</strong>：在海量无标注文本上训练通用语言模型</li>
<li><strong>微调</strong>：在少量标注数据上针对具体任务调整</li>
</ol>
<p>随后 GPT-3（2020，1750 亿参数）、ChatGPT（2022）、GPT-4（2023）不断刷新能力上限。现在的大模型具备：</p>
<ul>
<li><strong>少样本学习</strong>：给几个例子就能理解新任务</li>
<li><strong>指令跟随</strong>：理解"用正式语气写一封道歉信"</li>
<li><strong>多模态能力</strong>：同时处理文本、图像、语音</li>
</ul>
<h2 id="NLP-的现实应用场景"><a class="header-anchor" href="#NLP-的现实应用场景">¶</a>NLP 的现实应用场景</h2>
<h3 id="搜索引擎"><a class="header-anchor" href="#搜索引擎">¶</a>搜索引擎</h3>
<p>当你在 Google 搜索"苹果新品发布会"时，NLP 帮助：</p>
<ul>
<li><strong>查询理解</strong>：识别"苹果"指公司而非水果</li>
<li><strong>相关性排序</strong>：把最相关的新闻排在前面</li>
<li><strong>摘要生成</strong>：在搜索结果下方显示关键信息</li>
</ul>
<h3 id="机器翻译"><a class="header-anchor" href="#机器翻译">¶</a>机器翻译</h3>
<p>Google 翻译、DeepL 用神经机器翻译（NMT）实现：</p>
<ul>
<li><strong>Encoder-Decoder 架构</strong>：把源语言编码为语义表示，再解码为目标语言</li>
<li><strong>注意力机制</strong>：翻译长句时关注对应的源词</li>
</ul>
<h3 id="智能客服"><a class="header-anchor" href="#智能客服">¶</a>智能客服</h3>
<p>电商平台的客服机器人能：</p>
<ul>
<li><strong>意图识别</strong>：判断用户是要退货、查物流还是咨询尺码</li>
<li><strong>槽位填充</strong>：提取订单号、商品名等关键信息</li>
<li><strong>对话管理</strong>：多轮交互引导用户解决问题</li>
</ul>
<h3 id="推荐系统"><a class="header-anchor" href="#推荐系统">¶</a>推荐系统</h3>
<p>新闻 App 推荐你可能感兴趣的文章：</p>
<ul>
<li><strong>文本表示</strong>：用词向量或 BERT 把文章编码为向量</li>
<li><strong>相似度计算</strong>：推荐与你已读文章相似的内容</li>
<li><strong>用户画像</strong>：根据历史阅读构建兴趣模型</li>
</ul>
<h3 id="情感分析"><a class="header-anchor" href="#情感分析">¶</a>情感分析</h3>
<p>分析商品评论的情感倾向：</p>
<ul>
<li><strong>正面</strong>：“质量很好，物流快！”</li>
<li><strong>负面</strong>：“颜色和描述不符，申请退货”</li>
<li><strong>中性</strong>：“包装完好无损”</li>
</ul>
<p>企业可以据此监控品牌口碑、发现产品问题。</p>
<h2 id="为什么需要文本预处理？"><a class="header-anchor" href="#为什么需要文本预处理？">¶</a>为什么需要文本预处理？</h2>
<p>假设你要训练一个模型判断电影评论的情感。原始数据可能是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"This movie is GREAT!!! 😊"</span><br><span class="line">"i loved it, best film ever."</span><br><span class="line">"Terrible... worst experience :("</span><br></pre></td></tr></table></figure>
<p>直接把这些文本喂给模型会遇到问题：</p>
<ol>
<li><strong>大小写不统一</strong>：“GREAT” 和 “great” 被当作不同的词</li>
<li><strong>标点和表情</strong>：“!!!” 和 “😊” 干扰了核心词汇</li>
<li><strong>拼写和形态</strong>：“loved” 和 “love” 语义相同但形式不同</li>
<li><strong>噪音词</strong>：“this”、“is”、“it” 等高频词对情感判断贡献不大</li>
</ol>
<p>文本预处理就是<strong>清洗和标准化</strong>原始文本，提取有效信息，让模型更容易学习。</p>
<h2 id="文本预处理的核心流程"><a class="header-anchor" href="#文本预处理的核心流程">¶</a>文本预处理的核心流程</h2>
<h3 id="标准化（Normalization）"><a class="header-anchor" href="#标准化（Normalization）">¶</a>标准化（Normalization）</h3>
<p>把文本转为统一格式：</p>
<ul>
<li><strong>小写化</strong>：<code>"GREAT"</code> → <code>"great"</code></li>
<li><strong>去除标点</strong>：<code>"loved it!!!"</code> → <code>"loved it"</code></li>
<li><strong>统一编码</strong>：处理全角半角、繁简体转换</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This movie is GREAT!!! 😊"</span></span><br><span class="line"><span class="comment"># 小写化</span></span><br><span class="line">text = text.lower()</span><br><span class="line"><span class="comment"># 去除标点和表情</span></span><br><span class="line">text = re.sub(<span class="string">r'[^\w\s]'</span>, <span class="string">''</span>, text)</span><br><span class="line"><span class="built_in">print</span>(text)  <span class="comment"># "this movie is great"</span></span><br></pre></td></tr></table></figure>
<h3 id="分词（Tokenization）"><a class="header-anchor" href="#分词（Tokenization）">¶</a>分词（Tokenization）</h3>
<p>把句子切分为词或子词单元。英文用空格分词较简单，中文则需要专门算法。</p>
<p><strong>英文分词</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"I love machine learning"</span></span><br><span class="line">tokens = text.split()</span><br><span class="line"><span class="built_in">print</span>(tokens)  <span class="comment"># ['I', 'love', 'machine', 'learning']</span></span><br></pre></td></tr></table></figure>
<p><strong>中文分词</strong>（稍后详述）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">text = <span class="string">"我喜欢自然语言处理"</span></span><br><span class="line">tokens = jieba.lcut(text)</span><br><span class="line"><span class="built_in">print</span>(tokens)  <span class="comment"># ['我', '喜欢', '自然语言处理']</span></span><br></pre></td></tr></table></figure>
<h3 id="去除停用词（Stop-Words-Removal）"><a class="header-anchor" href="#去除停用词（Stop-Words-Removal）">¶</a>去除停用词（Stop Words Removal）</h3>
<p>停用词是高频但语义贡献小的词，如"的"、“了”、“is”、“the”。去掉它们可以：</p>
<ul>
<li>减少特征维度</li>
<li>突出关键词</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">stop_words = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">tokens = [<span class="string">'i'</span>, <span class="string">'love'</span>, <span class="string">'machine'</span>, <span class="string">'learning'</span>]</span><br><span class="line">filtered = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"><span class="built_in">print</span>(filtered)  <span class="comment"># ['love', 'machine', 'learning']</span></span><br></pre></td></tr></table></figure>
<h3 id="词干化与词形还原"><a class="header-anchor" href="#词干化与词形还原">¶</a>词干化与词形还原</h3>
<p>把词的不同形态归为统一形式：</p>
<ul>
<li>
<p><strong>词干化（Stemming）</strong>：粗暴地截取词根</p>
<ul>
<li><code>running</code> → <code>run</code></li>
<li><code>happily</code> → <code>happi</code>（不是真实单词）</li>
</ul>
</li>
<li>
<p><strong>词形还原（Lemmatization）</strong>：基于词典还原为基础形式</p>
<ul>
<li><code>running</code> → <code>run</code></li>
<li><code>better</code> → <code>good</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer, WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(stemmer.stem(<span class="string">"running"</span>))  <span class="comment"># "run"</span></span><br><span class="line"><span class="built_in">print</span>(lemmatizer.lemmatize(<span class="string">"running"</span>, pos=<span class="string">'v'</span>))  <span class="comment"># "run"</span></span><br><span class="line"><span class="built_in">print</span>(lemmatizer.lemmatize(<span class="string">"better"</span>, pos=<span class="string">'a'</span>))  <span class="comment"># "good"</span></span><br></pre></td></tr></table></figure>
<p>词形还原更准确但速度慢，词干化快但有时产生不存在的词。实际中根据任务选择。</p>
<h2 id="中文分词：挑战与工具"><a class="header-anchor" href="#中文分词：挑战与工具">¶</a>中文分词：挑战与工具</h2>
<h3 id="中文分词的特殊性"><a class="header-anchor" href="#中文分词的特殊性">¶</a>中文分词的特殊性</h3>
<p>英文单词间有空格，中文则是连续的字符流。"我爱自然语言处理"可以有多种切分方式：</p>
<ul>
<li><code>["我", "爱", "自然", "语言", "处理"]</code>（字级别）</li>
<li><code>["我", "爱", "自然语言", "处理"]</code></li>
<li><code>["我", "爱", "自然语言处理"]</code>（词级别）</li>
</ul>
<p>正确的分词需要理解语义。考虑歧义：“结婚的和尚未结婚的”</p>
<ul>
<li>错误分词：<code>["结婚", "的", "和", "尚未", "结婚", "的"]</code></li>
<li>正确分词：<code>["结婚", "的", "和", "尚", "未", "结婚", "的"]</code></li>
</ul>
<h3 id="主流中文分词工具"><a class="header-anchor" href="#主流中文分词工具">¶</a>主流中文分词工具</h3>
<h4 id="jieba（结巴分词）"><a class="header-anchor" href="#jieba（结巴分词）">¶</a>jieba（结巴分词）</h4>
<p>最流行的 Python 中文分词库，基于前缀词典和动态规划。</p>
<p><strong>安装</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure>
<p><strong>基本用法</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">text = <span class="string">"我来到北京清华大学"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 精确模式</span></span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"精确模式:"</span>, <span class="string">"/"</span>.join(seg_list))  </span><br><span class="line"><span class="comment"># "我/来到/北京/清华大学"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全模式（所有可能的词）</span></span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"全模式:"</span>, <span class="string">"/"</span>.join(seg_list))</span><br><span class="line"><span class="comment"># "我/来到/北京/清华/清华大学/华大/大学"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索引擎模式（细粒度切分）</span></span><br><span class="line">seg_list = jieba.cut_for_search(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"搜索引擎模式:"</span>, <span class="string">"/"</span>.join(seg_list))</span><br><span class="line"><span class="comment"># "我/来到/北京/清华/华大/大学/清华大学"</span></span><br></pre></td></tr></table></figure>
<p><strong>添加自定义词典</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jieba.add_word(<span class="string">"自然语言处理"</span>)</span><br><span class="line">text = <span class="string">"我在学习自然语言处理"</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"/"</span>.join(jieba.cut(text)))</span><br><span class="line"><span class="comment"># "我/在/学习/自然语言处理"</span></span><br></pre></td></tr></table></figure>
<h4 id="pkuseg"><a class="header-anchor" href="#pkuseg">¶</a>pkuseg</h4>
<p>北京大学开源的分词工具，针对不同领域训练了专用模型（新闻、医疗、旅游等），准确率更高但速度慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pkuseg</span><br><span class="line"></span><br><span class="line">seg = pkuseg.pkuseg()  <span class="comment"># 使用默认模型</span></span><br><span class="line">text = <span class="string">"我爱自然语言处理"</span></span><br><span class="line"><span class="built_in">print</span>(seg.cut(text))  <span class="comment"># ['我', '爱', '自然语言处理']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用领域模型</span></span><br><span class="line">seg_med = pkuseg.pkuseg(model_name=<span class="string">'medicine'</span>)</span><br><span class="line">text_med = <span class="string">"患者出现高血压症状"</span></span><br><span class="line"><span class="built_in">print</span>(seg_med.cut(text_med))  <span class="comment"># ['患者', '出现', '高血压', '症状']</span></span><br></pre></td></tr></table></figure>
<h4 id="LAC（Lexical-Analysis-of-Chinese）"><a class="header-anchor" href="#LAC（Lexical-Analysis-of-Chinese）">¶</a>LAC（Lexical Analysis of Chinese）</h4>
<p>百度开源的词法分析工具，不仅分词还能进行词性标注和命名实体识别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> LAC <span class="keyword">import</span> LAC</span><br><span class="line"></span><br><span class="line">lac = LAC(mode=<span class="string">'seg'</span>)  <span class="comment"># 只分词</span></span><br><span class="line">text = <span class="string">"百度是一家高科技公司"</span></span><br><span class="line"><span class="built_in">print</span>(lac.run(text))  <span class="comment"># ['百度', '是', '一家', '高科技', '公司']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词 + 词性标注</span></span><br><span class="line">lac = LAC(mode=<span class="string">'lac'</span>)</span><br><span class="line">seg_result, pos_result = lac.run(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">zip</span>(seg_result, pos_result)))</span><br><span class="line"><span class="comment"># [('百度', 'ORG'), ('是', 'v'), ('一家', 'm'), ('高科技', 'n'), ('公司', 'n')]</span></span><br></pre></td></tr></table></figure>
<h3 id="中文预处理完整流程"><a class="header-anchor" href="#中文预处理完整流程">¶</a>中文预处理完整流程</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_chinese</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 1. 去除标点和特殊字符（保留中文、英文、数字）</span></span><br><span class="line">    text = re.sub(<span class="string">r'[^\u4e00-\u9fa5a-zA-Z0-9\s]'</span>, <span class="string">''</span>, text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 分词</span></span><br><span class="line">    tokens = jieba.lcut(text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 去除停用词</span></span><br><span class="line">    stopwords = <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, <span class="string">'都'</span>])</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> w.strip()]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 过滤单字（可选）</span></span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt; <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">text = <span class="string">"我在北京的清华大学学习自然语言处理，这是一门很有趣的课程！"</span></span><br><span class="line"><span class="built_in">print</span>(preprocess_chinese(text))</span><br><span class="line"><span class="comment"># ['北京', '清华大学', '学习', '自然语言处理', '一门', '有趣', '课程']</span></span><br></pre></td></tr></table></figure>
<h2 id="英文文本预处理：NLTK-与-spaCy"><a class="header-anchor" href="#英文文本预处理：NLTK-与-spaCy">¶</a>英文文本预处理：NLTK 与 spaCy</h2>
<h3 id="NLTK（Natural-Language-Toolkit）"><a class="header-anchor" href="#NLTK（Natural-Language-Toolkit）">¶</a>NLTK（Natural Language Toolkit）</h3>
<p>经典的 NLP 工具包，适合学习和原型开发。</p>
<p><strong>安装与下载资源</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install nltk</span><br><span class="line">python -c <span class="string">"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"</span></span><br></pre></td></tr></table></figure>
<p><strong>完整预处理流程</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_english</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 1. 小写化</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 分词</span></span><br><span class="line">    tokens = word_tokenize(text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 去除标点</span></span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w.isalnum()]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 去除停用词</span></span><br><span class="line">    stop_words = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 词形还原</span></span><br><span class="line">    lemmatizer = WordNetLemmatizer()</span><br><span class="line">    tokens = [lemmatizer.lemmatize(w, pos=<span class="string">'v'</span>) <span class="keyword">for</span> w <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">text = <span class="string">"I'm learning Natural Language Processing! It's amazing."</span></span><br><span class="line"><span class="built_in">print</span>(preprocess_english(text))</span><br><span class="line"><span class="comment"># ['learning', 'natural', 'language', 'processing', 'amazing']</span></span><br></pre></td></tr></table></figure>
<h3 id="spaCy"><a class="header-anchor" href="#spaCy">¶</a>spaCy</h3>
<p>工业级 NLP 库，速度快、功能强、支持多语言。</p>
<p><strong>安装与下载模型</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install spacy</span><br><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure>
<p><strong>使用 spaCy 预处理</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_with_spacy</span>(<span class="params">text</span>):</span><br><span class="line">    doc = nlp(text.lower())</span><br><span class="line">    tokens = [</span><br><span class="line">        token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> doc </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop <span class="keyword">and</span> <span class="keyword">not</span> token.is_punct <span class="keyword">and</span> token.is_alpha</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">text = <span class="string">"I'm learning Natural Language Processing! It's amazing."</span></span><br><span class="line"><span class="built_in">print</span>(preprocess_with_spacy(text))</span><br><span class="line"><span class="comment"># ['learn', 'natural', 'language', 'processing', 'amazing']</span></span><br></pre></td></tr></table></figure>
<p><strong>spaCy 的优势</strong>：</p>
<ul>
<li>自动进行词性标注、命名实体识别</li>
<li>速度快（用 Cython 优化）</li>
<li>支持中文模型（<code>zh_core_web_sm</code>）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">nlp = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line">text = <span class="string">"Apple is looking at buying U.K. startup for $1 billion"</span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命名实体识别</span></span><br><span class="line"><span class="keyword">for</span> ent <span class="keyword">in</span> doc.ents:</span><br><span class="line">    <span class="built_in">print</span>(ent.text, ent.label_)</span><br><span class="line"><span class="comment"># Apple ORG</span></span><br><span class="line"><span class="comment"># U.K. GPE</span></span><br><span class="line"><span class="comment"># $1 billion MONEY</span></span><br></pre></td></tr></table></figure>
<h2 id="文本表示：从词袋到-TF-IDF"><a class="header-anchor" href="#文本表示：从词袋到-TF-IDF">¶</a>文本表示：从词袋到 TF-IDF</h2>
<p>预处理后的文本是词的列表，但机器学习模型需要数值输入。如何把文本转为向量？</p>
<h3 id="词袋模型（Bag-of-Words，BoW）"><a class="header-anchor" href="#词袋模型（Bag-of-Words，BoW）">¶</a>词袋模型（Bag of Words，BoW）</h3>
<p>把文本表示为词频向量，忽略语序。</p>
<p><strong>示例</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">文档1: "我 喜欢 机器学习"</span><br><span class="line">文档2: "我 喜欢 深度学习"</span><br><span class="line">文档3: "机器学习 和 深度学习 很有趣"</span><br><span class="line"></span><br><span class="line">词汇表: ["我", "喜欢", "机器学习", "深度学习", "和", "很有趣"]</span><br><span class="line"></span><br><span class="line">向量表示:</span><br><span class="line">文档1: [1, 1, 1, 0, 0, 0]</span><br><span class="line">文档2: [1, 1, 0, 1, 0, 0]</span><br><span class="line">文档3: [0, 0, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>
<p><strong>Python 实现</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">"我 喜欢 机器学习"</span>,</span><br><span class="line">    <span class="string">"我 喜欢 深度学习"</span>,</span><br><span class="line">    <span class="string">"机器学习 和 深度学习 很有趣"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">X = vectorizer.fit_transform(corpus)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"词汇表:"</span>, vectorizer.get_feature_names_out())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"向量表示:\n"</span>, X.toarray())</span><br></pre></td></tr></table></figure>
<p><strong>局限性</strong>：</p>
<ul>
<li>高频常见词（“的”、“是”）权重过高</li>
<li>无法区分重要词和普通词</li>
</ul>
<h3 id="TF-IDF（Term-Frequency-Inverse-Document-Frequency）"><a class="header-anchor" href="#TF-IDF（Term-Frequency-Inverse-Document-Frequency）">¶</a>TF-IDF（Term Frequency - Inverse Document Frequency）</h3>
<p>TF-IDF 根据词的重要性加权：</p>
<ul>
<li>
<p><strong>TF（词频）</strong>：词在文档中出现的次数<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="34.238ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 15133.2 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(1375,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1764,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2125,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2569.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3089.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3756.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(4812.2,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text><path data-c="A0" d="" transform="translate(1000,0)"/></g><g data-mml-node="mi" transform="translate(1250,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mtext" transform="translate(1611,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">在</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><path data-c="A0" d="" transform="translate(3250,0)"/></g><g data-mml-node="mi" transform="translate(5111,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mtext" transform="translate(5631,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">中</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">次</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g><g data-mml-node="mrow" transform="translate(1650.5,-710)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><path data-c="A0" d="" transform="translate(2000,0)"/></g><g data-mml-node="mi" transform="translate(2250,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mtext" transform="translate(2770,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g><rect width="10081" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
</li>
<li>
<p><strong>IDF（逆文档频率）</strong>：词在所有文档中的稀有程度<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="35.563ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 15718.7 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(1778,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(2167,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2528,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3194.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(4250.6,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(5528.6,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(5695.2,0)"><g data-mml-node="mtext" transform="translate(3011.7,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">包</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">含</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><path data-c="A0" d="" transform="translate(3000,0)"/></g><g data-mml-node="mi" transform="translate(3250,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mtext" transform="translate(3611,0)"><path data-c="A0" d=""/><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">文</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">档</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(8083.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(9083.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><rect width="9783.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
</li>
<li>
<p><strong>TF-IDF</strong>：<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="32.899ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 14541.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/><path data-c="2D" d="M11 179V252H277V179H11Z" transform="translate(1375,0)"/><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z" transform="translate(1708,0)"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(2069,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(2833,0)"/></g><g data-mml-node="mo" transform="translate(3486,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(3875,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(4236,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(4680.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(5200.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(5867.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mtext" transform="translate(6923.2,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"/></g><g data-mml-node="mo" transform="translate(8298.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(8687.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(9048.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(9492.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(10012.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(10624.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mtext" transform="translate(11624.3,0)"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(13402.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(13791.3,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(14152.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></p>
</li>
</ul>
<p><strong>直觉解释</strong>：</p>
<ul>
<li>如果词在某篇文档中频繁出现（高 TF），且在其他文档中很少见（高 IDF），则它对该文档很重要</li>
<li>“的”、"是"在所有文档中都常见（低 IDF），所以权重低</li>
<li>"量子计算"只在少数文档出现（高 IDF），权重高</li>
</ul>
<p><strong>Python 实现</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">"我 喜欢 机器学习"</span>,</span><br><span class="line">    <span class="string">"我 喜欢 深度学习"</span>,</span><br><span class="line">    <span class="string">"机器学习 和 深度学习 很有趣"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer()</span><br><span class="line">X = vectorizer.fit_transform(corpus)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"TF-IDF 矩阵:\n"</span>, X.toarray())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"特征名:"</span>, vectorizer.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出示例：</span></span><br><span class="line"><span class="comment"># [[0.58 0.58 0.58 0.   0.   0.  ]  # 文档1</span></span><br><span class="line"><span class="comment">#  [0.58 0.58 0.   0.58 0.   0.  ]  # 文档2</span></span><br><span class="line"><span class="comment">#  [0.   0.   0.41 0.41 0.58 0.58]]  # 文档3</span></span><br></pre></td></tr></table></figure>
<h3 id="文本表示方法对比"><a class="header-anchor" href="#文本表示方法对比">¶</a>文本表示方法对比</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>词袋模型</td>
<td>简单、快速</td>
<td>丢失语序、稀疏、无法捕捉语义</td>
<td>文本分类、简单检索</td>
</tr>
<tr>
<td>TF-IDF</td>
<td>突出重要词</td>
<td>仍无法捕捉语义相似性</td>
<td>信息检索、关键词提取</td>
</tr>
<tr>
<td>Word2Vec</td>
<td>稠密向量、捕捉语义</td>
<td>无法处理一词多义</td>
<td>文本相似度、情感分析</td>
</tr>
<tr>
<td>BERT</td>
<td>上下文相关、语义理解强</td>
<td>计算开销大</td>
<td>复杂 NLP 任务、问答系统</td>
</tr>
</tbody>
</table>
<p>现代 NLP 任务更多使用 Word2Vec、BERT 等深度学习方法，但 TF-IDF 在简单任务中仍有用武之地。</p>
<h2 id="实战案例：构建文本分类器"><a class="header-anchor" href="#实战案例：构建文本分类器">¶</a>实战案例：构建文本分类器</h2>
<p>现在我们用前面学的预处理技术，构建一个完整的情感分类器。</p>
<h3 id="任务描述-v3"><a class="header-anchor" href="#任务描述-v3">¶</a>任务描述</h3>
<p>判断电影评论的情感是正面还是负面。</p>
<h3 id="数据准备-v7"><a class="header-anchor" href="#数据准备-v7">¶</a>数据准备</h3>
<p>使用 scikit-learn 自带的影评数据（实际项目中可替换为自己的数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的中文影评数据集</span></span><br><span class="line">reviews = [</span><br><span class="line">    <span class="string">"这部电影太精彩了，强烈推荐"</span>,</span><br><span class="line">    <span class="string">"剧情拖沓，演技尴尬，浪费时间"</span>,</span><br><span class="line">    <span class="string">"导演功力深厚，画面唯美，值得一看"</span>,</span><br><span class="line">    <span class="string">"烂片，看了十分钟就想离场"</span>,</span><br><span class="line">    <span class="string">"演员表现出色，剧情引人入胜"</span>,</span><br><span class="line">    <span class="string">"完全是烂俗套路，毫无新意"</span>,</span><br><span class="line">    <span class="string">"特效震撼，故事感人，五星好评"</span>,</span><br><span class="line">    <span class="string">"无聊至极，不推荐"</span>,</span><br><span class="line">    <span class="string">"这是我今年看过最好的电影"</span>,</span><br><span class="line">    <span class="string">"差评，根本不值票价"</span>,</span><br><span class="line">    <span class="string">"演技炸裂，每个镜头都是精心设计"</span>,</span><br><span class="line">    <span class="string">"剧情漏洞百出，逻辑混乱"</span>,</span><br><span class="line">    <span class="string">"音乐动人，情感真挚，催人泪下"</span>,</span><br><span class="line">    <span class="string">"浪费我两个小时，强烈不推荐"</span>,</span><br><span class="line">    <span class="string">"制作精良，诚意满满，良心之作"</span>,</span><br><span class="line">    <span class="string">"看完只想骂人，烂透了"</span>,</span><br><span class="line">    <span class="string">"笑点密集，轻松愉快，适合全家观看"</span>,</span><br><span class="line">    <span class="string">"尴尬癌都犯了，实在看不下去"</span>,</span><br><span class="line">    <span class="string">"细节丰富，伏笔精妙，值得二刷"</span>,</span><br><span class="line">    <span class="string">"毫无亮点，彻底失望"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 1=正面, 0=负面</span></span><br></pre></td></tr></table></figure>
<h3 id="预处理函数"><a class="header-anchor" href="#预处理函数">¶</a>预处理函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载停用词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_stopwords</span>():</span><br><span class="line">    stopwords = <span class="built_in">set</span>([</span><br><span class="line">        <span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, </span><br><span class="line">        <span class="string">'都'</span>, <span class="string">'一'</span>, <span class="string">'一个'</span>, <span class="string">'上'</span>, <span class="string">'也'</span>, <span class="string">'很'</span>, <span class="string">'到'</span>, <span class="string">'说'</span>, <span class="string">'要'</span>, <span class="string">'去'</span>,</span><br><span class="line">        <span class="string">'你'</span>, <span class="string">'会'</span>, <span class="string">'着'</span>, <span class="string">'没有'</span>, <span class="string">'看'</span>, <span class="string">'好'</span>, <span class="string">'自己'</span>, <span class="string">'这'</span></span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 去除标点</span></span><br><span class="line">    text = re.sub(<span class="string">r'[^\u4e00-\u9fa5a-zA-Z0-9]'</span>, <span class="string">' '</span>, text)</span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    tokens = jieba.lcut(text)</span><br><span class="line">    <span class="comment"># 去停用词</span></span><br><span class="line">    stopwords = load_stopwords()</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> <span class="built_in">len</span>(w) &gt; <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理所有评论</span></span><br><span class="line">processed_reviews = [preprocess(review) <span class="keyword">for</span> review <span class="keyword">in</span> reviews]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"预处理示例:"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"原始: <span class="subst">{reviews[<span class="number">0</span>]}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"处理后: <span class="subst">{processed_reviews[<span class="number">0</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="特征提取与模型训练"><a class="header-anchor" href="#特征提取与模型训练">¶</a>特征提取与模型训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    processed_reviews, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-IDF 向量化</span></span><br><span class="line">vectorizer = TfidfVectorizer(max_features=<span class="number">100</span>)</span><br><span class="line">X_train_tfidf = vectorizer.fit_transform(X_train)</span><br><span class="line">X_test_tfidf = vectorizer.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练朴素贝叶斯分类器</span></span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(X_train_tfidf, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = classifier.predict(X_test_tfidf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"准确率: <span class="subst">{accuracy_score(y_test, y_pred):<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n分类报告:"</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=[<span class="string">'负面'</span>, <span class="string">'正面'</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="预测新评论"><a class="header-anchor" href="#预测新评论">¶</a>预测新评论</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_sentiment</span>(<span class="params">text</span>):</span><br><span class="line">    processed = preprocess(text)</span><br><span class="line">    tfidf = vectorizer.transform([processed])</span><br><span class="line">    prediction = classifier.predict(tfidf)[<span class="number">0</span>]</span><br><span class="line">    proba = classifier.predict_proba(tfidf)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    sentiment = <span class="string">"正面"</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"负面"</span></span><br><span class="line">    confidence = proba[prediction] * <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"评论: <span class="subst">{text}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"预测: <span class="subst">{sentiment}</span> (置信度: <span class="subst">{confidence:<span class="number">.1</span>f}</span>%)"</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试新数据</span></span><br><span class="line">predict_sentiment(<span class="string">"这部电影非常精彩，推荐大家去看"</span>)</span><br><span class="line">predict_sentiment(<span class="string">"太烂了，完全是浪费时间"</span>)</span><br><span class="line">predict_sentiment(<span class="string">"还可以，中规中矩"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="完整代码整合"><a class="header-anchor" href="#完整代码整合">¶</a>完整代码整合</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">reviews = [</span><br><span class="line">    <span class="string">"这部电影太精彩了，强烈推荐"</span>, <span class="string">"剧情拖沓，演技尴尬，浪费时间"</span>,</span><br><span class="line">    <span class="string">"导演功力深厚，画面唯美，值得一看"</span>, <span class="string">"烂片，看了十分钟就想离场"</span>,</span><br><span class="line">    <span class="string">"演员表现出色，剧情引人入胜"</span>, <span class="string">"完全是烂俗套路，毫无新意"</span>,</span><br><span class="line">    <span class="string">"特效震撼，故事感人，五星好评"</span>, <span class="string">"无聊至极，不推荐"</span>,</span><br><span class="line">    <span class="string">"这是我今年看过最好的电影"</span>, <span class="string">"差评，根本不值票价"</span>,</span><br><span class="line">    <span class="string">"演技炸裂，每个镜头都是精心设计"</span>, <span class="string">"剧情漏洞百出，逻辑混乱"</span>,</span><br><span class="line">    <span class="string">"音乐动人，情感真挚，催人泪下"</span>, <span class="string">"浪费我两个小时，强烈不推荐"</span>,</span><br><span class="line">    <span class="string">"制作精良，诚意满满，良心之作"</span>, <span class="string">"看完只想骂人，烂透了"</span>,</span><br><span class="line">    <span class="string">"笑点密集，轻松愉快，适合全家观看"</span>, <span class="string">"尴尬癌都犯了，实在看不下去"</span>,</span><br><span class="line">    <span class="string">"细节丰富，伏笔精妙，值得二刷"</span>, <span class="string">"毫无亮点，彻底失望"</span></span><br><span class="line">]</span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">stopwords = <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, <span class="string">'在'</span>, <span class="string">'是'</span>, <span class="string">'我'</span>, <span class="string">'有'</span>, <span class="string">'和'</span>, <span class="string">'就'</span>, <span class="string">'不'</span>, <span class="string">'人'</span>, <span class="string">'都'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text</span>):</span><br><span class="line">    text = re.sub(<span class="string">r'[^\u4e00-\u9fa5]'</span>, <span class="string">' '</span>, text)</span><br><span class="line">    tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> jieba.lcut(text) <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> <span class="built_in">len</span>(w) &gt; <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tokens)</span><br><span class="line"></span><br><span class="line">processed = [preprocess(r) <span class="keyword">for</span> r <span class="keyword">in</span> reviews]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(processed, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">vectorizer = TfidfVectorizer()</span><br><span class="line">X_train_vec = vectorizer.fit_transform(X_train)</span><br><span class="line">X_test_vec = vectorizer.transform(X_test)</span><br><span class="line"></span><br><span class="line">clf = MultinomialNB()</span><br><span class="line">clf.fit(X_train_vec, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line">y_pred = clf.predict(X_test_vec)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"准确率: <span class="subst">{accuracy_score(y_test, y_pred):<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测新样本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">text</span>):</span><br><span class="line">    vec = vectorizer.transform([preprocess(text)])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"正面"</span> <span class="keyword">if</span> clf.predict(vec)[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"负面"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"电影拍得很棒，非常感动"</span>))  <span class="comment"># 正面</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"太差了，不想看"</span>))  <span class="comment"># 负面</span></span><br></pre></td></tr></table></figure>
<h3 id="模型改进方向"><a class="header-anchor" href="#模型改进方向">¶</a>模型改进方向</h3>
<ol>
<li><strong>扩充数据集</strong>：20 个样本太少，实际需要数千到数万样本</li>
<li><strong>调整停用词</strong>：根据任务定制停用词表</li>
<li><strong>尝试其他模型</strong>：逻辑回归、SVM、LSTM</li>
<li><strong>使用预训练模型</strong>：BERT 中文模型（如 <code>bert-base-chinese</code>）</li>
<li><strong>考虑否定词</strong>：处理"不好"、"不推荐"等否定结构</li>
</ol>
<h2 id="❓-Q-A-NLP基础常见问题"><a class="header-anchor" href="#❓-Q-A-NLP基础常见问题">¶</a>❓ Q&amp;A: NLP基础常见问题</h2>
<h3 id="Q1-中文分词和英文分词的本质区别是什么？"><a class="header-anchor" href="#Q1-中文分词和英文分词的本质区别是什么？">¶</a>Q1: 中文分词和英文分词的本质区别是什么？</h3>
<p><strong>A</strong>: 英文单词间有天然分隔符（空格），分词只需按空格切分（复杂情况才需处理缩写如 “don’t”）。中文是连续字符流，没有明确边界，需要算法判断哪些字组成词：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">英文: "I love NLP" → 天然分隔 → ["I", "love", "NLP"]</span><br><span class="line">中文: "我爱NLP" → 需算法 → ["我", "爱", "NLP"] 或 ["我爱", "NLP"]</span><br></pre></td></tr></table></figure>
<p>中文分词的难点：</p>
<ul>
<li><strong>歧义消解</strong>：“乒乓球拍卖” → “乒乓球/拍卖” 还是 “乒乓/球拍/卖”？</li>
<li><strong>新词识别</strong>：“奥利给”、“yyds” 等网络词汇不在词典中</li>
<li><strong>领域适应</strong>：医疗、法律等领域有专门术语</li>
</ul>
<h3 id="Q2-词干化和词形还原应该选哪个？"><a class="header-anchor" href="#Q2-词干化和词形还原应该选哪个？">¶</a>Q2: 词干化和词形还原应该选哪个？</h3>
<p><strong>A</strong>: 看场景：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>推荐方法</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>信息检索、搜索引擎</td>
<td>词干化</td>
<td>快速，允许过匹配（“running” 和 “runner” 都匹配 “run”）</td>
</tr>
<tr>
<td>文本分类、情感分析</td>
<td>词形还原</td>
<td>准确，避免产生不存在的词</td>
</tr>
<tr>
<td>实时系统</td>
<td>词干化</td>
<td>速度优先</td>
</tr>
<tr>
<td>学术研究</td>
<td>词形还原</td>
<td>质量优先</td>
</tr>
</tbody>
</table>
<p><strong>实例对比</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer, WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line">words = [<span class="string">"studies"</span>, <span class="string">"studying"</span>, <span class="string">"better"</span>, <span class="string">"worse"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"<span class="subst">{w}</span>: stem=<span class="subst">{stemmer.stem(w)}</span>, lemma=<span class="subst">{lemmatizer.lemmatize(w, pos=<span class="string">'v'</span>)}</span>"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># studies: stem=studi, lemma=study</span></span><br><span class="line"><span class="comment"># studying: stem=studi, lemma=study</span></span><br><span class="line"><span class="comment"># better: stem=better, lemma=better (需要指定pos='a'才能还原为good)</span></span><br><span class="line"><span class="comment"># worse: stem=wors, lemma=worse</span></span><br></pre></td></tr></table></figure>
<h3 id="Q3-TF-IDF-的-IDF-为什么要取对数？"><a class="header-anchor" href="#Q3-TF-IDF-的-IDF-为什么要取对数？">¶</a>Q3: TF-IDF 的 IDF 为什么要取对数？</h3>
<p><strong>A</strong>: 三个原因：</p>
<ol>
<li>
<p><strong>数值稳定</strong>：假设总文档数 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="10.682ex" height="1.731ex" role="img" focusable="false" viewbox="0 -683 4721.6 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(2000,0)"/></g></g></g></svg></mjx-container>，某词出现在 1 个文档中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.984ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 8833.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(2055.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(3111.6,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3999.6,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mn" transform="translate(4499.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(5277.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(6333.1,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(2000,0)"/></g></g></g></svg></mjx-container>；另一词出现在 10 个文档中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="11.565ex" height="1.731ex" role="img" focusable="false" viewbox="0 -683 5111.6 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"/><path data-c="44" d="M130 622Q123 629 119 631T103 634T60 637H27V683H228Q399 682 419 682T461 676Q504 667 546 641T626 573T685 470T708 336Q708 210 634 116T442 3Q429 1 228 0H27V46H60Q102 47 111 49T130 61V622ZM593 338Q593 439 571 501T493 602Q439 637 355 637H322H294Q238 637 234 628Q231 624 231 344Q231 62 232 59Q233 49 248 48T339 46H350Q456 46 515 95Q561 133 577 191T593 338Z" transform="translate(361,0)"/><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1125,0)"/></g><g data-mml-node="mo" transform="translate(2055.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(3111.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/></g></g></g></svg></mjx-container>。差距过大会导致数值不稳定。</p>
</li>
<li>
<p><strong>符合人类直觉</strong>：词的重要性不应线性增长。出现在 1 篇 vs 2 篇文档的区别，比出现在 100 篇 vs 101 篇的区别更显著。</p>
</li>
<li>
<p><strong>匹配信息论</strong>：在信息论中，事件概率为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container> 的信息量是 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex;" xmlns="http://www.w3.org/2000/svg" width="6.544ex" height="2.036ex" role="img" focusable="false" viewbox="0 -694 2892.3 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(944.7,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(2222.7,0)"><path data-c="2061" d=""/></g><g data-mml-node="mi" transform="translate(2389.3,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container>。词出现在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container> 篇文档的概率是 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.498ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 1988 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(600,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mi" transform="translate(1100,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g></svg></mjx-container>，信息量为：<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.577ex;" xmlns="http://www.w3.org/2000/svg" width="17.7ex" height="4.652ex" role="img" focusable="false" viewbox="0 -1359 7823.6 2056"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(944.7,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(2222.7,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(2389.3,0)"><g data-mml-node="mi" transform="translate(364,676)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><rect width="1088" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(3995.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(5050.9,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(6328.9,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(6495.6,0)"><g data-mml-node="mi" transform="translate(220,676)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(364,-686)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><rect width="1088" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
</li>
</ol>
<p><strong>对比</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">N = <span class="number">10000</span>  <span class="comment"># 总文档数</span></span><br><span class="line">docs_contain = [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"不取对数:"</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> docs_contain:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"  出现在 <span class="subst">{n}</span> 篇: IDF = <span class="subst">{N/n}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n取对数:"</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> docs_contain:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"  出现在 <span class="subst">{n}</span> 篇: IDF = <span class="subst">{math.log(N/n):<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不取对数: 10000, 1000, 100, 10 (线性衰减)</span></span><br><span class="line"><span class="comment"># 取对数: 9.21, 6.91, 4.61, 2.30 (对数衰减，更平滑)</span></span><br></pre></td></tr></table></figure>
<h3 id="Q4-为什么要去除停用词？会不会丢失信息？"><a class="header-anchor" href="#Q4-为什么要去除停用词？会不会丢失信息？">¶</a>Q4: 为什么要去除停用词？会不会丢失信息？</h3>
<p><strong>A</strong>: 去除停用词有明确好处：</p>
<ul>
<li><strong>降维</strong>：词汇表从 10 万降到 5 万，节省存储和计算</li>
<li><strong>提升效果</strong>：高频低义词（“的”、“了”）会掩盖关键词</li>
</ul>
<p>但确实可能丢失信息，尤其是：</p>
<ul>
<li><strong>情感分析</strong>：“不好” 中的 “不” 是停用词但表否定</li>
<li><strong>问答系统</strong>：“who”、“when” 是疑问词，不应删除</li>
<li><strong>短文本</strong>：微博、评论本来就短，再删停用词就没什么了</li>
</ul>
<p><strong>权衡策略</strong>：</p>
<ol>
<li>任务相关停用词表（情感分析保留否定词）</li>
<li>对比实验（有/无停用词的模型效果）</li>
<li>深度学习模型可以不去停用词（模型自己学会忽略）</li>
</ol>
<h3 id="Q5-词袋模型丢失了语序，为什么还能工作？"><a class="header-anchor" href="#Q5-词袋模型丢失了语序，为什么还能工作？">¶</a>Q5: 词袋模型丢失了语序，为什么还能工作？</h3>
<p><strong>A</strong>: 对于某些任务，语序不是最关键的：</p>
<p><strong>文本分类示例</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正面评论: "电影很好，我喜欢"</span><br><span class="line">负面评论: "我喜欢好电影，但这部很差"</span><br></pre></td></tr></table></figure>
<p>词袋模型看到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正面: {电影:1, 很:1, 好:1, 我:1, 喜欢:1}</span><br><span class="line">负面: {我:1, 喜欢:1, 好:1, 电影:1, 但:1, 这:1, 部:1, 很:1, 差:1}</span><br></pre></td></tr></table></figure>
<p>虽然丢失了语序，但"差"、"但"的出现本身就是强信号。</p>
<p><strong>但在这些任务中词袋模型会失效</strong>：</p>
<ul>
<li><strong>机器翻译</strong>：“猫追老鼠” ≠ “老鼠追猫”</li>
<li><strong>问答系统</strong>：“谁打了谁” 需要知道主宾关系</li>
<li><strong>文本生成</strong>：生成的句子需要符合语法</li>
</ul>
<p>现代 NLP 用 RNN、Transformer 等模型保留语序信息。</p>
<h3 id="Q6-jieba-的精确模式、全模式、搜索引擎模式有什么区别？"><a class="header-anchor" href="#Q6-jieba-的精确模式、全模式、搜索引擎模式有什么区别？">¶</a>Q6: jieba 的精确模式、全模式、搜索引擎模式有什么区别？</h3>
<p><strong>A</strong>: 三种模式的切分粒度不同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">text = <span class="string">"我来到北京清华大学"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"精确模式:"</span>, jieba.lcut(text, cut_all=<span class="literal">False</span>))</span><br><span class="line"><span class="comment"># ['我', '来到', '北京', '清华大学']</span></span><br><span class="line"><span class="comment"># → 适合文本分析，每个词只出现一次</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"全模式:"</span>, jieba.lcut(text, cut_all=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># ['我', '来到', '北京', '清华', '清华大学', '华大', '大学']</span></span><br><span class="line"><span class="comment"># → 所有可能的词都输出，有冗余</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"搜索引擎模式:"</span>, jieba.lcut_for_search(text))</span><br><span class="line"><span class="comment"># ['我', '来到', '北京', '清华', '华大', '大学', '清华大学']</span></span><br><span class="line"><span class="comment"># → 在精确模式基础上，对长词再切分</span></span><br></pre></td></tr></table></figure>
<p><strong>使用场景</strong>：</p>
<ul>
<li><strong>精确模式</strong>：文本分类、情感分析（默认选择）</li>
<li><strong>全模式</strong>：关键词提取、词云（需要更多候选词）</li>
<li><strong>搜索引擎模式</strong>：搜索召回（“清华大学"被切为"清华”+“大学”+“清华大学”，用户搜"清华"也能匹配）</li>
</ul>
<h3 id="Q7-BERT-这么强大，还需要学习传统预处理吗？"><a class="header-anchor" href="#Q7-BERT-这么强大，还需要学习传统预处理吗？">¶</a>Q7: BERT 这么强大，还需要学习传统预处理吗？</h3>
<p><strong>A</strong>: 必须学，原因有四：</p>
<ol>
<li><strong>轻量级场景</strong>：嵌入式设备、实时系统跑不动 BERT（参数量上亿）</li>
<li><strong>数据稀缺时</strong>：标注数据少于 1000 条时，简单模型 + TF-IDF 可能比 BERT 微调效果好</li>
<li><strong>可解释性</strong>：TF-IDF 可以告诉你哪些词重要，BERT 是黑盒</li>
<li><strong>成本考虑</strong>：训练一次 BERT 可能花费数千元 GPU 费用，朴素贝叶斯几秒钟</li>
</ol>
<p><strong>实际项目流程</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一步：用 TF-IDF + 逻辑回归建立 baseline（1小时）</span><br><span class="line">第二步：评估效果，如果满足需求就部署（节省成本）</span><br><span class="line">第三步：不满足才上 BERT（几天调参 + 数千元成本）</span><br></pre></td></tr></table></figure>
<h3 id="Q8-中文预处理时要不要转繁体为简体？"><a class="header-anchor" href="#Q8-中文预处理时要不要转繁体为简体？">¶</a>Q8: 中文预处理时要不要转繁体为简体？</h3>
<p><strong>A</strong>: 看数据来源：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>是否转换</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>用户输入（搜索、评论）</td>
<td>转换</td>
<td>用户可能混用，统一避免"臺灣"和"台湾"被当作不同词</td>
</tr>
<tr>
<td>历史文献、古籍</td>
<td>不转换</td>
<td>繁体字承载语义信息（“乾坤” ≠ “干坤”）</td>
</tr>
<tr>
<td>台湾、香港数据</td>
<td>看下游任务</td>
<td>如果和大陆数据混合训练，建议转换</td>
</tr>
</tbody>
</table>
<p><strong>转换工具</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> opencc <span class="keyword">import</span> OpenCC</span><br><span class="line"></span><br><span class="line">cc = OpenCC(<span class="string">'t2s'</span>)  <span class="comment"># 繁体转简体</span></span><br><span class="line">text_traditional = <span class="string">"我愛臺灣"</span></span><br><span class="line">text_simplified = cc.convert(text_traditional)</span><br><span class="line"><span class="built_in">print</span>(text_simplified)  <span class="comment"># "我爱台湾"</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：转换可能有歧义，如"乾燥" → “干燥”，但 “乾隆” → “乾隆”（不应转换）。</p>
<h3 id="Q9-如何评估分词质量？"><a class="header-anchor" href="#Q9-如何评估分词质量？">¶</a>Q9: 如何评估分词质量？</h3>
<p><strong>A</strong>: 三个指标（需要人工标注的标准答案）：</p>
<ol>
<li>
<p><strong>准确率（Precision）</strong>：分出来的词有多少是对的<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="23.811ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 10524.6 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(1028.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(2084.6,0)"><g data-mml-node="mtext" transform="translate(720,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">确</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">切</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mtext" transform="translate(220,-710)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">模</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">型</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">切</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(7000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><rect width="8200" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
</li>
<li>
<p><strong>召回率（Recall）</strong>：标准答案中的词有多少被找到<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="23.829ex" height="5.285ex" role="img" focusable="false" viewbox="0 -1426 10532.6 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g><g data-mml-node="mo" transform="translate(1036.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(2092.6,0)"><g data-mml-node="mtext" transform="translate(720,676)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">确</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">切</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mtext" transform="translate(220,-710)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">准</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">答</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">案</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">总</text><text data-variant="normal" transform="translate(6000,0) scale(1,-1)" font-size="884px" font-family="serif">词</text><text data-variant="normal" transform="translate(7000,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><rect width="8200" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
</li>
<li>
<p><strong>F1 值</strong>：准确率和召回率的调和平均<br>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.738ex;" xmlns="http://www.w3.org/2000/svg" width="16.917ex" height="4.812ex" role="img" focusable="false" viewbox="0 -1359 7477.4 2127"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"/></g><g data-mml-node="mn" transform="translate(749,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(1526.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(2582.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mo" transform="translate(3304.8,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mfrac" transform="translate(4305,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(973.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mi" transform="translate(1973.4,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(973.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1973.4,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g></g><rect width="2932.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
</li>
</ol>
<p><strong>示例</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">原句: "我爱自然语言处理"</span><br><span class="line">标准答案: ["我", "爱", "自然语言处理"]</span><br><span class="line">模型输出: ["我", "爱", "自然", "语言", "处理"]</span><br><span class="line"></span><br><span class="line">正确切分: "我", "爱" (2个)</span><br><span class="line">模型总词数: 5</span><br><span class="line">标准答案总词数: 3</span><br><span class="line"></span><br><span class="line">准确率: 2/5 = 0.4</span><br><span class="line">召回率: 2/3 = 0.67</span><br><span class="line">F1: 2 × (0.4 × 0.67) / (0.4 + 0.67) = 0.5</span><br></pre></td></tr></table></figure>
<p>实际中更常用<strong>下游任务指标</strong>（如分类准确率）评估：分词好 → 分类效果好。</p>
<h3 id="Q10-实际项目中预处理流程应该怎么设计？"><a class="header-anchor" href="#Q10-实际项目中预处理流程应该怎么设计？">¶</a>Q10: 实际项目中预处理流程应该怎么设计？</h3>
<p><strong>A</strong>: 遵循这个检查清单：</p>
<p><strong>第一步：分析数据特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查数据长度分布</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame({<span class="string">'text'</span>: your_texts})</span><br><span class="line">df[<span class="string">'length'</span>] = df[<span class="string">'text'</span>].apply(<span class="built_in">len</span>)</span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">'length'</span>].describe())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否有特殊字符</span></span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">bool</span>(re.search(<span class="string">r'[^\u4e00-\u9fa5a-zA-Z0-9]'</span>, x))).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>
<p><strong>第二步：确定预处理步骤</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">短文本（&lt;50字）：保守去停用词（避免信息丢失）</span><br><span class="line">长文本（&gt;200字）：积极去停用词 + 过滤低频词</span><br><span class="line">含emoji/表情：看任务，情感分析可能需要保留</span><br><span class="line">含URL/邮箱：替换为特殊标记 &lt;URL&gt; &lt;EMAIL&gt;</span><br><span class="line">含数字：分类任务可删除，命名实体识别要保留</span><br></pre></td></tr></table></figure>
<p><strong>第三步：模块化实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextPreprocessor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, remove_stopwords=<span class="literal">True</span>, min_word_len=<span class="number">1</span></span>):</span><br><span class="line">        self.remove_stopwords = remove_stopwords</span><br><span class="line">        self.min_word_len = min_word_len</span><br><span class="line">        self.stopwords = self.load_stopwords()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_stopwords</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 加载停用词</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">set</span>([<span class="string">'的'</span>, <span class="string">'了'</span>, ...])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clean</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># 清洗文本</span></span><br><span class="line">        text = re.sub(<span class="string">r'http\S+'</span>, <span class="string">'&lt;URL&gt;'</span>, text)</span><br><span class="line">        text = re.sub(<span class="string">r'\S+@\S+'</span>, <span class="string">'&lt;EMAIL&gt;'</span>, text)</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># 分词</span></span><br><span class="line">        <span class="keyword">return</span> jieba.lcut(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter_tokens</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="comment"># 过滤</span></span><br><span class="line">        <span class="keyword">if</span> self.remove_stopwords:</span><br><span class="line">            tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> self.stopwords]</span><br><span class="line">        tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt;= self.min_word_len]</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self, text</span>):</span><br><span class="line">        text = self.clean(text)</span><br><span class="line">        tokens = self.tokenize(text)</span><br><span class="line">        tokens = self.filter_tokens(tokens)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">preprocessor = TextPreprocessor(remove_stopwords=<span class="literal">True</span>, min_word_len=<span class="number">2</span>)</span><br><span class="line">processed = preprocessor.process(<span class="string">"我在北京的清华大学学习NLP"</span>)</span><br><span class="line"><span class="built_in">print</span>(processed)</span><br></pre></td></tr></table></figure>
<p><strong>第四步：A/B 测试</strong></p>
<p>对比不同预处理方案的下游效果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">方案A: 去停用词 + 词形还原 → 准确率 85%</span><br><span class="line">方案B: 保留停用词 + 只小写化 → 准确率 87% ✓</span><br></pre></td></tr></table></figure>
<p>记住：<strong>预处理没有银弹，一切以下游任务效果为准</strong>。</p>
<h2 id="总结与展望-v2"><a class="header-anchor" href="#总结与展望-v2">¶</a>总结与展望</h2>
<p>文本预处理是 NLP 的基础设施，虽然深度学习降低了手工特征工程的需求，但理解这些经典方法仍然重要。你已经掌握了：</p>
<ul>
<li>NLP 的发展脉络和应用场景</li>
<li>中英文文本预处理的完整流程</li>
<li>jieba、NLTK、spaCy 等工具的使用</li>
<li>词袋模型和 TF-IDF 的原理与实现</li>
<li>端到端的文本分类实战案例</li>
</ul>
<p>下一篇文章将介绍<strong>词向量与语言模型</strong>，探讨 Word2Vec、GloVe、ELMo 如何把词表示为稠密向量，以及预训练语言模型的原理。敬请期待！</p>
<hr>
<p><strong>推荐资源</strong>：</p>
<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://github.com/fxsjy/jieba">jieba 官方文档<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://www.nltk.org/book/">NLTK Book（免费在线书）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://spacy.io/usage">spaCy 使用指南<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction">scikit-learn 文本特征提取教程<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p><strong>代码仓库</strong>：本文完整代码已上传至 <a href="#">GitHub</a>（请根据实际情况替换链接）</p>
<p>如果觉得有帮助，欢迎点赞分享！有任何问题欢迎在评论区讨论。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>本文标题：自然语言处理（一）—— NLP入门与文本预处理</li>
        <li>本文作者：Chen Kai</li>
        <li>创建时间：2025-02-01 00:00:00</li>
        <li>
            本文链接：https://www.chenk.top/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94-NLP%E5%85%A5%E9%97%A8%E4%B8%8E%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Deep-Learning/">#Deep Learning</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94-%E8%AF%8D%E5%90%91%E9%87%8F%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">自然语言处理（二）—— 词向量与语言模型</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5%E4%B8%8E%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">网络基础：原理、故障排查与网络模式</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- 由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E4%BB%8E%E7%AC%A6%E5%8F%B7%E4%B8%BB%E4%B9%89%E5%88%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF"><span class="nav-number">1.</span> <span class="nav-text">NLP 是什么？从符号主义到大模型的演进之路</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-number">1.1.</span> <span class="nav-text">自然语言处理的本质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NLP-%E5%8F%91%E5%B1%95%E7%9A%84%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.</span> <span class="nav-text">NLP 发展的四个阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A%E7%AC%A6%E5%8F%B7%E4%B8%BB%E4%B9%89%EF%BC%881950s-1980s%EF%BC%89"><span class="nav-number">1.2.1.</span> <span class="nav-text">第一阶段：符号主义（1950s-1980s）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95%EF%BC%881990s-2010s%EF%BC%89"><span class="nav-number">1.2.2.</span> <span class="nav-text">第二阶段：统计方法（1990s-2010s）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%882013-2020%EF%BC%89"><span class="nav-number">1.2.3.</span> <span class="nav-text">第三阶段：深度学习（2013-2020）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E9%98%B6%E6%AE%B5%EF%BC%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%882018%E8%87%B3%E4%BB%8A%EF%BC%89"><span class="nav-number">1.2.4.</span> <span class="nav-text">第四阶段：预训练大模型（2018至今）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP-%E7%9A%84%E7%8E%B0%E5%AE%9E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">2.</span> <span class="nav-text">NLP 的现实应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E"><span class="nav-number">2.1.</span> <span class="nav-text">搜索引擎</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="nav-number">2.2.</span> <span class="nav-text">机器翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D"><span class="nav-number">2.3.</span> <span class="nav-text">智能客服</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">2.4.</span> <span class="nav-text">推荐系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="nav-number">2.5.</span> <span class="nav-text">情感分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9F"><span class="nav-number">3.</span> <span class="nav-text">为什么需要文本预处理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B"><span class="nav-number">4.</span> <span class="nav-text">文本预处理的核心流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88Normalization%EF%BC%89"><span class="nav-number">4.1.</span> <span class="nav-text">标准化（Normalization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E8%AF%8D%EF%BC%88Tokenization%EF%BC%89"><span class="nav-number">4.2.</span> <span class="nav-text">分词（Tokenization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D%EF%BC%88Stop-Words-Removal%EF%BC%89"><span class="nav-number">4.3.</span> <span class="nav-text">去除停用词（Stop Words Removal）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E5%B9%B2%E5%8C%96%E4%B8%8E%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F"><span class="nav-number">4.4.</span> <span class="nav-text">词干化与词形还原</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%EF%BC%9A%E6%8C%91%E6%88%98%E4%B8%8E%E5%B7%A5%E5%85%B7"><span class="nav-number">5.</span> <span class="nav-text">中文分词：挑战与工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E7%89%B9%E6%AE%8A%E6%80%A7"><span class="nav-number">5.1.</span> <span class="nav-text">中文分词的特殊性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%B5%81%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7"><span class="nav-number">5.2.</span> <span class="nav-text">主流中文分词工具</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jieba%EF%BC%88%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8D%EF%BC%89"><span class="nav-number">5.2.1.</span> <span class="nav-text">jieba（结巴分词）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pkuseg"><span class="nav-number">5.2.2.</span> <span class="nav-text">pkuseg</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LAC%EF%BC%88Lexical-Analysis-of-Chinese%EF%BC%89"><span class="nav-number">5.2.3.</span> <span class="nav-text">LAC（Lexical Analysis of Chinese）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B"><span class="nav-number">5.3.</span> <span class="nav-text">中文预处理完整流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8B%B1%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9ANLTK-%E4%B8%8E-spaCy"><span class="nav-number">6.</span> <span class="nav-text">英文文本预处理：NLTK 与 spaCy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#NLTK%EF%BC%88Natural-Language-Toolkit%EF%BC%89"><span class="nav-number">6.1.</span> <span class="nav-text">NLTK（Natural Language Toolkit）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spaCy"><span class="nav-number">6.2.</span> <span class="nav-text">spaCy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%EF%BC%9A%E4%BB%8E%E8%AF%8D%E8%A2%8B%E5%88%B0-TF-IDF"><span class="nav-number">7.</span> <span class="nav-text">文本表示：从词袋到 TF-IDF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%EF%BC%88Bag-of-Words%EF%BC%8CBoW%EF%BC%89"><span class="nav-number">7.1.</span> <span class="nav-text">词袋模型（Bag of Words，BoW）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TF-IDF%EF%BC%88Term-Frequency-Inverse-Document-Frequency%EF%BC%89"><span class="nav-number">7.2.</span> <span class="nav-text">TF-IDF（Term Frequency - Inverse Document Frequency）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="nav-number">7.3.</span> <span class="nav-text">文本表示方法对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%EF%BC%9A%E6%9E%84%E5%BB%BA%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">8.</span> <span class="nav-text">实战案例：构建文本分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0-v3"><span class="nav-number">8.1.</span> <span class="nav-text">任务描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-v7"><span class="nav-number">8.2.</span> <span class="nav-text">数据准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="nav-number">8.3.</span> <span class="nav-text">预处理函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">8.4.</span> <span class="nav-text">特征提取与模型训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E6%96%B0%E8%AF%84%E8%AE%BA"><span class="nav-number">8.5.</span> <span class="nav-text">预测新评论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88"><span class="nav-number">8.6.</span> <span class="nav-text">完整代码整合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B%E6%96%B9%E5%90%91"><span class="nav-number">8.7.</span> <span class="nav-text">模型改进方向</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%9D%93-Q-A-NLP%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">9.</span> <span class="nav-text">❓ Q&amp;A: NLP基础常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Q1-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%92%8C%E8%8B%B1%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">9.1.</span> <span class="nav-text">Q1: 中文分词和英文分词的本质区别是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q2-%E8%AF%8D%E5%B9%B2%E5%8C%96%E5%92%8C%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%E5%BA%94%E8%AF%A5%E9%80%89%E5%93%AA%E4%B8%AA%EF%BC%9F"><span class="nav-number">9.2.</span> <span class="nav-text">Q2: 词干化和词形还原应该选哪个？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q3-TF-IDF-%E7%9A%84-IDF-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8F%96%E5%AF%B9%E6%95%B0%EF%BC%9F"><span class="nav-number">9.3.</span> <span class="nav-text">Q3: TF-IDF 的 IDF 为什么要取对数？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q4-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D%EF%BC%9F%E4%BC%9A%E4%B8%8D%E4%BC%9A%E4%B8%A2%E5%A4%B1%E4%BF%A1%E6%81%AF%EF%BC%9F"><span class="nav-number">9.4.</span> <span class="nav-text">Q4: 为什么要去除停用词？会不会丢失信息？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q5-%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E4%B8%A2%E5%A4%B1%E4%BA%86%E8%AF%AD%E5%BA%8F%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E5%B7%A5%E4%BD%9C%EF%BC%9F"><span class="nav-number">9.5.</span> <span class="nav-text">Q5: 词袋模型丢失了语序，为什么还能工作？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q6-jieba-%E7%9A%84%E7%B2%BE%E7%A1%AE%E6%A8%A1%E5%BC%8F%E3%80%81%E5%85%A8%E6%A8%A1%E5%BC%8F%E3%80%81%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%A8%A1%E5%BC%8F%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">9.6.</span> <span class="nav-text">Q6: jieba 的精确模式、全模式、搜索引擎模式有什么区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q7-BERT-%E8%BF%99%E4%B9%88%E5%BC%BA%E5%A4%A7%EF%BC%8C%E8%BF%98%E9%9C%80%E8%A6%81%E5%AD%A6%E4%B9%A0%E4%BC%A0%E7%BB%9F%E9%A2%84%E5%A4%84%E7%90%86%E5%90%97%EF%BC%9F"><span class="nav-number">9.7.</span> <span class="nav-text">Q7: BERT 这么强大，还需要学习传统预处理吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q8-%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E6%97%B6%E8%A6%81%E4%B8%8D%E8%A6%81%E8%BD%AC%E7%B9%81%E4%BD%93%E4%B8%BA%E7%AE%80%E4%BD%93%EF%BC%9F"><span class="nav-number">9.8.</span> <span class="nav-text">Q8: 中文预处理时要不要转繁体为简体？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q9-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%E5%88%86%E8%AF%8D%E8%B4%A8%E9%87%8F%EF%BC%9F"><span class="nav-number">9.9.</span> <span class="nav-text">Q9: 如何评估分词质量？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q10-%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE%E4%B8%AD%E9%A2%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1%EF%BC%9F"><span class="nav-number">9.10.</span> <span class="nav-text">Q10: 实际项目中预处理流程应该怎么设计？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B-v2"><span class="nav-number">10.</span> <span class="nav-text">总结与展望</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
