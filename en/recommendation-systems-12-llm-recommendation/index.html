<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            Recommendation Systems (12): Large Language Models and Recommendation |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">Recommendation Systems (12): Large Language Models and Recommendation</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-10-31 00:00:00</span>
        <span class="mobile">2025-10-31 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/categories/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/Large-Language-Models/">Large Language Models</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>6.7k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>41 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>When you ask ChatGPT "What movies should I watch if I liked The
Matrix?" it doesn't just match keywordsâ€”it understands the philosophical
themes, visual style, and narrative structure that made The Matrix
compelling, then reasons about similar films across genres and decades.
This capability represents a paradigm shift in recommendation systems:
moving from statistical pattern matching to semantic understanding and
reasoning. Large Language Models (LLMs) like GPT-4, Claude, and LLaMA
have revolutionized natural language processing, and their integration
into recommendation systems is transforming how we understand user
preferences, generate explanations, and handle cold-start scenarios.</p>
<p>Traditional recommendation systems excel at finding patterns in
user-item interaction matrices, but they struggle with understanding
rich textual content, explaining recommendations naturally, and adapting
to conversational contexts. LLMs bridge these gaps by bringing world
knowledge, semantic understanding, and natural language generation to
recommendation. From prompt-based zero-shot recommendation that requires
no training data, to LLM-enhanced feature extraction that enriches item
representations, to conversational recommenders that engage users in
natural dialogue, LLMs are reshaping the recommendation landscape.</p>
<p>This article provides a comprehensive exploration of LLM-powered
recommendation systems, covering the fundamental roles LLMs play (as
rankers, feature enhancers, and conversational agents), prompt
engineering techniques for recommendation tasks, state-of-the-art
architectures like A-LLMRec and XRec, conversational recommendation
systems (ChatREC, RA-Rec, ChatCRS), token efficiency optimization
strategies, and practical implementations with 10+ code examples and
detailed Q&amp;A sections addressing common challenges and design
decisions.</p>
<span id="more"></span>
<h2 id="the-role-of-llms-in-recommendation-systems">The Role of LLMs in
Recommendation Systems</h2>
<h3 id="why-llms-for-recommendation">Why LLMs for Recommendation?</h3>
<p>Traditional recommendation systems face several fundamental
limitations that LLMs address:</p>
<p><strong>Semantic Understanding Gap</strong>: Collaborative filtering
and content-based methods rely on explicit features (genres, tags,
ratings) but miss nuanced semantic relationships. LLMs understand that
"The Matrix" and "Blade Runner" share cyberpunk themes even if they're
tagged differently.</p>
<p><strong>Cold-Start Problem</strong>: New items and users lack
interaction history. LLMs can generate recommendations based solely on
item descriptions, user profiles, or natural language queries without
requiring historical data.</p>
<p><strong>Explainability</strong>: Traditional systems struggle to
explain why they recommend an item. LLMs can generate natural language
explanations that reference specific aspects of user preferences and
item characteristics.</p>
<p><strong>Conversational Interaction</strong>: Most recommendation
systems are one-shot: input preferences, get recommendations. LLMs
enable multi-turn conversations where users can refine preferences, ask
questions, and explore recommendations interactively.</p>
<p><strong>Cross-Domain Knowledge</strong>: LLMs bring world knowledge
that traditional systems lack. They understand that users who like "The
Godfather" might appreciate "Goodfellas" because both are mafia films,
even without explicit genre tags.</p>
<h3 id="three-primary-roles-of-llms">Three Primary Roles of LLMs</h3>
<p>LLMs serve three main roles in recommendation systems:</p>
<p><strong>1. LLM as Ranker</strong>: The LLM directly generates or
ranks recommendations based on user preferences and item descriptions.
This is the most direct application, often using few-shot prompting or
fine-tuning.</p>
<p><strong>2. LLM as Feature Enhancer</strong>: The LLM enriches item
and user representations by extracting semantic features from text
descriptions, generating embeddings, or creating structured metadata
that traditional models can use.</p>
<p><strong>3. LLM as Conversational Agent</strong>: The LLM engages
users in natural language dialogue to understand preferences, provide
recommendations, explain choices, and handle follow-up questions.</p>
<p>explore each role in detail.</p>
<h2 id="prompt-based-recommendation">Prompt-Based Recommendation</h2>
<p>Prompt-based recommendation leverages LLMs' in-context learning
capabilities to generate recommendations without fine-tuning. By
carefully crafting prompts that include user preferences, item
descriptions, and examples, we can guide LLMs to produce relevant
recommendations.</p>
<h3 id="basic-prompt-structure">Basic Prompt Structure</h3>
<p>A typical prompt for recommendation consists of:</p>
<ol type="1">
<li><strong>Task Description</strong>: What the LLM should do</li>
<li><strong>User Profile</strong>: User preferences, history, or
query</li>
<li><strong>Item Catalog</strong>: Available items with
descriptions</li>
<li><strong>Examples</strong> (few-shot): Example input-output
pairs</li>
<li><strong>Output Format</strong>: How recommendations should be
structured</li>
</ol>
<h3 id="zero-shot-recommendation">Zero-Shot Recommendation</h3>
<p>Zero-shot recommendation uses no training examples, relying entirely
on the LLM's pre-trained knowledge:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">zero_shot_recommendation</span>(<span class="params">user_query, items, llm_client</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generate recommendations using zero-shot prompting.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        user_query: Natural language query (e.g., "I want action movies")</span></span><br><span class="line"><span class="string">        items: List of items with descriptions</span></span><br><span class="line"><span class="string">        llm_client: LLM API client (OpenAI, Anthropic, etc.)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    item_descriptions = <span class="string">"\n"</span>.join([</span><br><span class="line">        <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item[<span class="string">'description'</span>]}</span>"</span></span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(items)</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    prompt = <span class="string">f"""You are a movie recommendation expert. Based on the user's request, recommend the top 5 most relevant movies from the catalog below.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Request: <span class="subst">{user_query}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Available Movies:</span></span><br><span class="line"><span class="string"><span class="subst">{item_descriptions}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Please provide your recommendations in the following format:</span></span><br><span class="line"><span class="string">1. Movie Title - Brief reason</span></span><br><span class="line"><span class="string">2. Movie Title - Brief reason</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Recommendations:"""</span></span><br><span class="line">    </span><br><span class="line">    response = llm_client.generate(prompt)</span><br><span class="line">    <span class="keyword">return</span> parse_recommendations(response)</span><br></pre></td></tr></table></figure>
<h3 id="few-shot-recommendation">Few-Shot Recommendation</h3>
<p>Few-shot recommendation includes examples to guide the LLM's
behavior:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">few_shot_recommendation</span>(<span class="params">user_query, items, llm_client</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generate recommendations using few-shot prompting with examples.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    examples = <span class="string">"""</span></span><br><span class="line"><span class="string">Example 1:</span></span><br><span class="line"><span class="string">User Request: I like psychological thrillers with plot twists</span></span><br><span class="line"><span class="string">Recommendations:</span></span><br><span class="line"><span class="string">1. Shutter Island - Complex psychological mystery with unexpected revelations</span></span><br><span class="line"><span class="string">2. The Prestige - Mind-bending narrative with multiple twists</span></span><br><span class="line"><span class="string">3. Memento - Non-linear storytelling that keeps you guessing</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Example 2:</span></span><br><span class="line"><span class="string">User Request: I want romantic comedies set in New York</span></span><br><span class="line"><span class="string">Recommendations:</span></span><br><span class="line"><span class="string">1. When Harry Met Sally - Classic NYC rom-com with witty dialogue</span></span><br><span class="line"><span class="string">2. You've Got Mail - Modern NYC romance with bookstore setting</span></span><br><span class="line"><span class="string">3. Serendipity - Magical NYC love story</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    </span><br><span class="line">    item_descriptions = <span class="string">"\n"</span>.join([</span><br><span class="line">        <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item[<span class="string">'description'</span>]}</span>"</span></span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(items)</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    prompt = <span class="string">f"""<span class="subst">{examples}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Now, based on the following user request, recommend the top 5 movies:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Request: <span class="subst">{user_query}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Available Movies:</span></span><br><span class="line"><span class="string"><span class="subst">{item_descriptions}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Recommendations:"""</span></span><br><span class="line">    </span><br><span class="line">    response = llm_client.generate(prompt)</span><br><span class="line">    <span class="keyword">return</span> parse_recommendations(response)</span><br></pre></td></tr></table></figure>
<h3 id="chain-of-thought-recommendation">Chain-of-Thought
Recommendation</h3>
<p>Chain-of-thought prompting helps LLMs reason through the
recommendation process:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">chain_of_thought_recommendation</span>(<span class="params">user_history, items, llm_client</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Use chain-of-thought reasoning for recommendations.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    history_str = <span class="string">"\n"</span>.join([</span><br><span class="line">        <span class="string">f"- <span class="subst">{item[<span class="string">'title'</span>]}</span> (<span class="subst">{item[<span class="string">'rating'</span>]}</span>/5): <span class="subst">{item[<span class="string">'review'</span>]}</span>"</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> user_history</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    item_descriptions = <span class="string">"\n"</span>.join([</span><br><span class="line">        <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item[<span class="string">'description'</span>]}</span>"</span></span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(items)</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    prompt = <span class="string">f"""Analyze the user's viewing history and recommend movies they would enjoy.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User's Viewing History:</span></span><br><span class="line"><span class="string"><span class="subst">{history_str}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Available Movies:</span></span><br><span class="line"><span class="string"><span class="subst">{item_descriptions}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Think step by step:</span></span><br><span class="line"><span class="string">1. What patterns do you notice in the user's preferences?</span></span><br><span class="line"><span class="string">2. What genres, themes, or styles do they prefer?</span></span><br><span class="line"><span class="string">3. Which movies from the catalog match these preferences?</span></span><br><span class="line"><span class="string">4. Rank them by relevance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Analysis:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    </span><br><span class="line">    response = llm_client.generate(prompt)</span><br><span class="line">    <span class="comment"># Continue with recommendation request</span></span><br><span class="line">    follow_up = <span class="string">f"""<span class="subst">{response}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Based on this analysis, provide your top 5 recommendations:"""</span></span><br><span class="line">    </span><br><span class="line">    recommendations = llm_client.generate(follow_up)</span><br><span class="line">    <span class="keyword">return</span> parse_recommendations(recommendations)</span><br></pre></td></tr></table></figure>
<h3 id="prompt-template-design">Prompt Template Design</h3>
<p>Effective prompt templates balance specificity with flexibility:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RecommendationPromptTemplate</span>:</span><br><span class="line">    <span class="string">"""Template for recommendation prompts."""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, task_type=<span class="string">"zero_shot"</span></span>):</span><br><span class="line">        self.task_type = task_type</span><br><span class="line">        self.templates = {</span><br><span class="line">            <span class="string">"zero_shot"</span>: self._zero_shot_template,</span><br><span class="line">            <span class="string">"few_shot"</span>: self._few_shot_template,</span><br><span class="line">            <span class="string">"conversational"</span>: self._conversational_template</span><br><span class="line">        }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_zero_shot_template</span>(<span class="params">self, user_context, items</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"""Task: Recommend items based on user preferences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Context:</span></span><br><span class="line"><span class="string"><span class="subst">{user_context}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Available Items:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_items(items)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Instructions:</span></span><br><span class="line"><span class="string">- Analyze the user's preferences</span></span><br><span class="line"><span class="string">- Select the top 5 most relevant items</span></span><br><span class="line"><span class="string">- Provide brief explanations for each recommendation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Recommendations:"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_few_shot_template</span>(<span class="params">self, user_context, items, examples</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"""Task: Recommend items based on user preferences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Examples:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_examples(examples)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Context:</span></span><br><span class="line"><span class="string"><span class="subst">{user_context}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Available Items:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_items(items)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Recommendations:"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_conversational_template</span>(<span class="params">self, conversation_history, items</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"""You are a helpful recommendation assistant. Based on the conversation, recommend items.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Conversation History:</span></span><br><span class="line"><span class="string"><span class="subst">{conversation_history}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Available Items:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_items(items)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Your response:"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_items</span>(<span class="params">self, items</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item.get(<span class="string">'title'</span>, item.get(<span class="string">'name'</span>))}</span>: <span class="subst">{item.get(<span class="string">'description'</span>, <span class="string">''</span>)}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(items)</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_examples</span>(<span class="params">self, examples</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n\n"</span>.join([</span><br><span class="line">            <span class="string">f"Example <span class="subst">{i+<span class="number">1</span>}</span>:\n<span class="subst">{ex[<span class="string">'input'</span>]}</span>\nRecommendations: <span class="subst">{ex[<span class="string">'output'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, ex <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples)</span><br><span class="line">        ])</span><br></pre></td></tr></table></figure>
<h2 id="a-llmrec-augmented-llm-for-recommendation">A-LLMRec: Augmented
LLM for Recommendation</h2>
<p>A-LLMRec (Augmented LLM for Recommendation) enhances LLMs with
external knowledge and structured data to improve recommendation
accuracy. It addresses LLMs' limitations in handling numerical features,
temporal patterns, and domain-specific knowledge.</p>
<h3 id="architecture-overview">Architecture Overview</h3>
<p>A-LLMRec combines: 1. <strong>LLM backbone</strong> for semantic
understanding 2. <strong>External knowledge bases</strong> for
domain-specific information 3. <strong>Structured feature
extractors</strong> for numerical/categorical data 4. <strong>Hybrid
ranking</strong> that combines LLM scores with traditional signals</p>
<h3 id="implementation">Implementation</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ALLMRec</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Augmented LLM for Recommendation.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Combines LLM semantic understanding with structured features.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_model_name, feature_dim, hidden_dim=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ALLMRec, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># LLM backbone</span></span><br><span class="line">        self.llm = AutoModel.from_pretrained(llm_model_name)</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Feature extractors</span></span><br><span class="line">        self.user_feature_encoder = nn.Linear(feature_dim, hidden_dim)</span><br><span class="line">        self.item_feature_encoder = nn.Linear(feature_dim, hidden_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Knowledge graph encoder (if available)</span></span><br><span class="line">        self.kg_encoder = nn.Linear(feature_dim, hidden_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fusion layers</span></span><br><span class="line">        self.fusion_layer = nn.Sequential(</span><br><span class="line">            nn.Linear(hidden_dim * <span class="number">3</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line">            nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Ranking head</span></span><br><span class="line">        self.ranking_head = nn.Sequential(</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim // <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(hidden_dim // <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_text</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">"""Encode text using LLM."""</span></span><br><span class="line">        inputs = self.tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">            padding=<span class="literal">True</span>,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">            max_length=<span class="number">512</span></span><br><span class="line">        )</span><br><span class="line">        outputs = self.llm(**inputs)</span><br><span class="line">        <span class="comment"># Use [CLS] token or mean pooling</span></span><br><span class="line">        <span class="keyword">return</span> outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, user_text, item_text, user_features, item_features, kg_features=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Forward pass.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            user_text: User description/preferences as text</span></span><br><span class="line"><span class="string">            item_text: Item description as text</span></span><br><span class="line"><span class="string">            user_features: Structured user features (e.g., age, location)</span></span><br><span class="line"><span class="string">            item_features: Structured item features (e.g., price, category)</span></span><br><span class="line"><span class="string">            kg_features: Knowledge graph features (optional)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># LLM encoding</span></span><br><span class="line">        user_llm_emb = self.encode_text(user_text)</span><br><span class="line">        item_llm_emb = self.encode_text(item_text)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Structured feature encoding</span></span><br><span class="line">        user_feat_emb = self.user_feature_encoder(user_features)</span><br><span class="line">        item_feat_emb = self.item_feature_encoder(item_features)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Knowledge graph encoding (if available)</span></span><br><span class="line">        <span class="keyword">if</span> kg_features <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kg_emb = self.kg_encoder(kg_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            kg_emb = torch.zeros_like(user_feat_emb)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Combine LLM embeddings</span></span><br><span class="line">        llm_combined = torch.cat([user_llm_emb, item_llm_emb], dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fuse all representations</span></span><br><span class="line">        combined = torch.cat([</span><br><span class="line">            llm_combined,</span><br><span class="line">            user_feat_emb + item_feat_emb,</span><br><span class="line">            kg_emb</span><br><span class="line">        ], dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        fused = self.fusion_layer(combined)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Ranking score</span></span><br><span class="line">        score = self.ranking_head(fused)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>
<h3 id="training-a-llmrec">Training A-LLMRec</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_allmrec</span>(<span class="params">model, train_loader, optimizer, device</span>):</span><br><span class="line">    <span class="string">"""Training loop for A-LLMRec."""</span></span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        user_text = batch[<span class="string">'user_text'</span>]</span><br><span class="line">        item_text = batch[<span class="string">'item_text'</span>]</span><br><span class="line">        user_features = batch[<span class="string">'user_features'</span>].to(device)</span><br><span class="line">        item_features = batch[<span class="string">'item_features'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'labels'</span>].to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        scores = model(</span><br><span class="line">            user_text,</span><br><span class="line">            item_text,</span><br><span class="line">            user_features,</span><br><span class="line">            item_features,</span><br><span class="line">            batch.get(<span class="string">'kg_features'</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute loss</span></span><br><span class="line">        loss = criterion(scores.squeeze(), labels.<span class="built_in">float</span>())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward pass</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(train_loader)</span><br></pre></td></tr></table></figure>
<h2 id="xrec-explainable-llm-based-recommendation">XRec: Explainable
LLM-Based Recommendation</h2>
<p>XRec focuses on generating natural language explanations for
recommendations, addressing the explainability gap in traditional
systems. It uses LLMs to create personalized explanations that reference
specific user preferences and item characteristics.</p>
<h3 id="architecture">Architecture</h3>
<p>XRec consists of: 1. <strong>Recommendation Module</strong>:
Generates candidate recommendations 2. <strong>Explanation
Generator</strong>: LLM-based module that creates explanations 3.
<strong>Explanation Ranker</strong>: Ranks explanations by quality and
relevance</p>
<h3 id="implementation-1">Implementation</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">XRecExplainer</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Explainable recommendation system using LLMs.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, recommendation_model</span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.recommendation_model = recommendation_model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">recommend_with_explanation</span>(<span class="params">self, user_id, user_profile, top_k=<span class="number">5</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Generate recommendations with explanations.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            user_id: User identifier</span></span><br><span class="line"><span class="string">            user_profile: User profile (preferences, history, etc.)</span></span><br><span class="line"><span class="string">            top_k: Number of recommendations</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Get recommendations</span></span><br><span class="line">        recommendations = self.recommendation_model.recommend(</span><br><span class="line">            user_id, top_k=top_k</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate explanations</span></span><br><span class="line">        explanations = []</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> recommendations:</span><br><span class="line">            explanation = self._generate_explanation(</span><br><span class="line">                user_profile,</span><br><span class="line">                item</span><br><span class="line">            )</span><br><span class="line">            explanations.append({</span><br><span class="line">                <span class="string">'item'</span>: item,</span><br><span class="line">                <span class="string">'explanation'</span>: explanation,</span><br><span class="line">                <span class="string">'score'</span>: item[<span class="string">'score'</span>]</span><br><span class="line">            })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> explanations</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_explanation</span>(<span class="params">self, user_profile, item</span>):</span><br><span class="line">        <span class="string">"""Generate explanation for a single recommendation."""</span></span><br><span class="line">        prompt = <span class="string">f"""You are a recommendation system that explains why items are recommended to users.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Profile:</span></span><br><span class="line"><span class="string">- Preferences: <span class="subst">{user_profile.get(<span class="string">'preferences'</span>, [])}</span></span></span><br><span class="line"><span class="string">- Past Interactions: <span class="subst">{user_profile.get(<span class="string">'history'</span>, [])}</span></span></span><br><span class="line"><span class="string">- Demographics: <span class="subst">{user_profile.get(<span class="string">'demographics'</span>, {}</span>)}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Recommended Item:</span></span><br><span class="line"><span class="string">- Title: <span class="subst">{item[<span class="string">'title'</span>]}</span></span></span><br><span class="line"><span class="string">- Description: <span class="subst">{item[<span class="string">'description'</span>]}</span></span></span><br><span class="line"><span class="string">- Features: <span class="subst">{item.get(<span class="string">'features'</span>, [])}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate a natural, personalized explanation (2-3 sentences) explaining why this item is recommended. Reference specific aspects of the user's preferences and the item's characteristics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Explanation:"""</span></span><br><span class="line">        </span><br><span class="line">        explanation = self.llm_client.generate(prompt)</span><br><span class="line">        <span class="keyword">return</span> explanation.strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_comparative_explanation</span>(<span class="params">self, user_profile, items</span>):</span><br><span class="line">        <span class="string">"""Generate explanation comparing multiple items."""</span></span><br><span class="line">        items_str = <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item[<span class="string">'description'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(items)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        prompt = <span class="string">f"""Compare these items and explain which one best matches the user's preferences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Profile:</span></span><br><span class="line"><span class="string"><span class="subst">{user_profile}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Items:</span></span><br><span class="line"><span class="string"><span class="subst">{items_str}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Provide:</span></span><br><span class="line"><span class="string">1. A comparison of the items</span></span><br><span class="line"><span class="string">2. Which item best matches the user and why</span></span><br><span class="line"><span class="string">3. When the other items might be preferred</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Analysis:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt)</span><br></pre></td></tr></table></figure>
<h3 id="multi-aspect-explanation">Multi-Aspect Explanation</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_multi_aspect_explanation</span>(<span class="params">user_profile, item, llm_client</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generate explanation covering multiple aspects.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    prompt = <span class="string">f"""Explain why this item is recommended, covering:</span></span><br><span class="line"><span class="string">1. Content similarity (how it matches user preferences)</span></span><br><span class="line"><span class="string">2. Popularity signals (why others like it)</span></span><br><span class="line"><span class="string">3. Diversity (how it adds variety to recommendations)</span></span><br><span class="line"><span class="string">4. Temporal relevance (why it's relevant now)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Profile: <span class="subst">{user_profile}</span></span></span><br><span class="line"><span class="string">Item: <span class="subst">{item}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Explanation:"""</span></span><br><span class="line">    </span><br><span class="line">    explanation = llm_client.generate(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parse into aspects</span></span><br><span class="line">    aspects = {</span><br><span class="line">        <span class="string">'content'</span>: extract_aspect(explanation, <span class="string">'content'</span>),</span><br><span class="line">        <span class="string">'popularity'</span>: extract_aspect(explanation, <span class="string">'popularity'</span>),</span><br><span class="line">        <span class="string">'diversity'</span>: extract_aspect(explanation, <span class="string">'diversity'</span>),</span><br><span class="line">        <span class="string">'temporal'</span>: extract_aspect(explanation, <span class="string">'temporal'</span>)</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> aspects</span><br></pre></td></tr></table></figure>
<h2 id="llm-as-feature-enhancer">LLM as Feature Enhancer</h2>
<p>LLMs excel at extracting semantic features from unstructured text,
enriching item and user representations that traditional recommendation
models can use.</p>
<h3 id="text-feature-extraction">Text Feature Extraction</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLMFeatureExtractor</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Extract semantic features from text using LLMs.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_model_name, feature_dim=<span class="number">768</span></span>):</span><br><span class="line">        self.model = AutoModel.from_pretrained(llm_model_name)</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)</span><br><span class="line">        self.feature_dim = feature_dim</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extract_item_features</span>(<span class="params">self, item_description, item_metadata=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Extract features from item description.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            item_description: Text description of the item</span></span><br><span class="line"><span class="string">            item_metadata: Additional metadata (optional)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Combine description and metadata</span></span><br><span class="line">        <span class="keyword">if</span> item_metadata:</span><br><span class="line">            text = <span class="string">f"<span class="subst">{item_description}</span>\nMetadata: <span class="subst">{item_metadata}</span>"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            text = item_description</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Tokenize and encode</span></span><br><span class="line">        inputs = self.tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">            padding=<span class="literal">True</span>,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">            max_length=<span class="number">512</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model(**inputs)</span><br><span class="line">            <span class="comment"># Mean pooling over sequence</span></span><br><span class="line">            features = outputs.last_hidden_state.mean(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> features.squeeze().numpy()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extract_user_features</span>(<span class="params">self, user_profile_text, interaction_history=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Extract features from user profile and history.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Combine profile and history</span></span><br><span class="line">        <span class="keyword">if</span> interaction_history:</span><br><span class="line">            history_text = <span class="string">"\n"</span>.join([</span><br><span class="line">                <span class="string">f"Interacted with: <span class="subst">{item[<span class="string">'title'</span>]}</span> (<span class="subst">{item.get(<span class="string">'rating'</span>, <span class="string">'N/A'</span>)}</span>)"</span></span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> interaction_history[:<span class="number">10</span>]  <span class="comment"># Last 10 interactions</span></span><br><span class="line">            ])</span><br><span class="line">            text = <span class="string">f"<span class="subst">{user_profile_text}</span>\n\nInteraction History:\n<span class="subst">{history_text}</span>"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            text = user_profile_text</span><br><span class="line">        </span><br><span class="line">        inputs = self.tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">            padding=<span class="literal">True</span>,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">            max_length=<span class="number">512</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model(**inputs)</span><br><span class="line">            features = outputs.last_hidden_state.mean(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> features.squeeze().numpy()</span><br></pre></td></tr></table></figure>
<h3 id="structured-feature-generation">Structured Feature
Generation</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_structured_features</span>(<span class="params">item_text, llm_client</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Use LLM to generate structured features from unstructured text.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    prompt = <span class="string">f"""Extract structured features from this item description.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Item Description:</span></span><br><span class="line"><span class="string"><span class="subst">{item_text}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Extract the following information in JSON format:</span></span><br><span class="line"><span class="string">{{</span></span><br><span class="line"><span class="string">    "genre": ["genre1", "genre2"],</span></span><br><span class="line"><span class="string">    "themes": ["theme1", "theme2"],</span></span><br><span class="line"><span class="string">    "target_audience": "audience description",</span></span><br><span class="line"><span class="string">    "mood": ["mood1", "mood2"],</span></span><br><span class="line"><span class="string">    "keywords": ["keyword1", "keyword2", "keyword3"]</span></span><br><span class="line"><span class="string">}}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">JSON:"""</span></span><br><span class="line">    </span><br><span class="line">    response = llm_client.generate(prompt)</span><br><span class="line">    <span class="comment"># Parse JSON response</span></span><br><span class="line">    features = json.loads(extract_json(response))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>
<h3 id="hybrid-feature-integration">Hybrid Feature Integration</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HybridFeatureModel</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model that combines LLM-extracted features with traditional features.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_feature_dim, traditional_feature_dim, hidden_dim=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(HybridFeatureModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.llm_feature_proj = nn.Linear(llm_feature_dim, hidden_dim)</span><br><span class="line">        self.traditional_feature_proj = nn.Linear(traditional_feature_dim, hidden_dim)</span><br><span class="line">        </span><br><span class="line">        self.fusion = nn.Sequential(</span><br><span class="line">            nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim // <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.output = nn.Linear(hidden_dim // <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, llm_features, traditional_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            llm_features: Features extracted by LLM</span></span><br><span class="line"><span class="string">            traditional_features: Traditional features (e.g., one-hot, embeddings)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        llm_proj = self.llm_feature_proj(llm_features)</span><br><span class="line">        trad_proj = self.traditional_feature_proj(traditional_features)</span><br><span class="line">        </span><br><span class="line">        combined = torch.cat([llm_proj, trad_proj], dim=-<span class="number">1</span>)</span><br><span class="line">        fused = self.fusion(combined)</span><br><span class="line">        </span><br><span class="line">        score = self.output(fused)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>
<h2 id="llm-as-reranker">LLM as Reranker</h2>
<p>LLMs can serve as powerful rerankers, taking candidate items from a
first-stage retrieval system and reordering them based on semantic
understanding and user context.</p>
<h3 id="reranking-architecture">Reranking Architecture</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLMReranker</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    LLM-based reranker for recommendation.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, max_candidates=<span class="number">100</span></span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.max_candidates = max_candidates</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rerank</span>(<span class="params">self, user_context, candidates, top_k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Rerank candidate items.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            user_context: User preferences/history</span></span><br><span class="line"><span class="string">            candidates: List of candidate items with initial scores</span></span><br><span class="line"><span class="string">            top_k: Number of items to return</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Limit candidates for efficiency</span></span><br><span class="line">        candidates = candidates[:self.max_candidates]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Format candidates</span></span><br><span class="line">        candidates_str = self._format_candidates(candidates)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate reranking prompt</span></span><br><span class="line">        prompt = <span class="string">f"""You are a recommendation reranker. Given a user's context and candidate items, rank them by relevance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Context:</span></span><br><span class="line"><span class="string"><span class="subst">{user_context}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Candidate Items (with initial scores):</span></span><br><span class="line"><span class="string"><span class="subst">{candidates_str}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Rank these items from most relevant to least relevant. Return only the item IDs in order, separated by commas.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ranked IDs:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get reranking from LLM</span></span><br><span class="line">        ranked_ids = self._parse_ranked_ids(</span><br><span class="line">            self.llm_client.generate(prompt)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Map back to candidates</span></span><br><span class="line">        id_to_item = {item[<span class="string">'id'</span>]: item <span class="keyword">for</span> item <span class="keyword">in</span> candidates}</span><br><span class="line">        reranked = [</span><br><span class="line">            id_to_item[<span class="built_in">id</span>] <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ranked_ids <span class="keyword">if</span> <span class="built_in">id</span> <span class="keyword">in</span> id_to_item</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fill remaining slots with original order</span></span><br><span class="line">        remaining = [</span><br><span class="line">            item <span class="keyword">for</span> item <span class="keyword">in</span> candidates</span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">not</span> <span class="keyword">in</span> ranked_ids</span><br><span class="line">        ]</span><br><span class="line">        reranked.extend(remaining)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> reranked[:top_k]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_candidates</span>(<span class="params">self, candidates</span>):</span><br><span class="line">        <span class="string">"""Format candidates for prompt."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. ID: <span class="subst">{item[<span class="string">'id'</span>]}</span>, Title: <span class="subst">{item[<span class="string">'title'</span>]}</span>, "</span></span><br><span class="line">            <span class="string">f"Score: <span class="subst">{item.get(<span class="string">'score'</span>, <span class="number">0</span>):<span class="number">.3</span>f}</span>, "</span></span><br><span class="line">            <span class="string">f"Description: <span class="subst">{item.get(<span class="string">'description'</span>, <span class="string">''</span>)[:<span class="number">100</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(candidates)</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parse_ranked_ids</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">"""Parse ranked IDs from LLM response."""</span></span><br><span class="line">        <span class="comment"># Extract IDs from response</span></span><br><span class="line">        <span class="keyword">import</span> re</span><br><span class="line">        ids = re.findall(<span class="string">r'\d+'</span>, response)</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">int</span>(<span class="built_in">id</span>) <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ids]</span><br></pre></td></tr></table></figure>
<h3 id="pairwise-reranking">Pairwise Reranking</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pairwise_rerank</span>(<span class="params">user_context, candidates, llm_client</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Rerank using pairwise comparisons.</span></span><br><span class="line"><span class="string">    More accurate but slower than listwise reranking.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = <span class="built_in">len</span>(candidates)</span><br><span class="line">    scores = {item[<span class="string">'id'</span>]: <span class="number">0</span> <span class="keyword">for</span> item <span class="keyword">in</span> candidates}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compare all pairs</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">            item1 = candidates[i]</span><br><span class="line">            item2 = candidates[j]</span><br><span class="line">            </span><br><span class="line">            prompt = <span class="string">f"""Given the user context, which item is more relevant?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Context:</span></span><br><span class="line"><span class="string"><span class="subst">{user_context}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Item 1:</span></span><br><span class="line"><span class="string">- ID: <span class="subst">{item1[<span class="string">'id'</span>]}</span></span></span><br><span class="line"><span class="string">- Title: <span class="subst">{item1[<span class="string">'title'</span>]}</span></span></span><br><span class="line"><span class="string">- Description: <span class="subst">{item1.get(<span class="string">'description'</span>, <span class="string">''</span>)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Item 2:</span></span><br><span class="line"><span class="string">- ID: <span class="subst">{item2[<span class="string">'id'</span>]}</span></span></span><br><span class="line"><span class="string">- Title: <span class="subst">{item2[<span class="string">'title'</span>]}</span></span></span><br><span class="line"><span class="string">- Description: <span class="subst">{item2.get(<span class="string">'description'</span>, <span class="string">''</span>)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Respond with only "1" or "2":"""</span></span><br><span class="line">            </span><br><span class="line">            response = llm_client.generate(prompt).strip()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> response == <span class="string">"1"</span>:</span><br><span class="line">                scores[item1[<span class="string">'id'</span>]] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> response == <span class="string">"2"</span>:</span><br><span class="line">                scores[item2[<span class="string">'id'</span>]] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sort by scores</span></span><br><span class="line">    reranked = <span class="built_in">sorted</span>(</span><br><span class="line">        candidates,</span><br><span class="line">        key=<span class="keyword">lambda</span> x: scores[x[<span class="string">'id'</span>]],</span><br><span class="line">        reverse=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> reranked</span><br></pre></td></tr></table></figure>
<h3 id="efficient-batch-reranking">Efficient Batch Reranking</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_rerank</span>(<span class="params">user_context, candidate_batches, llm_client, batch_size=<span class="number">20</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Rerank candidates in batches for efficiency.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    all_reranked = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> candidate_batches:</span><br><span class="line">        batch_str = <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item.get(<span class="string">'description'</span>, <span class="string">''</span>)[:<span class="number">50</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        prompt = <span class="string">f"""Rank these items by relevance to the user.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Context:</span></span><br><span class="line"><span class="string"><span class="subst">{user_context}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Items:</span></span><br><span class="line"><span class="string"><span class="subst">{batch_str}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Return ranked item numbers (1-N) separated by commas:"""</span></span><br><span class="line">        </span><br><span class="line">        ranked_indices = parse_ranked_indices(</span><br><span class="line">            llm_client.generate(prompt)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        reranked_batch = [batch[i-<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> ranked_indices <span class="keyword">if</span> <span class="number">1</span> &lt;= i &lt;= <span class="built_in">len</span>(batch)]</span><br><span class="line">        all_reranked.extend(reranked_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> all_reranked</span><br></pre></td></tr></table></figure>
<h2 id="conversational-recommendation-chatrec">Conversational
Recommendation: ChatREC</h2>
<p>ChatREC enables natural language conversations for recommendation,
allowing users to refine preferences, ask questions, and explore
recommendations interactively.</p>
<h3 id="architecture-1">Architecture</h3>
<p>ChatREC combines: 1. <strong>Conversation Manager</strong>: Maintains
dialogue state 2. <strong>Preference Extractor</strong>: Extracts
preferences from conversation 3. <strong>Recommendation Engine</strong>:
Generates recommendations 4. <strong>Response Generator</strong>:
Creates natural language responses</p>
<h3 id="implementation-2">Implementation</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChatREC</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Conversational recommendation system.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, recommendation_model</span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.recommendation_model = recommendation_model</span><br><span class="line">        self.conversation_history = []</span><br><span class="line">        self.user_preferences = {}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, user_message, session_id=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Process user message and generate response.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            user_message: User's message</span></span><br><span class="line"><span class="string">            session_id: Session identifier for multi-turn conversations</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Update conversation history</span></span><br><span class="line">        self.conversation_history.append({</span><br><span class="line">            <span class="string">'role'</span>: <span class="string">'user'</span>,</span><br><span class="line">            <span class="string">'content'</span>: user_message</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Extract preferences</span></span><br><span class="line">        self._update_preferences(user_message)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Determine intent</span></span><br><span class="line">        intent = self._classify_intent(user_message)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate response based on intent</span></span><br><span class="line">        <span class="keyword">if</span> intent == <span class="string">'request_recommendation'</span>:</span><br><span class="line">            response = self._handle_recommendation_request()</span><br><span class="line">        <span class="keyword">elif</span> intent == <span class="string">'clarify_preference'</span>:</span><br><span class="line">            response = self._handle_clarification()</span><br><span class="line">        <span class="keyword">elif</span> intent == <span class="string">'ask_question'</span>:</span><br><span class="line">            response = self._handle_question()</span><br><span class="line">        <span class="keyword">elif</span> intent == <span class="string">'refine_preference'</span>:</span><br><span class="line">            response = self._handle_refinement()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            response = self._handle_general()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update conversation history</span></span><br><span class="line">        self.conversation_history.append({</span><br><span class="line">            <span class="string">'role'</span>: <span class="string">'assistant'</span>,</span><br><span class="line">            <span class="string">'content'</span>: response</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_classify_intent</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Classify user intent."""</span></span><br><span class="line">        prompt = <span class="string">f"""Classify the user's intent in this message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Intent categories:</span></span><br><span class="line"><span class="string">1. request_recommendation - User wants recommendations</span></span><br><span class="line"><span class="string">2. clarify_preference - User is clarifying preferences</span></span><br><span class="line"><span class="string">3. ask_question - User is asking about an item</span></span><br><span class="line"><span class="string">4. refine_preference - User wants to refine previous preferences</span></span><br><span class="line"><span class="string">5. general - General conversation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Respond with only the intent category:"""</span></span><br><span class="line">        </span><br><span class="line">        intent = self.llm_client.generate(prompt).strip().lower()</span><br><span class="line">        <span class="keyword">return</span> intent</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_update_preferences</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Extract and update user preferences from message."""</span></span><br><span class="line">        prompt = <span class="string">f"""Extract user preferences from this message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Current Preferences: <span class="subst">{self.user_preferences}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Extract any new preferences or updates. Return JSON format:</span></span><br><span class="line"><span class="string">{{</span></span><br><span class="line"><span class="string">    "genres": ["genre1", "genre2"],</span></span><br><span class="line"><span class="string">    "themes": ["theme1"],</span></span><br><span class="line"><span class="string">    "constraints": {{"max_price": 100, "year": "2020+"}},</span></span><br><span class="line"><span class="string">    "dislikes": ["item1", "item2"]</span></span><br><span class="line"><span class="string">}}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">JSON:"""</span></span><br><span class="line">        </span><br><span class="line">        response = self.llm_client.generate(prompt)</span><br><span class="line">        new_preferences = json.loads(extract_json(response))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Merge with existing preferences</span></span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> new_preferences.items():</span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> self.user_preferences:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(value, <span class="built_in">list</span>):</span><br><span class="line">                    self.user_preferences[key].extend(value)</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">isinstance</span>(value, <span class="built_in">dict</span>):</span><br><span class="line">                    self.user_preferences[key].update(value)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.user_preferences[key] = value</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.user_preferences[key] = value</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_handle_recommendation_request</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Handle request for recommendations."""</span></span><br><span class="line">        <span class="comment"># Get recommendations</span></span><br><span class="line">        recommendations = self.recommendation_model.recommend(</span><br><span class="line">            preferences=self.user_preferences,</span><br><span class="line">            top_k=<span class="number">5</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate natural language response</span></span><br><span class="line">        items_str = <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item.get(<span class="string">'description'</span>, <span class="string">''</span>)[:<span class="number">100</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(recommendations)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        prompt = <span class="string">f"""Generate a natural, conversational response presenting these recommendations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Preferences: <span class="subst">{self.user_preferences}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Recommendations:</span></span><br><span class="line"><span class="string"><span class="subst">{items_str}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate a friendly response (2-3 sentences) that:</span></span><br><span class="line"><span class="string">1. Acknowledges the user's preferences</span></span><br><span class="line"><span class="string">2. Presents the recommendations naturally</span></span><br><span class="line"><span class="string">3. Invites further conversation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Response:"""</span></span><br><span class="line">        </span><br><span class="line">        response = self.llm_client.generate(prompt)</span><br><span class="line">        <span class="keyword">return</span> response.strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_handle_clarification</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Handle preference clarification."""</span></span><br><span class="line">        prompt = <span class="string">f"""The user is clarifying their preferences. Generate a helpful response.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Conversation History:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_history()}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Message: <span class="subst">{self.conversation_history[-<span class="number">1</span>][<span class="string">'content'</span>]}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate a response that:</span></span><br><span class="line"><span class="string">1. Acknowledges the clarification</span></span><br><span class="line"><span class="string">2. Confirms understanding</span></span><br><span class="line"><span class="string">3. Asks if they'd like recommendations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Response:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_handle_question</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Handle questions about items."""</span></span><br><span class="line">        <span class="comment"># Extract item from question</span></span><br><span class="line">        item = self._extract_item_from_message(</span><br><span class="line">            self.conversation_history[-<span class="number">1</span>][<span class="string">'content'</span>]</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> item:</span><br><span class="line">            prompt = <span class="string">f"""Answer the user's question about this item.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Item: <span class="subst">{item}</span></span></span><br><span class="line"><span class="string">Question: <span class="subst">{self.conversation_history[-<span class="number">1</span>][<span class="string">'content'</span>]}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Provide a helpful, accurate answer:"""</span></span><br><span class="line">            <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"I'd be happy to help! Could you clarify which item you're asking about?"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_handle_refinement</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Handle preference refinement."""</span></span><br><span class="line">        <span class="keyword">return</span> self._handle_clarification()  <span class="comment"># Similar handling</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_handle_general</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Handle general conversation."""</span></span><br><span class="line">        prompt = <span class="string">f"""You are a helpful recommendation assistant. Respond naturally to the user.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Conversation History:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_history()}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Message: <span class="subst">{self.conversation_history[-<span class="number">1</span>][<span class="string">'content'</span>]}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Response:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_history</span>(<span class="params">self, max_turns=<span class="number">5</span></span>):</span><br><span class="line">        <span class="string">"""Format conversation history for prompts."""</span></span><br><span class="line">        recent = self.conversation_history[-max_turns:]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{turn[<span class="string">'role'</span>]}</span>: <span class="subst">{turn[<span class="string">'content'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> turn <span class="keyword">in</span> recent</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_extract_item_from_message</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Extract item mention from message."""</span></span><br><span class="line">        <span class="comment"># Simple extraction - can be enhanced with NER</span></span><br><span class="line">        <span class="comment"># For now, return None and let LLM handle it</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="ra-rec-retrieval-augmented-recommendation">RA-Rec:
Retrieval-Augmented Recommendation</h2>
<p>RA-Rec combines retrieval-augmented generation (RAG) with
recommendation, using external knowledge bases to enhance LLM
recommendations.</p>
<h3 id="architecture-2">Architecture</h3>
<p>RA-Rec consists of: 1. <strong>Retriever</strong>: Retrieves relevant
items/knowledge from external sources 2. <strong>Augmenter</strong>:
Enhances LLM context with retrieved information 3.
<strong>Generator</strong>: LLM that generates recommendations using
augmented context</p>
<h3 id="implementation-3">Implementation</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RARec</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Retrieval-Augmented Recommendation system.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, retriever, knowledge_base</span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.retriever = retriever</span><br><span class="line">        self.knowledge_base = knowledge_base</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">recommend</span>(<span class="params">self, user_query, top_k=<span class="number">5</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Generate recommendations using retrieval augmentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Retrieve relevant knowledge</span></span><br><span class="line">        retrieved_items = self.retriever.retrieve(</span><br><span class="line">            query=user_query,</span><br><span class="line">            top_k=<span class="number">20</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve relevant knowledge graph facts</span></span><br><span class="line">        kg_facts = self.knowledge_base.retrieve_facts(</span><br><span class="line">            entities=[item[<span class="string">'id'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> retrieved_items],</span><br><span class="line">            top_k=<span class="number">10</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Augment context</span></span><br><span class="line">        augmented_context = self._augment_context(</span><br><span class="line">            user_query,</span><br><span class="line">            retrieved_items,</span><br><span class="line">            kg_facts</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate recommendations</span></span><br><span class="line">        prompt = <span class="string">f"""Generate recommendations based on the user query and retrieved information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Query: <span class="subst">{user_query}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Retrieved Items:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_items(retrieved_items)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Knowledge Graph Facts:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_kg_facts(kg_facts)}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate top <span class="subst">{top_k}</span> recommendations with explanations:"""</span></span><br><span class="line">        </span><br><span class="line">        recommendations = self.llm_client.generate(prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self._parse_recommendations(recommendations)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_augment_context</span>(<span class="params">self, query, items, kg_facts</span>):</span><br><span class="line">        <span class="string">"""Augment context with retrieved information."""</span></span><br><span class="line">        context = {</span><br><span class="line">            <span class="string">'query'</span>: query,</span><br><span class="line">            <span class="string">'items'</span>: items,</span><br><span class="line">            <span class="string">'kg_facts'</span>: kg_facts,</span><br><span class="line">            <span class="string">'item_relationships'</span>: self._extract_relationships(items, kg_facts)</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> context</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_extract_relationships</span>(<span class="params">self, items, kg_facts</span>):</span><br><span class="line">        <span class="string">"""Extract relationships between items."""</span></span><br><span class="line">        relationships = []</span><br><span class="line">        <span class="keyword">for</span> fact <span class="keyword">in</span> kg_facts:</span><br><span class="line">            <span class="keyword">if</span> fact[<span class="string">'relation'</span>] <span class="keyword">in</span> [<span class="string">'similar_to'</span>, <span class="string">'related_to'</span>, <span class="string">'sequel_of'</span>]:</span><br><span class="line">                relationships.append(fact)</span><br><span class="line">        <span class="keyword">return</span> relationships</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_items</span>(<span class="params">self, items</span>):</span><br><span class="line">        <span class="string">"""Format items for prompt."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"- <span class="subst">{item[<span class="string">'title'</span>]}</span>: <span class="subst">{item.get(<span class="string">'description'</span>, <span class="string">''</span>)}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> items</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_kg_facts</span>(<span class="params">self, facts</span>):</span><br><span class="line">        <span class="string">"""Format knowledge graph facts."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"- <span class="subst">{fact[<span class="string">'head'</span>]}</span> <span class="subst">{fact[<span class="string">'relation'</span>]}</span> <span class="subst">{fact[<span class="string">'tail'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> fact <span class="keyword">in</span> facts</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parse_recommendations</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">"""Parse recommendations from LLM output."""</span></span><br><span class="line">        <span class="comment"># Simple parsing - can be enhanced</span></span><br><span class="line">        lines = text.strip().split(<span class="string">'\n'</span>)</span><br><span class="line">        recommendations = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">if</span> line.strip() <span class="keyword">and</span> (line[<span class="number">0</span>].isdigit() <span class="keyword">or</span> line.startswith(<span class="string">'-'</span>)):</span><br><span class="line">                recommendations.append(line.strip())</span><br><span class="line">        <span class="keyword">return</span> recommendations</span><br></pre></td></tr></table></figure>
<h2 id="chatcrs-conversational-recommendation-system">ChatCRS:
Conversational Recommendation System</h2>
<p>ChatCRS is a comprehensive conversational recommendation system that
handles multi-turn dialogues, preference elicitation, and recommendation
generation.</p>
<h3 id="multi-turn-dialogue-management">Multi-Turn Dialogue
Management</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCRS</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Comprehensive conversational recommendation system.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, recommendation_engine</span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.recommendation_engine = recommendation_engine</span><br><span class="line">        self.sessions = {}  <span class="comment"># session_id -&gt; session data</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_message</span>(<span class="params">self, session_id, user_message</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Process user message in a conversational context.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Get or create session</span></span><br><span class="line">        <span class="keyword">if</span> session_id <span class="keyword">not</span> <span class="keyword">in</span> self.sessions:</span><br><span class="line">            self.sessions[session_id] = {</span><br><span class="line">                <span class="string">'history'</span>: [],</span><br><span class="line">                <span class="string">'preferences'</span>: {},</span><br><span class="line">                <span class="string">'current_recommendations'</span>: <span class="literal">None</span>,</span><br><span class="line">                <span class="string">'state'</span>: <span class="string">'greeting'</span></span><br><span class="line">            }</span><br><span class="line">        </span><br><span class="line">        session = self.sessions[session_id]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update history</span></span><br><span class="line">        session[<span class="string">'history'</span>].append({</span><br><span class="line">            <span class="string">'role'</span>: <span class="string">'user'</span>,</span><br><span class="line">            <span class="string">'content'</span>: user_message,</span><br><span class="line">            <span class="string">'timestamp'</span>: datetime.now()</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Determine system state</span></span><br><span class="line">        state = self._determine_state(session, user_message)</span><br><span class="line">        session[<span class="string">'state'</span>] = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate response based on state</span></span><br><span class="line">        response = self._generate_response(session, user_message, state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update history</span></span><br><span class="line">        session[<span class="string">'history'</span>].append({</span><br><span class="line">            <span class="string">'role'</span>: <span class="string">'assistant'</span>,</span><br><span class="line">            <span class="string">'content'</span>: response,</span><br><span class="line">            <span class="string">'timestamp'</span>: datetime.now()</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_determine_state</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Determine current conversation state."""</span></span><br><span class="line">        states = {</span><br><span class="line">            <span class="string">'greeting'</span>: <span class="string">'User just started conversation'</span>,</span><br><span class="line">            <span class="string">'preference_elicitation'</span>: <span class="string">'Collecting user preferences'</span>,</span><br><span class="line">            <span class="string">'recommendation_presentation'</span>: <span class="string">'Presenting recommendations'</span>,</span><br><span class="line">            <span class="string">'clarification'</span>: <span class="string">'Clarifying preferences or recommendations'</span>,</span><br><span class="line">            <span class="string">'exploration'</span>: <span class="string">'User exploring items'</span>,</span><br><span class="line">            <span class="string">'feedback'</span>: <span class="string">'Collecting feedback on recommendations'</span></span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        prompt = <span class="string">f"""Determine the conversation state based on history and current message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Conversation History:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_history(session[<span class="string">'history'</span>][-<span class="number">5</span>:])}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Current Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Current State: <span class="subst">{session[<span class="string">'state'</span>]}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Possible States: <span class="subst">{<span class="built_in">list</span>(states.keys())}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Respond with only the state name:"""</span></span><br><span class="line">        </span><br><span class="line">        state = self.llm_client.generate(prompt).strip().lower()</span><br><span class="line">        <span class="keyword">return</span> state <span class="keyword">if</span> state <span class="keyword">in</span> states <span class="keyword">else</span> session[<span class="string">'state'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_response</span>(<span class="params">self, session, message, state</span>):</span><br><span class="line">        <span class="string">"""Generate response based on state."""</span></span><br><span class="line">        <span class="keyword">if</span> state == <span class="string">'greeting'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._greet_user(session)</span><br><span class="line">        <span class="keyword">elif</span> state == <span class="string">'preference_elicitation'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._elicit_preferences(session, message)</span><br><span class="line">        <span class="keyword">elif</span> state == <span class="string">'recommendation_presentation'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._present_recommendations(session, message)</span><br><span class="line">        <span class="keyword">elif</span> state == <span class="string">'clarification'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._clarify(session, message)</span><br><span class="line">        <span class="keyword">elif</span> state == <span class="string">'exploration'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._explore_items(session, message)</span><br><span class="line">        <span class="keyword">elif</span> state == <span class="string">'feedback'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._collect_feedback(session, message)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._default_response(session, message)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_greet_user</span>(<span class="params">self, session</span>):</span><br><span class="line">        <span class="string">"""Greet user and start preference elicitation."""</span></span><br><span class="line">        prompt = <span class="string">"""Generate a friendly greeting for a recommendation assistant. </span></span><br><span class="line"><span class="string">        Introduce yourself and ask what the user is looking for. Keep it brief (2-3 sentences)."""</span></span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_elicit_preferences</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Elicit user preferences."""</span></span><br><span class="line">        <span class="comment"># Extract preferences</span></span><br><span class="line">        extracted = self._extract_preferences(message)</span><br><span class="line">        session[<span class="string">'preferences'</span>].update(extracted)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if we have enough information</span></span><br><span class="line">        <span class="keyword">if</span> self._has_sufficient_preferences(session[<span class="string">'preferences'</span>]):</span><br><span class="line">            <span class="comment"># Generate recommendations</span></span><br><span class="line">            recommendations = self.recommendation_engine.recommend(</span><br><span class="line">                preferences=session[<span class="string">'preferences'</span>],</span><br><span class="line">                top_k=<span class="number">5</span></span><br><span class="line">            )</span><br><span class="line">            session[<span class="string">'current_recommendations'</span>] = recommendations</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Present recommendations</span></span><br><span class="line">            <span class="keyword">return</span> self._present_recommendations(session, message)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Ask for more information</span></span><br><span class="line">            prompt = <span class="string">f"""The user has provided some preferences. Ask for more specific information to provide better recommendations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Current Preferences: <span class="subst">{session[<span class="string">'preferences'</span>]}</span></span></span><br><span class="line"><span class="string">User Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate a natural question (1-2 sentences) asking for more preferences:"""</span></span><br><span class="line">            <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_present_recommendations</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Present recommendations to user."""</span></span><br><span class="line">        recommendations = session.get(<span class="string">'current_recommendations'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> recommendations:</span><br><span class="line">            recommendations = self.recommendation_engine.recommend(</span><br><span class="line">                preferences=session[<span class="string">'preferences'</span>],</span><br><span class="line">                top_k=<span class="number">5</span></span><br><span class="line">            )</span><br><span class="line">            session[<span class="string">'current_recommendations'</span>] = recommendations</span><br><span class="line">        </span><br><span class="line">        items_str = <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{item[<span class="string">'title'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(recommendations)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        prompt = <span class="string">f"""Present these recommendations naturally to the user.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Preferences: <span class="subst">{session[<span class="string">'preferences'</span>]}</span></span></span><br><span class="line"><span class="string">Recommendations:</span></span><br><span class="line"><span class="string"><span class="subst">{items_str}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate a friendly response (3-4 sentences) that:</span></span><br><span class="line"><span class="string">1. References the user's preferences</span></span><br><span class="line"><span class="string">2. Presents the recommendations</span></span><br><span class="line"><span class="string">3. Invites questions or feedback</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Response:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_clarify</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Handle clarification requests."""</span></span><br><span class="line">        prompt = <span class="string">f"""The user is asking for clarification. Provide a helpful response.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Conversation History:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_history(session[<span class="string">'history'</span>][-<span class="number">3</span>:])}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Current Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Response:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_explore_items</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Handle item exploration."""</span></span><br><span class="line">        <span class="comment"># Extract item mention</span></span><br><span class="line">        item = self._extract_item_mention(message)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> item:</span><br><span class="line">            prompt = <span class="string">f"""Provide detailed information about this item.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Item: <span class="subst">{item}</span></span></span><br><span class="line"><span class="string">User Question: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Provide helpful information:"""</span></span><br><span class="line">            <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Which item would you like to know more about?"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_collect_feedback</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Collect feedback on recommendations."""</span></span><br><span class="line">        <span class="comment"># Extract feedback</span></span><br><span class="line">        feedback = self._extract_feedback(message)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update preferences based on feedback</span></span><br><span class="line">        <span class="keyword">if</span> feedback.get(<span class="string">'liked'</span>):</span><br><span class="line">            session[<span class="string">'preferences'</span>][<span class="string">'liked_items'</span>] = session[<span class="string">'preferences'</span>].get(<span class="string">'liked_items'</span>, [])</span><br><span class="line">            session[<span class="string">'preferences'</span>][<span class="string">'liked_items'</span>].extend(feedback[<span class="string">'liked'</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> feedback.get(<span class="string">'disliked'</span>):</span><br><span class="line">            session[<span class="string">'preferences'</span>][<span class="string">'disliked_items'</span>] = session[<span class="string">'preferences'</span>].get(<span class="string">'disliked_items'</span>, [])</span><br><span class="line">            session[<span class="string">'preferences'</span>][<span class="string">'disliked_items'</span>].extend(feedback[<span class="string">'disliked'</span>])</span><br><span class="line">        </span><br><span class="line">        prompt = <span class="string">f"""Acknowledge the user's feedback and offer to refine recommendations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Feedback: <span class="subst">{feedback}</span></span></span><br><span class="line"><span class="string">Current Recommendations: <span class="subst">{session.get(<span class="string">'current_recommendations'</span>, [])}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate a response (2-3 sentences):"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_default_response</span>(<span class="params">self, session, message</span>):</span><br><span class="line">        <span class="string">"""Default response handler."""</span></span><br><span class="line">        prompt = <span class="string">f"""Respond naturally to the user's message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Conversation History:</span></span><br><span class="line"><span class="string"><span class="subst">{self._format_history(session[<span class="string">'history'</span>][-<span class="number">3</span>:])}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Response:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_extract_preferences</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Extract preferences from message."""</span></span><br><span class="line">        prompt = <span class="string">f"""Extract user preferences from this message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Return JSON:</span></span><br><span class="line"><span class="string">{{</span></span><br><span class="line"><span class="string">    "genres": [],</span></span><br><span class="line"><span class="string">    "themes": [],</span></span><br><span class="line"><span class="string">    "constraints": {{}},</span></span><br><span class="line"><span class="string">    "explicit_preferences": []</span></span><br><span class="line"><span class="string">}}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">JSON:"""</span></span><br><span class="line">        </span><br><span class="line">        response = self.llm_client.generate(prompt)</span><br><span class="line">        <span class="keyword">return</span> json.loads(extract_json(response))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_has_sufficient_preferences</span>(<span class="params">self, preferences</span>):</span><br><span class="line">        <span class="string">"""Check if we have enough preferences."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(preferences.get(<span class="string">'genres'</span>, [])) &gt; <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(preferences.get(<span class="string">'themes'</span>, [])) &gt; <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_extract_item_mention</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Extract item mention from message."""</span></span><br><span class="line">        <span class="comment"># Simple implementation - can be enhanced with NER</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_extract_feedback</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Extract feedback from message."""</span></span><br><span class="line">        prompt = <span class="string">f"""Extract feedback from this message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Message: <span class="subst">{message}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Return JSON:</span></span><br><span class="line"><span class="string">{{</span></span><br><span class="line"><span class="string">    "liked": ["item1", "item2"],</span></span><br><span class="line"><span class="string">    "disliked": ["item3"],</span></span><br><span class="line"><span class="string">    "rating": {{"item1": 5}}</span></span><br><span class="line"><span class="string">}}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">JSON:"""</span></span><br><span class="line">        </span><br><span class="line">        response = self.llm_client.generate(prompt)</span><br><span class="line">        <span class="keyword">return</span> json.loads(extract_json(response))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_format_history</span>(<span class="params">self, history</span>):</span><br><span class="line">        <span class="string">"""Format conversation history."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"<span class="subst">{turn[<span class="string">'role'</span>]}</span>: <span class="subst">{turn[<span class="string">'content'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> turn <span class="keyword">in</span> history</span><br><span class="line">        ])</span><br></pre></td></tr></table></figure>
<h2 id="token-efficiency-optimization">Token Efficiency
Optimization</h2>
<p>LLM API calls are expensive, especially for recommendation systems
that need to process many items. Token efficiency is crucial for
production systems.</p>
<h3 id="strategies-for-token-efficiency">Strategies for Token
Efficiency</h3>
<p><strong>1. Prompt Compression</strong>: Reduce prompt size while
maintaining information</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compress_prompt</span>(<span class="params">user_context, items, max_tokens=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compress prompt to fit within token limit.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Truncate item descriptions</span></span><br><span class="line">    compressed_items = []</span><br><span class="line">    tokens_used = count_tokens(user_context)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        item_tokens = count_tokens(item[<span class="string">'description'</span>])</span><br><span class="line">        <span class="keyword">if</span> tokens_used + item_tokens &gt; max_tokens:</span><br><span class="line">            <span class="comment"># Truncate description</span></span><br><span class="line">            truncated = truncate_to_tokens(</span><br><span class="line">                item[<span class="string">'description'</span>],</span><br><span class="line">                max_tokens - tokens_used - <span class="number">50</span>  <span class="comment"># Buffer</span></span><br><span class="line">            )</span><br><span class="line">            item[<span class="string">'description'</span>] = truncated</span><br><span class="line">        compressed_items.append(item)</span><br><span class="line">        tokens_used += item_tokens</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compressed_items</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">truncate_to_tokens</span>(<span class="params">text, max_tokens</span>):</span><br><span class="line">    <span class="string">"""Truncate text to fit within token limit."""</span></span><br><span class="line">    tokens = text.split()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) &lt;= max_tokens:</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tokens[:max_tokens]) + <span class="string">'...'</span></span><br></pre></td></tr></table></figure>
<p><strong>2. Batch Processing</strong>: Process multiple requests
together</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_recommend</span>(<span class="params">requests, llm_client, batch_size=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Process multiple recommendation requests in batches.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    results = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(requests), batch_size):</span><br><span class="line">        batch = requests[i:i+batch_size]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Combine into single prompt</span></span><br><span class="line">        combined_prompt = create_batch_prompt(batch)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Single API call</span></span><br><span class="line">        response = llm_client.generate(combined_prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Parse results</span></span><br><span class="line">        batch_results = parse_batch_response(response, <span class="built_in">len</span>(batch))</span><br><span class="line">        results.extend(batch_results)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<p><strong>3. Caching</strong>: Cache LLM responses for similar
queries</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CachedLLMClient</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    LLM client with response caching.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, cache_size=<span class="number">1000</span></span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.cache = {}</span><br><span class="line">        self.cache_size = cache_size</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, prompt, use_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Generate response with caching.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> use_cache:</span><br><span class="line">            cache_key = self._hash_prompt(prompt)</span><br><span class="line">            <span class="keyword">if</span> cache_key <span class="keyword">in</span> self.cache:</span><br><span class="line">                <span class="keyword">return</span> self.cache[cache_key]</span><br><span class="line">        </span><br><span class="line">        response = self.llm_client.generate(prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> use_cache <span class="keyword">and</span> <span class="built_in">len</span>(self.cache) &lt; self.cache_size:</span><br><span class="line">            self.cache[cache_key] = response</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_hash_prompt</span>(<span class="params">self, prompt</span>):</span><br><span class="line">        <span class="string">"""Create hash of prompt for caching."""</span></span><br><span class="line">        <span class="keyword">return</span> hashlib.md5(prompt.encode()).hexdigest()</span><br></pre></td></tr></table></figure>
<p><strong>4. Selective LLM Usage</strong>: Use LLMs only when
necessary</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelectiveLLMRecommender</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Use LLM only for complex cases, fallback to traditional methods.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm_client, traditional_model</span>):</span><br><span class="line">        self.llm_client = llm_client</span><br><span class="line">        self.traditional_model = traditional_model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">recommend</span>(<span class="params">self, user_query, items</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Use LLM only if query is complex or cold-start.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Check if traditional model can handle it</span></span><br><span class="line">        <span class="keyword">if</span> self._is_simple_query(user_query):</span><br><span class="line">            <span class="keyword">return</span> self.traditional_model.recommend(user_query, items)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if user has sufficient history</span></span><br><span class="line">        <span class="keyword">if</span> self._has_sufficient_history(user_query):</span><br><span class="line">            <span class="keyword">return</span> self.traditional_model.recommend(user_query, items)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use LLM for complex/cold-start cases</span></span><br><span class="line">        <span class="keyword">return</span> self._llm_recommend(user_query, items)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_is_simple_query</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="string">"""Check if query is simple enough for traditional model."""</span></span><br><span class="line">        <span class="comment"># Simple heuristics: length, keyword matching, etc.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(query.split()) &lt; <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_has_sufficient_history</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="string">"""Check if user has sufficient interaction history."""</span></span><br><span class="line">        <span class="comment"># Implementation depends on your system</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_llm_recommend</span>(<span class="params">self, query, items</span>):</span><br><span class="line">        <span class="string">"""LLM-based recommendation."""</span></span><br><span class="line">        <span class="comment"># Use LLM</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>5. Two-Stage Approach</strong>: Use LLM for ranking, not
retrieval</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoStageRecommender</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Two-stage recommendation: traditional retrieval + LLM reranking.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, retriever, llm_reranker</span>):</span><br><span class="line">        self.retriever = retriever</span><br><span class="line">        self.llm_reranker = llm_reranker</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">recommend</span>(<span class="params">self, user_query, top_k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Retrieve candidates, then rerank with LLM.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Stage 1: Retrieve many candidates (fast, cheap)</span></span><br><span class="line">        candidates = self.retriever.retrieve(user_query, top_k=<span class="number">100</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Stage 2: Rerank top candidates with LLM (slow, expensive)</span></span><br><span class="line">        reranked = self.llm_reranker.rerank(</span><br><span class="line">            user_query,</span><br><span class="line">            candidates[:<span class="number">20</span>],  <span class="comment"># Only rerank top 20</span></span><br><span class="line">            top_k=top_k</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> reranked</span><br></pre></td></tr></table></figure>
<h2 id="evaluation-metrics-for-llm-based-recommendation">Evaluation
Metrics for LLM-Based Recommendation</h2>
<p>Evaluating LLM-based recommendation systems requires both traditional
metrics and LLM-specific metrics.</p>
<h3 id="traditional-metrics">Traditional Metrics</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_recommendations</span>(<span class="params">recommended_items, ground_truth, k=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Evaluate recommendations using traditional metrics.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    metrics = {}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Precision@K</span></span><br><span class="line">    recommended_set = <span class="built_in">set</span>(recommended_items[:k])</span><br><span class="line">    ground_truth_set = <span class="built_in">set</span>(ground_truth)</span><br><span class="line">    intersection = recommended_set &amp; ground_truth_set</span><br><span class="line">    </span><br><span class="line">    metrics[<span class="string">'precision@k'</span>] = <span class="built_in">len</span>(intersection) / k <span class="keyword">if</span> k &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    metrics[<span class="string">'recall@k'</span>] = <span class="built_in">len</span>(intersection) / <span class="built_in">len</span>(ground_truth_set) <span class="keyword">if</span> ground_truth_set <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># NDCG@K</span></span><br><span class="line">    metrics[<span class="string">'ndcg@k'</span>] = compute_ndcg(recommended_items[:k], ground_truth)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MRR</span></span><br><span class="line">    metrics[<span class="string">'mrr'</span>] = compute_mrr(recommended_items, ground_truth)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ndcg</span>(<span class="params">recommended, ground_truth, k=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">"""Compute NDCG@K."""</span></span><br><span class="line">    dcg = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(recommended[:k]):</span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">in</span> ground_truth:</span><br><span class="line">            relevance = <span class="number">1</span>  <span class="comment"># Binary relevance</span></span><br><span class="line">            dcg += relevance / np.log2(i + <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Ideal DCG</span></span><br><span class="line">    idcg = <span class="built_in">sum</span>(<span class="number">1</span> / np.log2(i + <span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">min</span>(k, <span class="built_in">len</span>(ground_truth))))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dcg / idcg <span class="keyword">if</span> idcg &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_mrr</span>(<span class="params">recommended, ground_truth</span>):</span><br><span class="line">    <span class="string">"""Compute Mean Reciprocal Rank."""</span></span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(recommended):</span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">in</span> ground_truth:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> / (i + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="llm-specific-metrics">LLM-Specific Metrics</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_explanation_quality</span>(<span class="params">explanations, user_feedback</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Evaluate quality of LLM-generated explanations.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    metrics = {}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Relevance: Does explanation reference user preferences?</span></span><br><span class="line">    metrics[<span class="string">'relevance'</span>] = compute_relevance(explanations, user_feedback)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Coherence: Is explanation coherent?</span></span><br><span class="line">    metrics[<span class="string">'coherence'</span>] = compute_coherence(explanations)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Informativeness: Does explanation provide useful information?</span></span><br><span class="line">    metrics[<span class="string">'informativeness'</span>] = compute_informativeness(explanations)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_conversation_quality</span>(<span class="params">conversation_history</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Evaluate quality of conversational interactions.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    metrics = {}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Task completion: Did conversation achieve its goal?</span></span><br><span class="line">    metrics[<span class="string">'task_completion'</span>] = check_task_completion(conversation_history)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># User satisfaction: Based on explicit/implicit feedback</span></span><br><span class="line">    metrics[<span class="string">'user_satisfaction'</span>] = compute_satisfaction(conversation_history)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Efficiency: Number of turns to complete task</span></span><br><span class="line">    metrics[<span class="string">'efficiency'</span>] = <span class="built_in">len</span>(conversation_history) / <span class="number">2</span>  <span class="comment"># Pairs of user-assistant turns</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> metrics</span><br></pre></td></tr></table></figure>
<h2 id="practical-implementation-complete-system">Practical
Implementation: Complete System</h2>
<p>Here's a complete implementation combining multiple approaches:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLMRecommendationSystem</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Complete LLM-based recommendation system.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        self.config = config</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize components</span></span><br><span class="line">        self.llm_client = self._init_llm_client(config[<span class="string">'llm'</span>])</span><br><span class="line">        self.feature_extractor = LLMFeatureExtractor(</span><br><span class="line">            config[<span class="string">'llm'</span>][<span class="string">'model_name'</span>]</span><br><span class="line">        )</span><br><span class="line">        self.recommendation_model = self._init_recommendation_model(config)</span><br><span class="line">        self.reranker = LLMReranker(self.llm_client)</span><br><span class="line">        self.conversation_manager = ChatCRS(</span><br><span class="line">            self.llm_client,</span><br><span class="line">            self.recommendation_model</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">recommend</span>(<span class="params">self, user_id, user_query=<span class="literal">None</span>, top_k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Main recommendation interface.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Get user profile</span></span><br><span class="line">        user_profile = self._get_user_profile(user_id)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Extract features</span></span><br><span class="line">        <span class="keyword">if</span> user_query:</span><br><span class="line">            user_features = self.feature_extractor.extract_user_features(</span><br><span class="line">                user_query,</span><br><span class="line">                user_profile.get(<span class="string">'history'</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            user_features = self.feature_extractor.extract_user_features(</span><br><span class="line">                user_profile.get(<span class="string">'description'</span>, <span class="string">''</span>),</span><br><span class="line">                user_profile.get(<span class="string">'history'</span>)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get initial recommendations</span></span><br><span class="line">        recommendations = self.recommendation_model.recommend(</span><br><span class="line">            user_features=user_features,</span><br><span class="line">            user_id=user_id,</span><br><span class="line">            top_k=top_k * <span class="number">2</span>  <span class="comment"># Get more for reranking</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Rerank with LLM</span></span><br><span class="line">        reranked = self.reranker.rerank(</span><br><span class="line">            user_context=user_query <span class="keyword">or</span> user_profile.get(<span class="string">'description'</span>, <span class="string">''</span>),</span><br><span class="line">            candidates=recommendations,</span><br><span class="line">            top_k=top_k</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate explanations</span></span><br><span class="line">        explanations = []</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> reranked:</span><br><span class="line">            explanation = self._generate_explanation(</span><br><span class="line">                user_profile,</span><br><span class="line">                item</span><br><span class="line">            )</span><br><span class="line">            explanations.append({</span><br><span class="line">                <span class="string">'item'</span>: item,</span><br><span class="line">                <span class="string">'explanation'</span>: explanation</span><br><span class="line">            })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> explanations</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, session_id, user_message</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Conversational recommendation interface.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.conversation_manager.process_message(</span><br><span class="line">            session_id,</span><br><span class="line">            user_message</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_llm_client</span>(<span class="params">self, llm_config</span>):</span><br><span class="line">        <span class="string">"""Initialize LLM client."""</span></span><br><span class="line">        <span class="comment"># Implementation depends on LLM provider</span></span><br><span class="line">        <span class="keyword">if</span> llm_config[<span class="string">'provider'</span>] == <span class="string">'openai'</span>:</span><br><span class="line">            <span class="keyword">return</span> OpenAIClient(llm_config[<span class="string">'api_key'</span>])</span><br><span class="line">        <span class="keyword">elif</span> llm_config[<span class="string">'provider'</span>] == <span class="string">'anthropic'</span>:</span><br><span class="line">            <span class="keyword">return</span> AnthropicClient(llm_config[<span class="string">'api_key'</span>])</span><br><span class="line">        <span class="comment"># Add more providers</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_recommendation_model</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="string">"""Initialize recommendation model."""</span></span><br><span class="line">        <span class="comment"># Can be traditional model or LLM-based</span></span><br><span class="line">        <span class="keyword">return</span> TraditionalRecommendationModel(config[<span class="string">'model'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_user_profile</span>(<span class="params">self, user_id</span>):</span><br><span class="line">        <span class="string">"""Get user profile from database."""</span></span><br><span class="line">        <span class="comment"># Implementation depends on your data storage</span></span><br><span class="line">        <span class="keyword">return</span> {}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_explanation</span>(<span class="params">self, user_profile, item</span>):</span><br><span class="line">        <span class="string">"""Generate explanation for recommendation."""</span></span><br><span class="line">        prompt = <span class="string">f"""Explain why this item is recommended.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">User Profile: <span class="subst">{user_profile}</span></span></span><br><span class="line"><span class="string">Item: <span class="subst">{item}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Explanation:"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.llm_client.generate(prompt).strip()</span><br></pre></td></tr></table></figure>
<h2 id="questions-and-answers">Questions and Answers</h2>
<h3 id="q1-when-should-i-use-llms-for-recommendation-vs.-traditional-methods">Q1:
When should I use LLMs for recommendation vs. traditional methods?</h3>
<p><strong>A:</strong> Use LLMs when: - You have rich textual content
(descriptions, reviews, user profiles) - You need natural language
explanations - You're dealing with cold-start problems (new users/items)
- You want conversational recommendation interfaces - You need
cross-domain knowledge</p>
<p>Use traditional methods when: - You have abundant interaction data -
Latency and cost are critical constraints - You're working with
structured, numerical features - You need deterministic, reproducible
results</p>
<p><strong>Hybrid approach</strong>: Use traditional methods for
retrieval, LLMs for reranking and explanation.</p>
<h3 id="q2-how-do-i-handle-the-cost-of-llm-api-calls-in-production">Q2:
How do I handle the cost of LLM API calls in production?</h3>
<p><strong>A:</strong> Several strategies:</p>
<ol type="1">
<li><strong>Two-stage architecture</strong>: Use cheap retrieval
(traditional methods) to get candidates, then use LLM only for reranking
top candidates</li>
<li><strong>Caching</strong>: Cache LLM responses for similar
queries</li>
<li><strong>Batch processing</strong>: Combine multiple requests into
single API calls</li>
<li><strong>Selective usage</strong>: Use LLMs only for complex queries
or cold-start cases</li>
<li><strong>Prompt optimization</strong>: Minimize prompt size while
maintaining quality</li>
<li><strong>Fine-tuning</strong>: Fine-tune smaller models for your
specific domain (cheaper than API calls)</li>
</ol>
<h3 id="q3-how-do-i-ensure-llm-recommendations-are-fair-and-unbiased">Q3:
How do I ensure LLM recommendations are fair and unbiased?</h3>
<p><strong>A:</strong> LLMs can inherit biases from training data.
Mitigation strategies:</p>
<ol type="1">
<li><strong>Bias detection</strong>: Monitor recommendations for
demographic biases</li>
<li><strong>Prompt engineering</strong>: Include fairness constraints in
prompts</li>
<li><strong>Post-processing</strong>: Apply fairness filters to LLM
outputs</li>
<li><strong>Diverse sampling</strong>: Ensure diversity in
recommendations</li>
<li><strong>User feedback</strong>: Collect and incorporate user
feedback on fairness</li>
<li><strong>Regular audits</strong>: Periodically audit recommendations
for bias</li>
</ol>
<h3 id="q4-can-i-fine-tune-llms-for-recommendation-tasks">Q4: Can I
fine-tune LLMs for recommendation tasks?</h3>
<p><strong>A:</strong> Yes, fine-tuning can improve performance:</p>
<ol type="1">
<li><strong>Task-specific fine-tuning</strong>: Fine-tune on
recommendation datasets</li>
<li><strong>Domain adaptation</strong>: Fine-tune on your specific
domain (movies, products, etc.)</li>
<li><strong>Parameter-efficient methods</strong>: Use LoRA or adapter
layers to reduce costs</li>
<li><strong>Instruction tuning</strong>: Fine-tune to follow
recommendation-specific instructions</li>
</ol>
<p>Example: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fine_tune_llm_for_recommendation</span>(<span class="params">model, tokenizer, dataset</span>):</span><br><span class="line">    <span class="string">"""Fine-tune LLM for recommendation."""</span></span><br><span class="line">    training_args = TrainingArguments(</span><br><span class="line">        output_dir=<span class="string">'./results'</span>,</span><br><span class="line">        num_train_epochs=<span class="number">3</span>,</span><br><span class="line">        per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">        learning_rate=<span class="number">2e-5</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model=model,</span><br><span class="line">        args=training_args,</span><br><span class="line">        train_dataset=dataset</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    trainer.train()</span><br></pre></td></tr></table></figure></p>
<h3 id="q5-how-do-i-handle-long-item-catalogs-that-exceed-token-limits">Q5:
How do I handle long item catalogs that exceed token limits?</h3>
<p><strong>A:</strong> Strategies:</p>
<ol type="1">
<li><strong>Retrieval first</strong>: Use traditional retrieval to
narrow down candidates before LLM processing</li>
<li><strong>Hierarchical approach</strong>: Process items in batches,
then combine results</li>
<li><strong>Summarization</strong>: Summarize item descriptions to
reduce tokens</li>
<li><strong>Embedding-based filtering</strong>: Use embeddings to filter
items before LLM processing</li>
<li><strong>Iterative refinement</strong>: Start with broad categories,
then narrow down</li>
</ol>
<h3 id="q6-how-do-i-evaluate-llm-based-recommendation-systems">Q6: How
do I evaluate LLM-based recommendation systems?</h3>
<p><strong>A:</strong> Use multiple evaluation dimensions:</p>
<ol type="1">
<li><strong>Accuracy metrics</strong>: Precision@K, Recall@K, NDCG@K
(same as traditional)</li>
<li><strong>Explanation quality</strong>: Relevance, coherence,
informativeness</li>
<li><strong>Conversation quality</strong>: Task completion, user
satisfaction, efficiency</li>
<li><strong>Diversity</strong>: Ensure recommendations are diverse</li>
<li><strong>Fairness</strong>: Check for demographic biases</li>
<li><strong>User studies</strong>: A/B testing with real users</li>
</ol>
<h3 id="q7-whats-the-difference-between-zero-shot-few-shot-and-fine-tuned-llm-recommendation">Q7:
What's the difference between zero-shot, few-shot, and fine-tuned LLM
recommendation?</h3>
<p><strong>A:</strong></p>
<ul>
<li><strong>Zero-shot</strong>: No examples, relies entirely on
pre-trained knowledge. Fastest to deploy but may be less accurate.</li>
<li><strong>Few-shot</strong>: Includes examples in prompt. Better
accuracy than zero-shot, moderate deployment complexity.</li>
<li><strong>Fine-tuned</strong>: Model weights updated on recommendation
data. Best accuracy but requires training data and computational
resources.</li>
</ul>
<p>Choose based on your accuracy requirements, available data, and
computational budget.</p>
<h3 id="q8-how-do-i-handle-multi-modal-recommendation-with-llms">Q8: How
do I handle multi-modal recommendation with LLMs?</h3>
<p><strong>A:</strong> LLMs can process text, and with vision models
(like GPT-4V), they can handle images:</p>
<ol type="1">
<li><strong>Text + Images</strong>: Use vision-language models for items
with images</li>
<li><strong>Structured data</strong>: Convert structured features to
text descriptions</li>
<li><strong>Multi-modal embeddings</strong>: Combine text and image
embeddings</li>
<li><strong>Hybrid approach</strong>: Use specialized models for each
modality, LLM for fusion</li>
</ol>
<h3 id="q9-can-llms-handle-real-time-recommendation-with-low-latency">Q9:
Can LLMs handle real-time recommendation with low latency?</h3>
<p><strong>A:</strong> LLM API calls can be slow (1-5 seconds). For
real-time systems:</p>
<ol type="1">
<li><strong>Caching</strong>: Cache frequent queries</li>
<li><strong>Async processing</strong>: Generate recommendations
asynchronously</li>
<li><strong>Two-stage</strong>: Use fast retrieval, slower LLM
reranking</li>
<li><strong>Local models</strong>: Deploy smaller models locally for
lower latency</li>
<li><strong>Pre-computation</strong>: Pre-compute recommendations for
common queries</li>
</ol>
<h3 id="q10-how-do-i-handle-privacy-concerns-with-llm-based-recommendation">Q10:
How do I handle privacy concerns with LLM-based recommendation?</h3>
<p><strong>A:</strong> Privacy considerations:</p>
<ol type="1">
<li><strong>Data minimization</strong>: Only send necessary data to LLM
APIs</li>
<li><strong>Local deployment</strong>: Use locally deployed models when
possible</li>
<li><strong>Data anonymization</strong>: Remove PII before sending to
APIs</li>
<li><strong>User consent</strong>: Obtain consent for data usage</li>
<li><strong>Federated learning</strong>: Train models without
centralizing user data</li>
<li><strong>Differential privacy</strong>: Add noise to protect
individual privacy</li>
</ol>
<h3 id="q11-what-are-the-main-challenges-in-deploying-llm-based-recommendation-systems">Q11:
What are the main challenges in deploying LLM-based recommendation
systems?</h3>
<p><strong>A:</strong> Key challenges:</p>
<ol type="1">
<li><strong>Cost</strong>: LLM API calls are expensive at scale</li>
<li><strong>Latency</strong>: API calls can be slow</li>
<li><strong>Reliability</strong>: API availability and rate limits</li>
<li><strong>Consistency</strong>: Non-deterministic outputs</li>
<li><strong>Evaluation</strong>: Harder to evaluate than traditional
methods</li>
<li><strong>Bias</strong>: Inherited biases from training data</li>
<li><strong>Token limits</strong>: Handling large catalogs</li>
</ol>
<h3 id="q12-how-do-i-combine-llm-recommendations-with-traditional-collaborative-filtering">Q12:
How do I combine LLM recommendations with traditional collaborative
filtering?</h3>
<p><strong>A:</strong> Hybrid approaches:</p>
<ol type="1">
<li><strong>Ensemble</strong>: Combine scores from both methods</li>
<li><strong>Two-stage</strong>: Use CF for retrieval, LLM for
reranking</li>
<li><strong>Feature fusion</strong>: Use LLM features in traditional
models</li>
<li><strong>Weighted combination</strong>: Learn weights for combining
methods</li>
<li><strong>Context-aware switching</strong>: Use LLM for cold-start, CF
for warm users</li>
</ol>
<p>Example: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hybrid_recommend</span>(<span class="params">user_id, user_query</span>):</span><br><span class="line">    <span class="string">"""Hybrid recommendation combining CF and LLM."""</span></span><br><span class="line">    <span class="comment"># CF recommendations</span></span><br><span class="line">    cf_recs = collaborative_filtering.recommend(user_id, top_k=<span class="number">20</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LLM reranking</span></span><br><span class="line">    llm_recs = llm_reranker.rerank(user_query, cf_recs, top_k=<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Combine scores</span></span><br><span class="line">    final_recs = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> llm_recs:</span><br><span class="line">        cf_score = item.get(<span class="string">'cf_score'</span>, <span class="number">0</span>)</span><br><span class="line">        llm_score = item.get(<span class="string">'llm_score'</span>, <span class="number">0</span>)</span><br><span class="line">        combined_score = <span class="number">0.6</span> * cf_score + <span class="number">0.4</span> * llm_score</span><br><span class="line">        item[<span class="string">'final_score'</span>] = combined_score</span><br><span class="line">        final_recs.append(item)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(final_recs, key=<span class="keyword">lambda</span> x: x[<span class="string">'final_score'</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="conclusion">Conclusion</h2>
<p>Large Language Models are transforming recommendation systems by
bringing semantic understanding, natural language generation, and
conversational capabilities. From zero-shot prompt-based recommendation
to sophisticated conversational systems like ChatCRS, LLMs address
fundamental limitations of traditional methods while introducing new
possibilities for explainability and user interaction.</p>
<p>However, LLM-based recommendation is not a panacea. Cost, latency,
and reliability concerns require careful architecture design, often
combining LLMs with traditional methods in hybrid systems. Key: to
leverage LLMs where they add the most valueâ€”semantic understanding,
explanation generation, and conversational interactionâ€”while using
efficient traditional methods for retrieval and ranking.</p>
<p>As LLM technology continues to evolve, we can expect more efficient
models, better fine-tuning techniques, and improved integration with
recommendation systems. The future of recommendation lies in combining
the pattern recognition strength of traditional methods with the
semantic understanding and natural language capabilities of LLMs,
creating systems that are both accurate and intuitive for users.</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>Post titleï¼šRecommendation Systems (12): Large Language Models and Recommendation</li>
        <li>Post authorï¼šChen Kai</li>
        <li>Create timeï¼š2025-10-31 00:00:00</li>
        <li>
            Post linkï¼šhttps://www.chenk.top/en/recommendation-systems-12-llm-recommendation/
        </li>
        <li>
            Copyright Noticeï¼šAll articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/en/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/Recommendation-Systems/">#Recommendation Systems</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/Large-Language-Models/">#Large Language Models</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/en/recommendation-systems-14-cross-domain-cold-start/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Recommendation Systems (14): Cross-Domain Recommendation and Cold-Start Solutions</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/en/prompt-engineering-complete-guide/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Prompt Engineering Complete Guide: From Zero to Advanced Optimization</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: 'ðŸ˜œ å°½æƒ…èµžç¾Žå¸…æ°”ä¼Ÿå¤§çš„ckå§~',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return 'åšä¸»';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-role-of-llms-in-recommendation-systems"><span class="nav-number">1.</span> <span class="nav-text">The Role of LLMs in
Recommendation Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-llms-for-recommendation"><span class="nav-number">1.1.</span> <span class="nav-text">Why LLMs for Recommendation?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#three-primary-roles-of-llms"><span class="nav-number">1.2.</span> <span class="nav-text">Three Primary Roles of LLMs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prompt-based-recommendation"><span class="nav-number">2.</span> <span class="nav-text">Prompt-Based Recommendation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#basic-prompt-structure"><span class="nav-number">2.1.</span> <span class="nav-text">Basic Prompt Structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zero-shot-recommendation"><span class="nav-number">2.2.</span> <span class="nav-text">Zero-Shot Recommendation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#few-shot-recommendation"><span class="nav-number">2.3.</span> <span class="nav-text">Few-Shot Recommendation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chain-of-thought-recommendation"><span class="nav-number">2.4.</span> <span class="nav-text">Chain-of-Thought
Recommendation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prompt-template-design"><span class="nav-number">2.5.</span> <span class="nav-text">Prompt Template Design</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a-llmrec-augmented-llm-for-recommendation"><span class="nav-number">3.</span> <span class="nav-text">A-LLMRec: Augmented
LLM for Recommendation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#architecture-overview"><span class="nav-number">3.1.</span> <span class="nav-text">Architecture Overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation"><span class="nav-number">3.2.</span> <span class="nav-text">Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-a-llmrec"><span class="nav-number">3.3.</span> <span class="nav-text">Training A-LLMRec</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xrec-explainable-llm-based-recommendation"><span class="nav-number">4.</span> <span class="nav-text">XRec: Explainable
LLM-Based Recommendation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#architecture"><span class="nav-number">4.1.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation-1"><span class="nav-number">4.2.</span> <span class="nav-text">Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-aspect-explanation"><span class="nav-number">4.3.</span> <span class="nav-text">Multi-Aspect Explanation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-as-feature-enhancer"><span class="nav-number">5.</span> <span class="nav-text">LLM as Feature Enhancer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#text-feature-extraction"><span class="nav-number">5.1.</span> <span class="nav-text">Text Feature Extraction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#structured-feature-generation"><span class="nav-number">5.2.</span> <span class="nav-text">Structured Feature
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hybrid-feature-integration"><span class="nav-number">5.3.</span> <span class="nav-text">Hybrid Feature Integration</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-as-reranker"><span class="nav-number">6.</span> <span class="nav-text">LLM as Reranker</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#reranking-architecture"><span class="nav-number">6.1.</span> <span class="nav-text">Reranking Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pairwise-reranking"><span class="nav-number">6.2.</span> <span class="nav-text">Pairwise Reranking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#efficient-batch-reranking"><span class="nav-number">6.3.</span> <span class="nav-text">Efficient Batch Reranking</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conversational-recommendation-chatrec"><span class="nav-number">7.</span> <span class="nav-text">Conversational
Recommendation: ChatREC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#architecture-1"><span class="nav-number">7.1.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation-2"><span class="nav-number">7.2.</span> <span class="nav-text">Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ra-rec-retrieval-augmented-recommendation"><span class="nav-number">8.</span> <span class="nav-text">RA-Rec:
Retrieval-Augmented Recommendation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#architecture-2"><span class="nav-number">8.1.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation-3"><span class="nav-number">8.2.</span> <span class="nav-text">Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chatcrs-conversational-recommendation-system"><span class="nav-number">9.</span> <span class="nav-text">ChatCRS:
Conversational Recommendation System</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-turn-dialogue-management"><span class="nav-number">9.1.</span> <span class="nav-text">Multi-Turn Dialogue
Management</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#token-efficiency-optimization"><span class="nav-number">10.</span> <span class="nav-text">Token Efficiency
Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#strategies-for-token-efficiency"><span class="nav-number">10.1.</span> <span class="nav-text">Strategies for Token
Efficiency</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#evaluation-metrics-for-llm-based-recommendation"><span class="nav-number">11.</span> <span class="nav-text">Evaluation
Metrics for LLM-Based Recommendation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#traditional-metrics"><span class="nav-number">11.1.</span> <span class="nav-text">Traditional Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#llm-specific-metrics"><span class="nav-number">11.2.</span> <span class="nav-text">LLM-Specific Metrics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#practical-implementation-complete-system"><span class="nav-number">12.</span> <span class="nav-text">Practical
Implementation: Complete System</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#questions-and-answers"><span class="nav-number">13.</span> <span class="nav-text">Questions and Answers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-when-should-i-use-llms-for-recommendation-vs.-traditional-methods"><span class="nav-number">13.1.</span> <span class="nav-text">Q1:
When should I use LLMs for recommendation vs. traditional methods?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-how-do-i-handle-the-cost-of-llm-api-calls-in-production"><span class="nav-number">13.2.</span> <span class="nav-text">Q2:
How do I handle the cost of LLM API calls in production?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-how-do-i-ensure-llm-recommendations-are-fair-and-unbiased"><span class="nav-number">13.3.</span> <span class="nav-text">Q3:
How do I ensure LLM recommendations are fair and unbiased?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-can-i-fine-tune-llms-for-recommendation-tasks"><span class="nav-number">13.4.</span> <span class="nav-text">Q4: Can I
fine-tune LLMs for recommendation tasks?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-how-do-i-handle-long-item-catalogs-that-exceed-token-limits"><span class="nav-number">13.5.</span> <span class="nav-text">Q5:
How do I handle long item catalogs that exceed token limits?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-how-do-i-evaluate-llm-based-recommendation-systems"><span class="nav-number">13.6.</span> <span class="nav-text">Q6: How
do I evaluate LLM-based recommendation systems?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q7-whats-the-difference-between-zero-shot-few-shot-and-fine-tuned-llm-recommendation"><span class="nav-number">13.7.</span> <span class="nav-text">Q7:
What&#39;s the difference between zero-shot, few-shot, and fine-tuned LLM
recommendation?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q8-how-do-i-handle-multi-modal-recommendation-with-llms"><span class="nav-number">13.8.</span> <span class="nav-text">Q8: How
do I handle multi-modal recommendation with LLMs?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q9-can-llms-handle-real-time-recommendation-with-low-latency"><span class="nav-number">13.9.</span> <span class="nav-text">Q9:
Can LLMs handle real-time recommendation with low latency?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q10-how-do-i-handle-privacy-concerns-with-llm-based-recommendation"><span class="nav-number">13.10.</span> <span class="nav-text">Q10:
How do I handle privacy concerns with LLM-based recommendation?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q11-what-are-the-main-challenges-in-deploying-llm-based-recommendation-systems"><span class="nav-number">13.11.</span> <span class="nav-text">Q11:
What are the main challenges in deploying LLM-based recommendation
systems?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q12-how-do-i-combine-llm-recommendations-with-traditional-collaborative-filtering"><span class="nav-number">13.12.</span> <span class="nav-text">Q12:
How do I combine LLM recommendations with traditional collaborative
filtering?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conclusion"><span class="nav-number">14.</span> <span class="nav-text">Conclusion</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
