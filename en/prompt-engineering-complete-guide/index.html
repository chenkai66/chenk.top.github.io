<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            Prompt Engineering Complete Guide: From Zero to Advanced Optimization |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">Prompt Engineering Complete Guide: From Zero to Advanced Optimization</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-04-01 00:00:00</span>
        <span class="mobile">2025-04-01 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/categories/Large-Language-Models/">Large Language Models</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/Prompt-Engineering/">Prompt Engineering</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>11.7k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>73 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>Large language models have fundamentally changed how we interact with
AI systems. Yet most users still struggle to extract their full
potential. The difference between a mediocre response and an exceptional
one often comes down to prompt engineering—a practice that blends
empirical experimentation with systematic methodology.</p>
<p>This guide walks you through the entire spectrum of prompt
engineering, from foundational techniques that require no special
knowledge to cutting-edge optimization frameworks used in production
systems. You'll learn not just what works, but why it works, backed by
research findings and practical code examples. Whether you're building
AI applications or simply want better ChatGPT responses, the principles
here apply universally across modern LLMs.</p>
<span id="more"></span>
<h2 id="why-prompt-engineering-matters">Why Prompt Engineering
Matters</h2>
<p>When OpenAI released GPT-3 in 2020, researchers quickly discovered
something surprising: the same model could produce vastly different
results depending on how you phrased your request. A poorly worded
prompt might generate nonsense, while a carefully crafted one could
solve complex reasoning tasks. This wasn't a bug—it was a fundamental
property of how these models learn language patterns.</p>
<p>Traditional programming operates on exact instructions: write a
function, specify inputs and outputs, and the computer executes
deterministically. Language models work differently. They predict the
most likely continuation of text based on patterns learned from
trillions of tokens. Your prompt doesn't command the model; it sets up a
context that nudges probability distributions toward useful outputs.</p>
<p>Think of it like this: asking "What's the capital of France?" is
simple lookup. But asking "Explain why the French Revolution began"
requires the model to synthesize historical context, identify causal
relationships, and structure a coherent narrative. The quality of that
synthesis depends heavily on how you frame the problem.</p>
<p>Early prompt engineering was mostly trial and error. Researchers
would tweak phrasing, add examples, or modify formatting until results
improved. But as models grew more capable, systematic approaches
emerged. Today's advanced techniques—chain-of-thought reasoning, tree
search algorithms, automatic prompt optimization—represent a mature
field backed by rigorous evaluation.</p>
<p>The stakes are high. A well-engineered prompt can reduce API costs by
10x through more efficient context usage. It can boost task accuracy
from 40% to 90% on complex reasoning benchmarks. For production systems
handling millions of requests, these improvements translate to real
business value.</p>
<h2 id="fundamental-techniques">Fundamental Techniques</h2>
<p>Before diving into advanced methods, master these core approaches.
They form the building blocks of all prompt engineering strategies.</p>
<h3 id="zero-shot-prompting">Zero-Shot Prompting</h3>
<p>Zero-shot prompting means asking the model to perform a task without
providing any examples. You rely entirely on the model's pre-training to
understand and execute your request.</p>
<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Classify the sentiment of this review:</span></span><br><span class="line"><span class="string">&quot;The movie was boring and predictable. I wouldn&#x27;t recommend it.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Sentiment:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>The model sees no examples of sentiment classification. It must infer
from its training that "boring" and "wouldn't recommend" indicate
negative sentiment.</p>
<p><strong>When zero-shot works well:</strong></p>
<ul>
<li>Simple, well-defined tasks the model has seen during training
(translation, summarization, basic Q&amp;A)</li>
<li>Tasks with clear conventions (like sentiment being "positive" or
"negative")</li>
<li>When you want fast prototyping without gathering examples</li>
</ul>
<p><strong>When zero-shot fails:</strong></p>
<ul>
<li>Domain-specific jargon or specialized tasks</li>
<li>Ambiguous instructions</li>
<li>Tasks requiring specific output formatting</li>
</ul>
<p>Research from the GPT-3 paper (Brown et al., 2020) showed zero-shot
accuracy of 59% on natural language inference tasks. For comparison,
few-shot prompting improved this to 70%, demonstrating the value of
examples.</p>
<p><strong>Optimization tips:</strong></p>
<ol type="1">
<li><p><strong>Be explicit about the task.</strong> Instead of "Tell me
about this review," say "Classify the sentiment as positive, negative,
or neutral."</p></li>
<li><p><strong>Specify output format.</strong> Add "Return only one
word: positive, negative, or neutral" to prevent verbose
responses.</p></li>
<li><p><strong>Add constraints.</strong> "Ignore sarcasm and focus on
literal sentiment" helps avoid common pitfalls.</p></li>
</ol>
<p>Here's a production-ready template:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">zero_shot_classify</span>(<span class="params">text, labels</span>):</span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Task: Classify the following text into one of these categories: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(labels)&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Text: <span class="subst">&#123;text&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Category (respond with only the category name):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> prompt</span><br></pre></td></tr></table></figure>
<h3 id="few-shot-prompting">Few-Shot Prompting</h3>
<p>Few-shot prompting provides 2-10 examples before asking the model to
perform your task. This dramatically improves accuracy by establishing
patterns the model can follow.</p>
<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Classify the sentiment of these reviews:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Review: &quot;Absolutely loved this film! Best movie I&#x27;ve seen all year.&quot;</span></span><br><span class="line"><span class="string">Sentiment: positive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Review: &quot;It was okay. Nothing special but not terrible either.&quot;</span></span><br><span class="line"><span class="string">Sentiment: neutral</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Review: &quot;Waste of time and money. Horrible acting.&quot;</span></span><br><span class="line"><span class="string">Sentiment: negative</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Review: &quot;The cinematography was stunning and the story kept me engaged.&quot;</span></span><br><span class="line"><span class="string">Sentiment:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>The model now has concrete examples showing the mapping from text to
sentiment labels. This helps in several ways:</p>
<ol type="1">
<li><strong>Demonstrates output format</strong> (single word,
lowercase)</li>
<li><strong>Shows edge cases</strong> (mixed reviews map to
"neutral")</li>
<li><strong>Primes the model's context</strong> with relevant semantic
patterns</li>
</ol>
<p><strong>Key insight:</strong> Few-shot examples act as soft
conditioning. The model's next-token prediction mechanism looks for
patterns in the examples and applies them to the new input. You're
essentially programming through demonstration.</p>
<p><strong>Choosing examples:</strong></p>
<p>Research by Liu et al. (2021) on "What Makes Good In-Context
Examples?" found that:</p>
<ul>
<li><strong>Diversity matters more than volume.</strong> 5 diverse
examples beat 20 similar ones.</li>
<li><strong>Hard examples help.</strong> Include edge cases the model
might struggle with.</li>
<li><strong>Order affects results.</strong> Place most relevant examples
nearest to the query.</li>
</ul>
<p><strong>Example selection algorithm:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_examples</span>(<span class="params">query, example_pool, k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Select k diverse, relevant examples for few-shot prompting.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Uses embedding similarity to find relevant examples,</span></span><br><span class="line"><span class="string">    then filters for diversity.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute embeddings (using any embedding model)</span></span><br><span class="line">    query_emb = get_embedding(query)</span><br><span class="line">    example_embs = [get_embedding(ex[<span class="string">&#x27;text&#x27;</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> example_pool]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get top-k most similar</span></span><br><span class="line">    similarities = cosine_similarity([query_emb], example_embs)[<span class="number">0</span>]</span><br><span class="line">    top_k_indices = similarities.argsort()[-k*<span class="number">2</span>:][::-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Filter for diversity (select dissimilar examples from top candidates)</span></span><br><span class="line">    selected = [top_k_indices[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> top_k_indices[<span class="number">1</span>:]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(selected) &gt;= k:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># Add if not too similar to already selected</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>(cosine_similarity(</span><br><span class="line">            [example_embs[idx]], </span><br><span class="line">            [example_embs[s]]</span><br><span class="line">        )[<span class="number">0</span>][<span class="number">0</span>] &lt; <span class="number">0.9</span> <span class="keyword">for</span> s <span class="keyword">in</span> selected):</span><br><span class="line">            selected.append(idx)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [example_pool[i] <span class="keyword">for</span> i <span class="keyword">in</span> selected]</span><br></pre></td></tr></table></figure>
<p><strong>Performance benchmarks:</strong></p>
<p>On the SuperGLUE benchmark (Wang et al., 2019), GPT-3 achieved: -
Zero-shot: 69.5% average accuracy - One-shot: 71.8% - Few-shot (32
examples): 75.2%</p>
<p>Diminishing returns kick in around 10-15 examples for most tasks.</p>
<h3 id="many-shot-prompting">Many-Shot Prompting</h3>
<p>Recent research (Anthropic, 2024) shows that extremely long contexts
(100K+ tokens) enable "many-shot" prompting with hundreds of examples.
This bridges the gap between few-shot prompting and traditional
fine-tuning.</p>
<p><strong>Example scenario:</strong> You're building a specialized code
reviewer that catches company-specific anti-patterns. Instead of
fine-tuning (which requires infrastructure and expertise), you provide
200 examples of code reviews in the prompt.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Abbreviated example - real prompt would have 100+ examples</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Review the following code for anti-patterns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[Example 1]</span></span><br><span class="line"><span class="string">Code: def getData(): return db.query(&#x27;SELECT * FROM users&#x27;)</span></span><br><span class="line"><span class="string">Issue: Raw SQL injection risk. Use parameterized queries.</span></span><br><span class="line"><span class="string">Severity: High</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[Example 2]</span></span><br><span class="line"><span class="string">Code: result = api.call(); result.field</span></span><br><span class="line"><span class="string">Issue: No error handling. Add try-catch for network failures.</span></span><br><span class="line"><span class="string">Severity: Medium</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[... 198 more examples ...]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[Example 200]</span></span><br><span class="line"><span class="string">Code: if x == True: return True</span></span><br><span class="line"><span class="string">Issue: Redundant comparison. Use &#x27;if x: return True&#x27;</span></span><br><span class="line"><span class="string">Severity: Low</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Now review this code:</span></span><br><span class="line"><span class="string">&#123;user_code&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Issue:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>Why many-shot works:</strong></p>
<ol type="1">
<li><p><strong>Statistical learning.</strong> With hundreds of examples,
the model effectively learns a task-specific distribution, similar to
fine-tuning but without weight updates.</p></li>
<li><p><strong>Reduced ambiguity.</strong> More examples cover more edge
cases, leaving less room for misinterpretation.</p></li>
<li><p><strong>Format consistency.</strong> The model sees the output
pattern so many times it rarely deviates.</p></li>
</ol>
<p><strong>Anthropic's findings (2024):</strong></p>
<ul>
<li>On specialized tasks, 500-shot prompting approaches fine-tuned model
performance</li>
<li>Gains plateau around 200-300 examples for most tasks</li>
<li>Works best with Claude's 200K context window</li>
</ul>
<p><strong>Trade-offs:</strong></p>
<ul>
<li><strong>Cost:</strong> Processing 100K+ token prompts is expensive.
At GPT-4 pricing ($0.01/1K tokens), a 200K context costs $2 per
request.</li>
<li><strong>Latency:</strong> Longer prompts mean slower generation
(though batching can amortize costs).</li>
<li><strong>Caching:</strong> Use prompt caching (available in Claude,
GPT-4) to reuse long contexts across requests.</li>
</ul>
<p><strong>When to use many-shot:</strong></p>
<ul>
<li>Specialized domains where fine-tuning is impractical</li>
<li>Rapid iteration (adding examples is easier than retraining)</li>
<li>Tasks requiring nuanced judgment that benefits from diverse
examples</li>
</ul>
<h3 id="task-decomposition">Task Decomposition</h3>
<p>Complex tasks often fail because you're asking the model to do too
much at once. Decomposition breaks a hard problem into simpler
sub-problems.</p>
<p><strong>Example:</strong> Instead of "Analyze this legal contract and
extract all obligations," decompose:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 1: Identify sections</span></span><br><span class="line">sections = extract_sections(contract)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: For each section, extract obligations</span></span><br><span class="line">obligations = []</span><br><span class="line"><span class="keyword">for</span> section <span class="keyword">in</span> sections:</span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Legal text:</span></span><br><span class="line"><span class="string">    <span class="subst">&#123;section&#125;</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    List all obligations mentioned in this text.</span></span><br><span class="line"><span class="string">    Format each as: [Party] must [Action] [Conditions]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    obligations.extend(llm_call(prompt))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Classify obligations by type</span></span><br><span class="line">classified = classify_obligations(obligations)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Generate summary</span></span><br><span class="line">summary = summarize_obligations(classified)</span><br></pre></td></tr></table></figure>
<p><strong>Why decomposition helps:</strong></p>
<ol type="1">
<li><strong>Reduces cognitive load.</strong> Each sub-task is simpler
and has clearer success criteria.</li>
<li><strong>Enables validation.</strong> You can check intermediate
outputs before proceeding.</li>
<li><strong>Improves debugging.</strong> When something fails, you know
exactly which step broke.</li>
</ol>
<p><strong>Real-world example:</strong> GitHub Copilot Workspace
decomposes "Implement a feature" into: 1. Understand the codebase
(semantic search) 2. Identify affected files 3. Generate individual file
edits 4. Synthesize a complete solution</p>
<p>Each step uses specialized prompts optimized for that sub-task.</p>
<p><strong>Pattern library:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extract-Transform-Load pattern</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">etl_pattern</span>(<span class="params">data, extraction_prompt, transformation_prompt</span>):</span><br><span class="line">    extracted = llm_call(extraction_prompt.<span class="built_in">format</span>(data=data))</span><br><span class="line">    transformed = llm_call(transformation_prompt.<span class="built_in">format</span>(data=extracted))</span><br><span class="line">    <span class="keyword">return</span> transformed</span><br><span class="line"></span><br><span class="line"><span class="comment"># Map-Reduce pattern</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">map_reduce_pattern</span>(<span class="params">items, map_prompt, reduce_prompt</span>):</span><br><span class="line">    mapped = [llm_call(map_prompt.<span class="built_in">format</span>(item=item)) <span class="keyword">for</span> item <span class="keyword">in</span> items]</span><br><span class="line">    reduced = llm_call(reduce_prompt.<span class="built_in">format</span>(items=mapped))</span><br><span class="line">    <span class="keyword">return</span> reduced</span><br><span class="line"></span><br><span class="line"><span class="comment"># Validate-Retry pattern</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validate_retry_pattern</span>(<span class="params">prompt, validator, max_retries=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">for</span> attempt <span class="keyword">in</span> <span class="built_in">range</span>(max_retries):</span><br><span class="line">        result = llm_call(prompt)</span><br><span class="line">        <span class="keyword">if</span> validator(result):</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        prompt += <span class="string">f&quot;\n\nPrevious attempt was invalid: <span class="subst">&#123;result&#125;</span>\nTry again:&quot;</span></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Max retries exceeded&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="advanced-reasoning-techniques">Advanced Reasoning
Techniques</h2>
<p>Moving beyond basic prompting, these techniques explicitly guide the
model's reasoning process, dramatically improving performance on complex
tasks.</p>
<h3 id="chain-of-thought-cot-prompting">Chain-of-Thought (CoT)
Prompting</h3>
<p>Chain-of-thought prompting asks the model to show its work—to
generate intermediate reasoning steps before producing a final answer.
This simple change yields massive improvements on math, logic, and
multi-step reasoning tasks.</p>
<p><strong>The discovery:</strong> Wei et al. (2022) found that adding
"think step by step" to prompts improved accuracy on grade-school math
problems from 17% to 78% with GPT-3.</p>
<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Without CoT</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. </span></span><br><span class="line"><span class="string">Each can has 3 tennis balls. How many tennis balls does he have now?</span></span><br><span class="line"><span class="string">A:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Model often outputs: &quot;10&quot; (incorrect)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With CoT</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. </span></span><br><span class="line"><span class="string">Each can has 3 tennis balls. How many tennis balls does he have now?</span></span><br><span class="line"><span class="string">A: think step by step.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Model outputs:</span></span><br><span class="line"><span class="comment"># &quot;Roger starts with 5 balls.</span></span><br><span class="line"><span class="comment"># He buys 2 cans, each with 3 balls.</span></span><br><span class="line"><span class="comment"># 2 cans × 3 balls = 6 balls.</span></span><br><span class="line"><span class="comment"># Total: 5 + 6 = 11 balls.&quot;</span></span><br><span class="line"><span class="comment"># Answer: 11 (correct!)</span></span><br></pre></td></tr></table></figure>
<p><strong>Why CoT works:</strong></p>
<p>The prevailing theory (from interpretability research) is that
language models perform implicit computation through their forward pass.
Each layer refines representations, but there's a limit to how much
computation a single forward pass can do. By generating intermediate
steps, the model gets to "think longer" through multiple forward passes
(one per generated token).</p>
<p>Think of it like working memory: humans solve complex problems by
writing down intermediate results. CoT lets models do the same.</p>
<p><strong>Variants:</strong></p>
<p><strong>1. Zero-shot CoT:</strong> Just add "think step by step"
without examples. Surprisingly effective across diverse tasks.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">zero_shot_cot</span>(<span class="params">question</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;question&#125;</span>\n\nthink step by step.&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>2. Few-shot CoT:</strong> Provide examples that include
reasoning chains.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q: If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?</span></span><br><span class="line"><span class="string">A: think step by step.</span></span><br><span class="line"><span class="string">- 5 machines make 5 widgets in 5 minutes</span></span><br><span class="line"><span class="string">- That means each machine makes 1 widget in 5 minutes</span></span><br><span class="line"><span class="string">- So 100 machines make 100 widgets in 5 minutes</span></span><br><span class="line"><span class="string">Answer: 5 minutes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: A farmer has 15 sheep and all but 8 die. How many are left?</span></span><br><span class="line"><span class="string">A: think step by step.</span></span><br><span class="line"><span class="string">- &quot;All but 8 die&quot; means 8 survive</span></span><br><span class="line"><span class="string">- The number who die is 15 - 8 = 7</span></span><br><span class="line"><span class="string">- The number left is 8</span></span><br><span class="line"><span class="string">Answer: 8 sheep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: &#123;new_question&#125;</span></span><br><span class="line"><span class="string">A: think step by step.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>3. Structured CoT:</strong> Enforce specific reasoning
formats.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Problem: <span class="subst">&#123;problem&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Solve this using the following structure:</span></span><br><span class="line"><span class="string">1. What is known: [List given information]</span></span><br><span class="line"><span class="string">2. What is unknown: [What we need to find]</span></span><br><span class="line"><span class="string">3. Relevant formulas/principles: [What applies here]</span></span><br><span class="line"><span class="string">4. Step-by-step solution:</span></span><br><span class="line"><span class="string">   a) [First step with explanation]</span></span><br><span class="line"><span class="string">   b) [Second step with explanation]</span></span><br><span class="line"><span class="string">   ...</span></span><br><span class="line"><span class="string">5. Final answer: [Concise answer]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>Benchmark results (Wei et al., 2022):</strong></p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Baseline</th>
<th>CoT</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8K (math)</td>
<td>17.1%</td>
<td>78.2%</td>
<td>+357%</td>
</tr>
<tr>
<td>SVAMP (math)</td>
<td>63.7%</td>
<td>79.0%</td>
<td>+24%</td>
</tr>
<tr>
<td>CommonsenseQA</td>
<td>72.5%</td>
<td>78.1%</td>
<td>+7.7%</td>
</tr>
<tr>
<td>StrategyQA</td>
<td>54.3%</td>
<td>66.1%</td>
<td>+21.7%</td>
</tr>
</tbody>
</table>
<p><strong>When CoT helps most:</strong></p>
<ul>
<li>Multi-step reasoning (math, logic puzzles)</li>
<li>Problems requiring intermediate calculations</li>
<li>Tasks where the reasoning path matters (explainability)</li>
<li>Complex decision-making with trade-offs</li>
</ul>
<p><strong>When CoT doesn't help:</strong></p>
<ul>
<li>Simple lookup tasks ("What's the capital of France?")</li>
<li>When the model lacks requisite knowledge (CoT can't fix missing
facts)</li>
<li>Very short answers (generating reasoning is wasteful)</li>
</ul>
<p><strong>Implementation best practices:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChainOfThoughtEngine</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, temperature=<span class="number">0.7</span></span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.temperature = temperature</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">self, question, mode=<span class="string">&#x27;zero-shot&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;zero-shot&#x27;</span>:</span><br><span class="line">            prompt = <span class="string">f&quot;<span class="subst">&#123;question&#125;</span>\n\nthink step by step.&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&#x27;few-shot&#x27;</span>:</span><br><span class="line">            prompt = self._build_few_shot_prompt(question)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Unknown mode: <span class="subst">&#123;mode&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate reasoning</span></span><br><span class="line">        response = self.model.generate(</span><br><span class="line">            prompt, </span><br><span class="line">            temperature=self.temperature,</span><br><span class="line">            max_tokens=<span class="number">512</span>  <span class="comment"># Allow room for reasoning</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Extract answer (often after &quot;Answer:&quot; or &quot;Therefore,&quot;)</span></span><br><span class="line">        answer = self._extract_answer(response)</span><br><span class="line">        <span class="keyword">return</span> answer, response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_extract_answer</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Extract final answer from reasoning chain.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Look for common answer markers</span></span><br><span class="line">        markers = [<span class="string">&quot;Answer:&quot;</span>, <span class="string">&quot;Therefore,&quot;</span>, <span class="string">&quot;The answer is&quot;</span>]</span><br><span class="line">        <span class="keyword">for</span> marker <span class="keyword">in</span> markers:</span><br><span class="line">            <span class="keyword">if</span> marker <span class="keyword">in</span> response:</span><br><span class="line">                <span class="comment"># Get text after marker</span></span><br><span class="line">                after_marker = response.split(marker)[-<span class="number">1</span>].strip()</span><br><span class="line">                <span class="comment"># Take first sentence/line</span></span><br><span class="line">                answer = after_marker.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].split(<span class="string">&#x27;\n&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">return</span> answer.strip()</span><br><span class="line">        <span class="comment"># Fallback: return last line</span></span><br><span class="line">        <span class="keyword">return</span> response.strip().split(<span class="string">&#x27;\n&#x27;</span>)[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="self-consistency">Self-Consistency</h3>
<p>Self-consistency (Wang et al., 2022) improves CoT by generating
multiple reasoning paths and selecting the most common answer through
majority vote.</p>
<p><strong>Intuition:</strong> A single reasoning chain might make a
mistake. But if you generate 10 chains and 7 reach the same answer,
that's likely correct.</p>
<p><strong>Algorithm:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_consistency</span>(<span class="params">question, n=<span class="number">5</span>, temperature=<span class="number">0.7</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate n reasoning chains and return majority answer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        question: Problem to solve</span></span><br><span class="line"><span class="string">        n: Number of chains to generate</span></span><br><span class="line"><span class="string">        temperature: Sampling temperature (higher = more diversity)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Most common answer and confidence score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">    </span><br><span class="line">    prompt = <span class="string">f&quot;<span class="subst">&#123;question&#125;</span>\n\nthink step by step.&quot;</span></span><br><span class="line">    </span><br><span class="line">    answers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        response = llm_call(</span><br><span class="line">            prompt, </span><br><span class="line">            temperature=temperature,</span><br><span class="line">            max_tokens=<span class="number">512</span></span><br><span class="line">        )</span><br><span class="line">        answer = extract_answer(response)</span><br><span class="line">        answers.append(answer)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Majority vote</span></span><br><span class="line">    vote_counts = Counter(answers)</span><br><span class="line">    majority_answer, count = vote_counts.most_common(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    confidence = count / n</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> majority_answer, confidence</span><br></pre></td></tr></table></figure>
<p><strong>Example:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Question: &quot;If you overtake the person in 2nd place, what place are you in?&quot;</span><br><span class="line"></span><br><span class="line">Chain 1: &quot;You overtake 2nd, so you&#x27;re now 2nd. Answer: 2nd place&quot; ✓</span><br><span class="line">Chain 2: &quot;You were behind 2nd, now you&#x27;re ahead, so 1st. Answer: 1st place&quot; ✗</span><br><span class="line">Chain 3: &quot;Overtaking 2nd means you take their position. Answer: 2nd place&quot; ✓</span><br><span class="line">Chain 4: &quot;You pass the person in 2nd. You&#x27;re now 2nd. Answer: 2nd place&quot; ✓</span><br><span class="line">Chain 5: &quot;You overtake 2nd, making you 1st. Answer: 1st place&quot; ✗</span><br><span class="line"></span><br><span class="line">Majority: &quot;2nd place&quot; (3/5 = 60% confidence)</span><br></pre></td></tr></table></figure>
<p><strong>Performance gains (Wang et al., 2022):</strong></p>
<p>On GSM8K math problems: - Standard CoT: 74.4% - Self-consistency
(n=40): 83.7% (+12.5%)</p>
<p>On CommonsenseQA: - Standard CoT: 78.1% - Self-consistency (n=40):
81.5% (+4.4%)</p>
<p><strong>Cost-performance trade-offs:</strong></p>
<p>Self-consistency requires n×cost of single inference. Choose n based
on task importance:</p>
<table>
<thead>
<tr>
<th>Task criticality</th>
<th>n</th>
<th>Cost multiplier</th>
</tr>
</thead>
<tbody>
<tr>
<td>Exploratory</td>
<td>3</td>
<td>3×</td>
</tr>
<tr>
<td>Production</td>
<td>5</td>
<td>5×</td>
</tr>
<tr>
<td>High-stakes</td>
<td>10-20</td>
<td>10-20×</td>
</tr>
</tbody>
</table>
<p><strong>Optimization:</strong> Use lower n initially, then increase
for low-confidence cases:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adaptive_self_consistency</span>(<span class="params">question, confidence_threshold=<span class="number">0.7</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Start with n=3, increase if confidence is low.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    answer, confidence = self_consistency(question, n=<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> confidence &gt;= confidence_threshold:</span><br><span class="line">        <span class="keyword">return</span> answer, confidence</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Low confidence, generate more chains</span></span><br><span class="line">    answer, confidence = self_consistency(question, n=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> answer, confidence</span><br></pre></td></tr></table></figure>
<p><strong>Advanced variant - weighted voting:</strong></p>
<p>Not all reasoning chains are equal. Weight votes by chain
quality:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_self_consistency</span>(<span class="params">question, n=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Weight each answer by reasoning chain quality.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="string">f&quot;<span class="subst">&#123;question&#125;</span>\n\nthink step by step.&quot;</span></span><br><span class="line">    </span><br><span class="line">    answers_with_scores = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        response = llm_call(prompt, temperature=<span class="number">0.7</span>)</span><br><span class="line">        answer = extract_answer(response)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Score reasoning quality</span></span><br><span class="line">        quality_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Rate the logical quality of this reasoning (0-10):</span></span><br><span class="line"><span class="string">        <span class="subst">&#123;response&#125;</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Score:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        quality = <span class="built_in">int</span>(llm_call(quality_prompt, temperature=<span class="number">0</span>))</span><br><span class="line">        </span><br><span class="line">        answers_with_scores.append((answer, quality))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Weighted vote</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">    vote_weights = defaultdict(<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">for</span> answer, quality <span class="keyword">in</span> answers_with_scores:</span><br><span class="line">        vote_weights[answer] += quality</span><br><span class="line">    </span><br><span class="line">    best_answer = <span class="built_in">max</span>(vote_weights, key=vote_weights.get)</span><br><span class="line">    confidence = vote_weights[best_answer] / <span class="built_in">sum</span>(vote_weights.values())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_answer, confidence</span><br></pre></td></tr></table></figure>
<h3 id="tree-of-thoughts-tot">Tree of Thoughts (ToT)</h3>
<p>Tree of Thoughts (Yao et al., 2023) takes CoT further by exploring
multiple reasoning paths simultaneously through tree search. Instead of
a linear chain, the model explores a tree of possibilities, backtracking
when paths seem unpromising.</p>
<p><strong>Key idea:</strong> Model reasoning as search through a state
space. Each "thought" is a partial solution. Use heuristics to
prioritize promising branches.</p>
<p><strong>Algorithm (simplified DFS variant):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tree_of_thoughts</span>(<span class="params">problem, depth=<span class="number">3</span>, breadth=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Explore solution tree using depth-first search.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        problem: Initial problem statement</span></span><br><span class="line"><span class="string">        depth: Maximum depth to explore</span></span><br><span class="line"><span class="string">        breadth: Number of thoughts to generate at each node</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Best solution found</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">explore</span>(<span class="params">state, current_depth</span>):</span><br><span class="line">        <span class="keyword">if</span> current_depth &gt;= depth:</span><br><span class="line">            <span class="comment"># Terminal state, evaluate quality</span></span><br><span class="line">            <span class="keyword">return</span> evaluate_solution(state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate possible next thoughts</span></span><br><span class="line">        thoughts = generate_thoughts(state, k=breadth)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Evaluate each thought&#x27;s promise</span></span><br><span class="line">        thought_values = []</span><br><span class="line">        <span class="keyword">for</span> thought <span class="keyword">in</span> thoughts:</span><br><span class="line">            <span class="comment"># Combine current state with thought</span></span><br><span class="line">            new_state = state + <span class="string">&quot;\n&quot;</span> + thought</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Recursively explore this branch</span></span><br><span class="line">            value = explore(new_state, current_depth + <span class="number">1</span>)</span><br><span class="line">            thought_values.append((thought, value))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Return best branch</span></span><br><span class="line">        best_thought, best_value = <span class="built_in">max</span>(thought_values, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> best_value</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Start exploration from initial problem</span></span><br><span class="line">    explore(problem, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_thoughts</span>(<span class="params">state, k=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate k possible next reasoning steps.&quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Current reasoning:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;state&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate <span class="subst">&#123;k&#125;</span> different possible next steps:</span></span><br><span class="line"><span class="string">1.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    response = llm_call(prompt, temperature=<span class="number">0.8</span>, max_tokens=<span class="number">300</span>)</span><br><span class="line">    <span class="comment"># Parse into list of thoughts</span></span><br><span class="line">    thoughts = [t.strip() <span class="keyword">for</span> t <span class="keyword">in</span> response.split(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">if</span> t.strip()]</span><br><span class="line">    <span class="keyword">return</span> thoughts[:k]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_solution</span>(<span class="params">state</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Score how promising this reasoning path is.&quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Rate this reasoning on a scale of 1-10:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;state&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Consider: logical coherence, progress toward solution, likelihood of correctness.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Score (integer 1-10):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    score = <span class="built_in">int</span>(llm_call(prompt, temperature=<span class="number">0</span>, max_tokens=<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>
<p><strong>Concrete example (Game of 24):</strong></p>
<p>Task: Use four numbers (e.g., 4, 9, 10, 13) with +, -, ×, ÷ to make
24.</p>
<p><strong>ToT exploration:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Root: &quot;4, 9, 10, 13&quot;</span><br><span class="line">├─ Thought 1: &quot;13 - 9 = 4&quot; → State: &quot;4, 4, 10&quot;</span><br><span class="line">│  ├─ Thought 1.1: &quot;10 - 4 = 6&quot; → State: &quot;4, 6&quot;</span><br><span class="line">│  │  └─ Thought 1.1.1: &quot;6 × 4 = 24&quot; ✓ SOLUTION FOUND</span><br><span class="line">│  ├─ Thought 1.2: &quot;4 × 4 = 16&quot; → State: &quot;10, 16&quot;</span><br><span class="line">│  │  └─ Dead end (can&#x27;t make 24)</span><br><span class="line">│  └─ Thought 1.3: &quot;10 + 4 = 14&quot; → State: &quot;4, 14&quot;</span><br><span class="line">│      └─ Dead end</span><br><span class="line">├─ Thought 2: &quot;10 - 4 = 6&quot; → State: &quot;6, 9, 13&quot;</span><br><span class="line">│  └─ ... (explore further)</span><br><span class="line">└─ Thought 3: &quot;9 + 10 = 19&quot; → State: &quot;4, 13, 19&quot;</span><br><span class="line">   └─ ... (explore further)</span><br></pre></td></tr></table></figure>
<p>The tree search backtracks when a path seems unpromising (evaluated
by the model itself), exploring alternative branches.</p>
<p><strong>Benchmark results (Yao et al., 2023):</strong></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CoT</th>
<th>ToT</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Game of 24</td>
<td>7.3%</td>
<td>74%</td>
<td>+914%</td>
</tr>
<tr>
<td>Creative writing</td>
<td>7.3</td>
<td>7.9</td>
<td>+8.2%</td>
</tr>
<tr>
<td>Crosswords</td>
<td>15.6%</td>
<td>78%</td>
<td>+400%</td>
</tr>
</tbody>
</table>
<p><strong>Why ToT works:</strong></p>
<ol type="1">
<li><strong>Exploration:</strong> Tries multiple approaches rather than
committing to first thought</li>
<li><strong>Self-evaluation:</strong> Model judges its own reasoning
quality</li>
<li><strong>Backtracking:</strong> Abandons dead ends early</li>
</ol>
<p><strong>Implementation challenges:</strong></p>
<ul>
<li><strong>Cost:</strong> Exploring a tree with breadth=3 and depth=4
requires 3^4 = 81 LLM calls</li>
<li><strong>Evaluation accuracy:</strong> Model must reliably judge
which thoughts are promising</li>
<li><strong>Search strategy:</strong> DFS, BFS, or best-first? Each has
trade-offs.</li>
</ul>
<p><strong>Production-ready implementation:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TreeOfThoughts</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, max_calls=<span class="number">50</span></span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.max_calls = max_calls</span><br><span class="line">        self.call_count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">self, problem</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Solve problem using BFS-based ToT.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">from</span> queue <span class="keyword">import</span> PriorityQueue</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Priority queue of (value, state)</span></span><br><span class="line">        queue = PriorityQueue()</span><br><span class="line">        queue.put((<span class="number">0</span>, problem, []))  <span class="comment"># (priority, state, path)</span></span><br><span class="line">        </span><br><span class="line">        best_solution = <span class="literal">None</span></span><br><span class="line">        best_score = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> queue.empty() <span class="keyword">and</span> self.call_count &lt; self.max_calls:</span><br><span class="line">            priority, state, path = queue.get()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if this is a terminal state</span></span><br><span class="line">            <span class="keyword">if</span> self._is_terminal(state):</span><br><span class="line">                score = self._evaluate(state)</span><br><span class="line">                <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">                    best_score = score</span><br><span class="line">                    best_solution = state</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Generate next thoughts</span></span><br><span class="line">            thoughts = self._generate_thoughts(state, k=<span class="number">3</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> thought <span class="keyword">in</span> thoughts:</span><br><span class="line">                new_state = state + <span class="string">&quot;\n&quot;</span> + thought</span><br><span class="line">                new_path = path + [thought]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Evaluate promise of this thought</span></span><br><span class="line">                value = self._evaluate_thought(new_state)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Add to queue (negative because PriorityQueue is min-heap)</span></span><br><span class="line">                queue.put((-value, new_state, new_path))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> best_solution, best_score</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_thoughts</span>(<span class="params">self, state, k</span>):</span><br><span class="line">        self.call_count += <span class="number">1</span></span><br><span class="line">        prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Current state:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;state&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate <span class="subst">&#123;k&#125;</span> possible next steps:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        response = self.model.generate(prompt, temperature=<span class="number">0.8</span>)</span><br><span class="line">        <span class="keyword">return</span> self._parse_thoughts(response, k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_evaluate_thought</span>(<span class="params">self, state</span>):</span><br><span class="line">        self.call_count += <span class="number">1</span></span><br><span class="line">        prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Rate this reasoning (1-10):</span></span><br><span class="line"><span class="string"><span class="subst">&#123;state&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Score:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(self.model.generate(prompt, temperature=<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_is_terminal</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="comment"># Task-specific logic</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Answer:&quot;</span> <span class="keyword">in</span> state <span class="keyword">or</span> <span class="built_in">len</span>(state.split(<span class="string">&#x27;\n&#x27;</span>)) &gt; <span class="number">10</span></span><br></pre></td></tr></table></figure>
<h3 id="graph-of-thoughts-got">Graph of Thoughts (GoT)</h3>
<p>Graph of Thoughts (Besta et al., 2023) generalizes ToT by allowing
arbitrary graph structures. Thoughts can merge (combine multiple
reasoning paths) or branch, enabling more complex reasoning
patterns.</p>
<p><strong>Key insight:</strong> Not all reasoning follows trees.
Sometimes you want to: - <strong>Merge:</strong> Combine insights from
multiple branches - <strong>Loop:</strong> Refine a solution iteratively
- <strong>Aggregate:</strong> Synthesize information from many
sources</p>
<p><strong>Example - Document analysis:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Documents → Split into chunks → Process each chunk</span><br><span class="line">                                ↓</span><br><span class="line">                          Extract key points</span><br><span class="line">                                ↓</span><br><span class="line">                          Merge all key points → Synthesize summary</span><br></pre></td></tr></table></figure>
<p>This is a graph: multiple parallel paths (one per chunk) that merge
into a synthesis step.</p>
<p><strong>Graph structure representation:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ThoughtGraph</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.nodes = &#123;&#125;  <span class="comment"># id -&gt; thought content</span></span><br><span class="line">        self.edges = []  <span class="comment"># (from_id, to_id, operation)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_node</span>(<span class="params">self, node_id, content</span>):</span><br><span class="line">        self.nodes[node_id] = content</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_edge</span>(<span class="params">self, from_id, to_id, operation=<span class="string">&#x27;continue&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        operation can be:</span></span><br><span class="line"><span class="string">        - &#x27;continue&#x27;: Standard progression</span></span><br><span class="line"><span class="string">        - &#x27;merge&#x27;: Combine multiple thoughts</span></span><br><span class="line"><span class="string">        - &#x27;refine&#x27;: Iteratively improve</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.edges.append((from_id, to_id, operation))</span><br></pre></td></tr></table></figure>
<p><strong>Example implementation - multi-document
summarization:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">graph_of_thoughts_summarization</span>(<span class="params">documents</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Summarize multiple documents using GoT.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Graph structure:</span></span><br><span class="line"><span class="string">    1. Generate summary for each document (parallel)</span></span><br><span class="line"><span class="string">    2. Extract key themes from each summary (parallel)</span></span><br><span class="line"><span class="string">    3. Merge all themes</span></span><br><span class="line"><span class="string">    4. Generate final synthesis</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    graph = ThoughtGraph()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Layer 1: Individual summaries</span></span><br><span class="line">    summaries = []</span><br><span class="line">    <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(documents):</span><br><span class="line">        node_id = <span class="string">f&quot;summary_<span class="subst">&#123;i&#125;</span>&quot;</span></span><br><span class="line">        summary = llm_call(<span class="string">f&quot;Summarize this document:\n<span class="subst">&#123;doc&#125;</span>&quot;</span>)</span><br><span class="line">        graph.add_node(node_id, summary)</span><br><span class="line">        summaries.append((node_id, summary))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Layer 2: Key themes from each summary</span></span><br><span class="line">    themes = []</span><br><span class="line">    <span class="keyword">for</span> node_id, summary <span class="keyword">in</span> summaries:</span><br><span class="line">        theme_node_id = <span class="string">f&quot;theme_<span class="subst">&#123;node_id&#125;</span>&quot;</span></span><br><span class="line">        theme = llm_call(<span class="string">f&quot;Extract key themes from:\n<span class="subst">&#123;summary&#125;</span>&quot;</span>)</span><br><span class="line">        graph.add_node(theme_node_id, theme)</span><br><span class="line">        graph.add_edge(node_id, theme_node_id, <span class="string">&#x27;continue&#x27;</span>)</span><br><span class="line">        themes.append((theme_node_id, theme))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Layer 3: Merge themes</span></span><br><span class="line">    merge_node_id = <span class="string">&quot;merged_themes&quot;</span></span><br><span class="line">    all_themes = <span class="string">&quot;\n\n&quot;</span>.join([t[<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> themes])</span><br><span class="line">    merged = llm_call(<span class="string">f&quot;Merge these themes:\n<span class="subst">&#123;all_themes&#125;</span>&quot;</span>)</span><br><span class="line">    graph.add_node(merge_node_id, merged)</span><br><span class="line">    <span class="keyword">for</span> theme_id, _ <span class="keyword">in</span> themes:</span><br><span class="line">        graph.add_edge(theme_id, merge_node_id, <span class="string">&#x27;merge&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Layer 4: Final synthesis</span></span><br><span class="line">    final_node_id = <span class="string">&quot;final_summary&quot;</span></span><br><span class="line">    synthesis = llm_call(<span class="string">f&quot;Synthesize a final summary from:\n<span class="subst">&#123;merged&#125;</span>&quot;</span>)</span><br><span class="line">    graph.add_node(final_node_id, synthesis)</span><br><span class="line">    graph.add_edge(merge_node_id, final_node_id, <span class="string">&#x27;continue&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> synthesis, graph</span><br></pre></td></tr></table></figure>
<p><strong>Advantages over ToT:</strong></p>
<ol type="1">
<li><strong>Parallelization:</strong> Independent branches can run
concurrently</li>
<li><strong>Information aggregation:</strong> Merge multiple
perspectives</li>
<li><strong>Iterative refinement:</strong> Loops for incremental
improvement</li>
</ol>
<p><strong>Performance (Besta et al., 2023):</strong></p>
<p>On sorting tasks (sort 32 numbers), GoT achieved: - 89% accuracy -
62% cost reduction vs. ToT (through parallelization)</p>
<h3 id="react-reasoning-acting">ReAct (Reasoning + Acting)</h3>
<p>ReAct (Yao et al., 2022) interleaves reasoning with actions in the
world. Instead of pure thought, the model alternates between thinking
(reasoning) and acting (calling tools, APIs, executing code).</p>
<p><strong>Pattern:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Thought: I need to find the population of Paris</span><br><span class="line">Action: search(&quot;Paris population&quot;)</span><br><span class="line">Observation: 2.16 million (2019)</span><br><span class="line">Thought: Now I need Tokyo&#x27;s population</span><br><span class="line">Action: search(&quot;Tokyo population&quot;)</span><br><span class="line">Observation: 37.4 million (2021)</span><br><span class="line">Thought: Tokyo is larger than Paris</span><br><span class="line">Action: finish(&quot;Tokyo has a larger population than Paris&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Why ReAct matters:</strong></p>
<p>Language models struggle with: - Current information (training data
is stale) - Precise calculations - Accessing private data</p>
<p>ReAct bridges this by letting models use external tools.</p>
<p><strong>Architecture:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReActAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, tools, max_steps=<span class="number">10</span></span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.tools = tools  <span class="comment"># Dict of tool_name -&gt; function</span></span><br><span class="line">        self.max_steps = max_steps</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, task</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Execute task using ReAct loop.&quot;&quot;&quot;</span></span><br><span class="line">        trajectory = [<span class="string">f&quot;Task: <span class="subst">&#123;task&#125;</span>&quot;</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(self.max_steps):</span><br><span class="line">            <span class="comment"># Generate thought + action</span></span><br><span class="line">            prompt = self._build_prompt(trajectory)</span><br><span class="line">            response = self.model.generate(prompt, temperature=<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Parse response</span></span><br><span class="line">            thought, action, action_input = self._parse_response(response)</span><br><span class="line">            </span><br><span class="line">            trajectory.append(<span class="string">f&quot;Thought: <span class="subst">&#123;thought&#125;</span>&quot;</span>)</span><br><span class="line">            trajectory.append(<span class="string">f&quot;Action: <span class="subst">&#123;action&#125;</span>[<span class="subst">&#123;action_input&#125;</span>]&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Execute action</span></span><br><span class="line">            <span class="keyword">if</span> action == <span class="string">&quot;finish&quot;</span>:</span><br><span class="line">                <span class="keyword">return</span> action_input</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> action <span class="keyword">not</span> <span class="keyword">in</span> self.tools:</span><br><span class="line">                observation = <span class="string">f&quot;Error: Unknown action &#x27;<span class="subst">&#123;action&#125;</span>&#x27;&quot;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                observation = self.tools[action](action_input)</span><br><span class="line">            </span><br><span class="line">            trajectory.append(<span class="string">f&quot;Observation: <span class="subst">&#123;observation&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Maximum steps reached without finishing&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_prompt</span>(<span class="params">self, trajectory</span>):</span><br><span class="line">        tools_desc = <span class="string">&quot;\n&quot;</span>.join([</span><br><span class="line">            <span class="string">f&quot;- <span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;func.__doc__&#125;</span>&quot;</span> </span><br><span class="line">            <span class="keyword">for</span> name, func <span class="keyword">in</span> self.tools.items()</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">You can use these tools:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;tools_desc&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Always respond in this format:</span></span><br><span class="line"><span class="string">Thought: [your reasoning]</span></span><br><span class="line"><span class="string">Action: [tool_name]</span></span><br><span class="line"><span class="string">Action Input: [input for tool]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">When done:</span></span><br><span class="line"><span class="string">Thought: [final reasoning]</span></span><br><span class="line"><span class="string">Action: finish</span></span><br><span class="line"><span class="string">Action Input: [final answer]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;<span class="built_in">chr</span>(<span class="number">10</span>).join(trajectory)&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Thought:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parse_response</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Extract thought, action, and input from model response.&quot;&quot;&quot;</span></span><br><span class="line">        lines = response.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        thought = <span class="string">&quot;&quot;</span></span><br><span class="line">        action = <span class="string">&quot;&quot;</span></span><br><span class="line">        action_input = <span class="string">&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">&quot;Thought:&quot;</span>):</span><br><span class="line">                thought = line.split(<span class="string">&quot;Thought:&quot;</span>)[-<span class="number">1</span>].strip()</span><br><span class="line">            <span class="keyword">elif</span> line.startswith(<span class="string">&quot;Action:&quot;</span>):</span><br><span class="line">                action = line.split(<span class="string">&quot;Action:&quot;</span>)[-<span class="number">1</span>].strip()</span><br><span class="line">            <span class="keyword">elif</span> line.startswith(<span class="string">&quot;Action Input:&quot;</span>):</span><br><span class="line">                action_input = line.split(<span class="string">&quot;Action Input:&quot;</span>)[-<span class="number">1</span>].strip()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> thought, action, action_input</span><br></pre></td></tr></table></figure>
<p><strong>Example tools:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">query</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Search the web for information.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Integration with search API</span></span><br><span class="line">    <span class="keyword">import</span> requests</span><br><span class="line">    response = requests.get(</span><br><span class="line">        <span class="string">&quot;https://api.search.com/search&quot;</span>,</span><br><span class="line">        params=&#123;<span class="string">&quot;q&quot;</span>: query&#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response.json()[<span class="string">&quot;snippet&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate</span>(<span class="params">expression</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Evaluate a mathematical expression.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># Safe eval (in production, use a proper math parser)</span></span><br><span class="line">        result = <span class="built_in">eval</span>(expression, &#123;<span class="string">&quot;__builtins__&quot;</span>: &#123;&#125;&#125;, &#123;&#125;)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(result)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;Error: <span class="subst">&#123;e&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">wikipedia</span>(<span class="params">query</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get Wikipedia summary.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">import</span> wikipediaapi</span><br><span class="line">    wiki = wikipediaapi.Wikipedia(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">    page = wiki.page(query)</span><br><span class="line">    <span class="keyword">return</span> page.summary[:<span class="number">500</span>]</span><br><span class="line"></span><br><span class="line">tools = &#123;</span><br><span class="line">    <span class="string">&quot;search&quot;</span>: search,</span><br><span class="line">    <span class="string">&quot;calculate&quot;</span>: calculate,</span><br><span class="line">    <span class="string">&quot;wikipedia&quot;</span>: wikipedia</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Real-world usage:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">agent = ReActAgent(model, tools, max_steps=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">result = agent.run(</span><br><span class="line">    <span class="string">&quot;What is the total GDP of the three largest EU countries?&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execution trace:</span></span><br><span class="line"><span class="comment"># Thought: I need to identify the three largest EU countries by GDP</span></span><br><span class="line"><span class="comment"># Action: search</span></span><br><span class="line"><span class="comment"># Action Input: largest EU countries by GDP</span></span><br><span class="line"><span class="comment"># Observation: Germany, France, Italy are top 3</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Thought: Now I need each country&#x27;s GDP</span></span><br><span class="line"><span class="comment"># Action: search</span></span><br><span class="line"><span class="comment"># Action Input: Germany GDP 2023</span></span><br><span class="line"><span class="comment"># Observation: $4.3 trillion</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Thought: Get France GDP</span></span><br><span class="line"><span class="comment"># Action: search</span></span><br><span class="line"><span class="comment"># Action Input: France GDP 2023</span></span><br><span class="line"><span class="comment"># Observation: $2.9 trillion</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Thought: Get Italy GDP</span></span><br><span class="line"><span class="comment"># Action: search</span></span><br><span class="line"><span class="comment"># Action Input: Italy GDP 2023</span></span><br><span class="line"><span class="comment"># Observation: $2.0 trillion</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Thought: Sum the three GDPs</span></span><br><span class="line"><span class="comment"># Action: calculate</span></span><br><span class="line"><span class="comment"># Action Input: 4.3 + 2.9 + 2.0</span></span><br><span class="line"><span class="comment"># Observation: 9.2</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Thought: I have the total</span></span><br><span class="line"><span class="comment"># Action: finish</span></span><br><span class="line"><span class="comment"># Action Input: The total GDP of Germany, France, and Italy is $9.2 trillion</span></span><br></pre></td></tr></table></figure>
<p><strong>Performance benchmarks (Yao et al., 2022):</strong></p>
<p>On HotpotQA (multi-hop question answering): - Standard prompting:
28.7% - CoT: 32.9% - ReAct: 37.4%</p>
<p>On AlfWorld (interactive environment): - Standard prompting: 12% -
ReAct: 34%</p>
<p><strong>Best practices:</strong></p>
<ol type="1">
<li><strong>Tool documentation:</strong> Clear docstrings help the model
choose appropriate tools</li>
<li><strong>Error handling:</strong> Tools should return descriptive
error messages</li>
<li><strong>Rate limiting:</strong> Prevent infinite loops or excessive
API calls</li>
<li><strong>Observation truncation:</strong> Limit observation length to
avoid context overflow</li>
</ol>
<h2 id="optimization-automation">Optimization &amp; Automation</h2>
<p>Manual prompt engineering doesn't scale. These techniques automate
the optimization process.</p>
<h3 id="automatic-prompt-engineering-ape">Automatic Prompt Engineering
(APE)</h3>
<p>APE (Zhou et al., 2022) automatically generates and selects optimal
prompts. Instead of manually iterating, you provide examples and let an
LLM discover effective prompts.</p>
<p><strong>Algorithm:</strong></p>
<ol type="1">
<li><strong>Generate candidate prompts:</strong> Use an LLM to create
diverse prompt variations</li>
<li><strong>Evaluate each prompt:</strong> Test on a validation set</li>
<li><strong>Select the best:</strong> Choose the highest-performing
prompt</li>
</ol>
<p><strong>Implementation:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">automatic_prompt_engineering</span>(<span class="params"></span></span><br><span class="line"><span class="params">    task_description,</span></span><br><span class="line"><span class="params">    train_examples,</span></span><br><span class="line"><span class="params">    val_examples,</span></span><br><span class="line"><span class="params">    num_candidates=<span class="number">20</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Automatically discover optimal prompt for a task.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        task_description: Natural language description of task</span></span><br><span class="line"><span class="string">        train_examples: List of (input, output) pairs for prompt generation</span></span><br><span class="line"><span class="string">        val_examples: List of (input, output) pairs for evaluation</span></span><br><span class="line"><span class="string">        num_candidates: Number of prompt variations to try</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Best prompt and its accuracy</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Step 1: Generate candidate prompts</span></span><br><span class="line">    meta_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Task: <span class="subst">&#123;task_description&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Here are some examples:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;format_examples(train_examples[:<span class="number">5</span>])&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate <span class="subst">&#123;num_candidates&#125;</span> different prompts that could solve this task.</span></span><br><span class="line"><span class="string">Each prompt should use a different approach or phrasing.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Prompts:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    candidates_text = llm_call(meta_prompt, temperature=<span class="number">1.0</span>, max_tokens=<span class="number">2000</span>)</span><br><span class="line">    candidates = parse_prompts(candidates_text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Evaluate each candidate</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> prompt <span class="keyword">in</span> candidates:</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> input_text, expected_output <span class="keyword">in</span> val_examples:</span><br><span class="line">            full_prompt = <span class="string">f&quot;<span class="subst">&#123;prompt&#125;</span>\n\nInput: <span class="subst">&#123;input_text&#125;</span>\nOutput:&quot;</span></span><br><span class="line">            output = llm_call(full_prompt, temperature=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> normalize(output) == normalize(expected_output):</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        accuracy = correct / <span class="built_in">len</span>(val_examples)</span><br><span class="line">        results.append((prompt, accuracy))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Return best prompt</span></span><br><span class="line">    best_prompt, best_accuracy = <span class="built_in">max</span>(results, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> best_prompt, best_accuracy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_examples</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;\n&quot;</span>.join([</span><br><span class="line">        <span class="string">f&quot;Input: <span class="subst">&#123;inp&#125;</span>\nOutput: <span class="subst">&#123;out&#125;</span>&quot;</span> </span><br><span class="line">        <span class="keyword">for</span> inp, out <span class="keyword">in</span> examples</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_prompts</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Extract individual prompts from generated text.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Simple parsing - in production, use more robust extraction</span></span><br><span class="line">    prompts = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> text.split(<span class="string">&#x27;\n&#x27;</span>):</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="keyword">if</span> line <span class="keyword">and</span> <span class="built_in">len</span>(line) &gt; <span class="number">20</span>:  <span class="comment"># Filter very short lines</span></span><br><span class="line">            prompts.append(line)</span><br><span class="line">    <span class="keyword">return</span> prompts</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Normalize text for comparison.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> text.strip().lower()</span><br></pre></td></tr></table></figure>
<p><strong>Example usage:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">task = <span class="string">&quot;Classify whether a product review is positive or negative&quot;</span></span><br><span class="line"></span><br><span class="line">train_examples = [</span><br><span class="line">    (<span class="string">&quot;Great product, works perfectly!&quot;</span>, <span class="string">&quot;positive&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Terrible quality, broke after 1 day&quot;</span>, <span class="string">&quot;negative&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Decent but overpriced&quot;</span>, <span class="string">&quot;negative&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Exceeded my expectations&quot;</span>, <span class="string">&quot;positive&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">val_examples = [</span><br><span class="line">    (<span class="string">&quot;Love it!&quot;</span>, <span class="string">&quot;positive&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Disappointed&quot;</span>, <span class="string">&quot;negative&quot;</span>),</span><br><span class="line">    <span class="comment"># ... more examples</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">best_prompt, accuracy = automatic_prompt_engineering(</span><br><span class="line">    task, train_examples, val_examples</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best prompt (accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>%&#125;</span>):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(best_prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output might be:</span></span><br><span class="line"><span class="comment"># &quot;Determine if the customer review expresses satisfaction (positive) </span></span><br><span class="line"><span class="comment">#  or dissatisfaction (negative) with the product.&quot;</span></span><br><span class="line"><span class="comment"># Accuracy: 87.3%</span></span><br></pre></td></tr></table></figure>
<p><strong>Discovered prompts often outperform human-written
ones.</strong> Zhou et al. found APE-generated prompts achieved 3-8%
higher accuracy than human baselines on various tasks.</p>
<p><strong>Why APE works:</strong></p>
<ol type="1">
<li><strong>Exploration:</strong> Tries diverse phrasings humans might
not consider</li>
<li><strong>Data-driven:</strong> Optimizes directly on your task, not
general heuristics</li>
<li><strong>Scales:</strong> Can test hundreds of candidates
automatically</li>
</ol>
<p><strong>Advanced: Iterative APE:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">iterative_ape</span>(<span class="params">task, train_examples, val_examples, iterations=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Iteratively refine prompts.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    best_prompt = task  <span class="comment"># Start with task description</span></span><br><span class="line">    best_accuracy = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;iterations&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate variations of current best</span></span><br><span class="line">        refinement_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Current prompt: <span class="subst">&#123;best_prompt&#125;</span></span></span><br><span class="line"><span class="string">Current accuracy: <span class="subst">&#123;best_accuracy:<span class="number">.1</span>%&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generate 10 improved variations of this prompt.</span></span><br><span class="line"><span class="string">Consider:</span></span><br><span class="line"><span class="string">- More specific instructions</span></span><br><span class="line"><span class="string">- Better examples</span></span><br><span class="line"><span class="string">- Clearer output format</span></span><br><span class="line"><span class="string">- Edge case handling</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Variations:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        candidates_text = llm_call(refinement_prompt, temperature=<span class="number">0.9</span>)</span><br><span class="line">        candidates = parse_prompts(candidates_text)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Evaluate</span></span><br><span class="line">        <span class="keyword">for</span> prompt <span class="keyword">in</span> candidates:</span><br><span class="line">            accuracy = evaluate_prompt(prompt, val_examples)</span><br><span class="line">            <span class="keyword">if</span> accuracy &gt; best_accuracy:</span><br><span class="line">                best_accuracy = accuracy</span><br><span class="line">                best_prompt = prompt</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;New best: <span class="subst">&#123;accuracy:<span class="number">.1</span>%&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_prompt, best_accuracy</span><br></pre></td></tr></table></figure>
<h3 id="dspy-declarative-self-improving-prompts">DSPy: Declarative
Self-improving Prompts</h3>
<p>DSPy (Khattab et al., 2023) is a framework that treats prompting as a
programming problem. Instead of writing prompts, you write programs that
generate prompts.</p>
<p><strong>Core concepts:</strong></p>
<ol type="1">
<li><strong>Signatures:</strong> Type-annotated function specs
(input/output)</li>
<li><strong>Modules:</strong> Composable prompt templates</li>
<li><strong>Optimizers:</strong> Automatic prompt tuning</li>
</ol>
<p><strong>Example - sentiment classification:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dspy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define signature (input -&gt; output types)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentSignature</span>(dspy.Signature):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Classify sentiment of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = dspy.InputField()</span><br><span class="line">    sentiment = dspy.OutputField(desc=<span class="string">&quot;positive, negative, or neutral&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentClassifier</span>(dspy.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.predictor = dspy.Predict(SentimentSignature)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">return</span> self.predictor(text=text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure LLM</span></span><br><span class="line">lm = dspy.OpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">dspy.settings.configure(lm=lm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use classifier</span></span><br><span class="line">classifier = SentimentClassifier()</span><br><span class="line">result = classifier(<span class="string">&quot;This movie was amazing!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result.sentiment)  <span class="comment"># &quot;positive&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>Automatic optimization:</strong></p>
<p>DSPy can automatically tune prompts using a training set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dspy.teleprompt <span class="keyword">import</span> BootstrapFewShot</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training data</span></span><br><span class="line">train_data = [</span><br><span class="line">    dspy.Example(text=<span class="string">&quot;Great product!&quot;</span>, sentiment=<span class="string">&quot;positive&quot;</span>),</span><br><span class="line">    dspy.Example(text=<span class="string">&quot;Terrible service&quot;</span>, sentiment=<span class="string">&quot;negative&quot;</span>),</span><br><span class="line">    <span class="comment"># ... more examples</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimize</span></span><br><span class="line">optimizer = BootstrapFewShot(metric=exact_match)</span><br><span class="line">optimized_classifier = optimizer.<span class="built_in">compile</span>(</span><br><span class="line">    SentimentClassifier(),</span><br><span class="line">    trainset=train_data</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The optimized classifier now has better prompts</span></span><br><span class="line">result = optimized_classifier(<span class="string">&quot;I love this&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>What DSPy does under the hood:</strong></p>
<ol type="1">
<li><strong>Bootstrapping:</strong> Generates examples by running the
model on training data</li>
<li><strong>Filtering:</strong> Keeps only high-quality examples</li>
<li><strong>Compilation:</strong> Inserts these examples into few-shot
prompts</li>
<li><strong>Iteration:</strong> Refines through multiple passes</li>
</ol>
<p><strong>Advanced: Multi-stage programs:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHopQA</span>(dspy.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Answer questions requiring multiple reasoning steps.&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.retrieve = dspy.Retrieve(k=<span class="number">3</span>)  <span class="comment"># Get relevant docs</span></span><br><span class="line">        self.generate_query = dspy.ChainOfThought(<span class="string">&quot;question -&gt; search_query&quot;</span>)</span><br><span class="line">        self.answer = dspy.ChainOfThought(<span class="string">&quot;context, question -&gt; answer&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, question</span>):</span><br><span class="line">        <span class="comment"># Generate search query</span></span><br><span class="line">        search_query = self.generate_query(question=question).search_query</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve context</span></span><br><span class="line">        context = self.retrieve(search_query).passages</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate answer</span></span><br><span class="line">        answer = self.answer(context=context, question=question).answer</span><br><span class="line">        <span class="keyword">return</span> answer</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSPy will automatically optimize all three sub-prompts</span></span><br></pre></td></tr></table></figure>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Modularity:</strong> Compose complex pipelines from simple
components</li>
<li><strong>Automatic optimization:</strong> No manual prompt
tuning</li>
<li><strong>Type safety:</strong> Catch errors before runtime</li>
</ul>
<p><strong>Trade-offs:</strong></p>
<ul>
<li><strong>Learning curve:</strong> Requires understanding DSPy
abstractions</li>
<li><strong>Black box:</strong> Less control than manual prompting</li>
<li><strong>Overhead:</strong> Optimization requires compute and
training data</li>
</ul>
<h3 id="llmlingua-prompt-compression">LLMLingua: Prompt Compression</h3>
<p>LLMLingua (Jiang et al., 2023) compresses prompts to reduce costs
while preserving performance. It selectively removes tokens that
contribute least to the model's understanding.</p>
<p><strong>Motivation:</strong> Long prompts are expensive. A
10,000-token prompt costs 10× more than 1,000 tokens. Can we compress
without hurting accuracy?</p>
<p><strong>Method:</strong></p>
<p>LLMLingua uses a smaller LLM to score each token's importance, then
removes low-scoring tokens.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llmlingua <span class="keyword">import</span> PromptCompressor</span><br><span class="line"></span><br><span class="line">compressor = PromptCompressor()</span><br><span class="line"></span><br><span class="line">original_prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">You are a helpful assistant. Please answer the following question accurately.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Context: The French Revolution was a period of radical political and societal </span></span><br><span class="line"><span class="string">change in France that began with the Estates-General of 1789 and ended with </span></span><br><span class="line"><span class="string">the formation of the French Consulate in November 1799. Many of its ideas </span></span><br><span class="line"><span class="string">are considered fundamental principles of liberal democracy.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: When did the French Revolution begin?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Please provide a concise answer based on the context above.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">compressed_prompt = compressor.compress(</span><br><span class="line">    original_prompt,</span><br><span class="line">    rate=<span class="number">0.5</span>,  <span class="comment"># Target 50% compression</span></span><br><span class="line">    target_token=<span class="number">200</span>  <span class="comment"># Or specify exact token budget</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original: <span class="subst">&#123;<span class="built_in">len</span>(original_prompt)&#125;</span> chars&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Compressed: <span class="subst">&#123;<span class="built_in">len</span>(compressed_prompt)&#125;</span> chars&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Compression: <span class="subst">&#123;<span class="number">100</span>*(<span class="number">1</span>-<span class="built_in">len</span>(compressed_prompt)/<span class="built_in">len</span>(original_prompt)):<span class="number">.1</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># Original: 487 chars</span></span><br><span class="line"><span class="comment"># Compressed: 201 chars</span></span><br><span class="line"><span class="comment"># Compression: 58.7%</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Compressed prompt:</span></span><br><span class="line"><span class="comment"># &quot;helpful assistant answer accurately</span></span><br><span class="line"><span class="comment"># Context: French Revolution radical political societal change France began </span></span><br><span class="line"><span class="comment"># Estates-General 1789 ended French Consulate November 1799</span></span><br><span class="line"><span class="comment"># Question: When French Revolution begin?</span></span><br><span class="line"><span class="comment"># concise answer based context&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>Key technique: Token-level importance scoring:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_token_importance</span>(<span class="params">prompt, model</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Score each token by how much it affects model&#x27;s predictions.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Uses conditional perplexity: remove token i, measure </span></span><br><span class="line"><span class="string">    how much perplexity increases.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tokens = tokenize(prompt)</span><br><span class="line">    importances = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tokens)):</span><br><span class="line">        <span class="comment"># Compute perplexity with token i</span></span><br><span class="line">        ppl_with = model.perplexity(tokens)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute perplexity without token i</span></span><br><span class="line">        tokens_without = tokens[:i] + tokens[i+<span class="number">1</span>:]</span><br><span class="line">        ppl_without = model.perplexity(tokens_without)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Importance = how much perplexity increases when removed</span></span><br><span class="line">        importance = ppl_without - ppl_with</span><br><span class="line">        importances.append(importance)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> importances</span><br></pre></td></tr></table></figure>
<p><strong>Performance (Jiang et al., 2023):</strong></p>
<p>On question-answering tasks with 2× compression: - Accuracy drop:
Only 2-3% - Cost savings: 50% - Latency improvement: 1.4× faster</p>
<p>On retrieval-augmented generation with 4× compression: - Accuracy
drop: 5-7% - Cost savings: 75%</p>
<p><strong>When to use LLMLingua:</strong></p>
<ul>
<li>Long context scenarios (RAG, document analysis)</li>
<li>Cost-sensitive applications</li>
<li>When slight accuracy trade-off is acceptable</li>
</ul>
<p><strong>Implementation tips:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AdaptiveCompressor</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compress prompts adaptively based on budget and importance.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_compressor</span>):</span><br><span class="line">        self.compressor = base_compressor</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compress</span>(<span class="params">self, prompt, budget_tokens</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compress prompt to fit within token budget.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Preserves:</span></span><br><span class="line"><span class="string">        1. Instructions (high priority)</span></span><br><span class="line"><span class="string">        2. Examples (medium priority)</span></span><br><span class="line"><span class="string">        3. Context (compress most aggressively)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sections = self._split_sections(prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Allocate budget</span></span><br><span class="line">        instruction_budget = <span class="built_in">int</span>(budget_tokens * <span class="number">0.3</span>)</span><br><span class="line">        example_budget = <span class="built_in">int</span>(budget_tokens * <span class="number">0.4</span>)</span><br><span class="line">        context_budget = budget_tokens - instruction_budget - example_budget</span><br><span class="line">        </span><br><span class="line">        compressed_sections = &#123;</span><br><span class="line">            <span class="string">&#x27;instruction&#x27;</span>: self.compressor.compress(</span><br><span class="line">                sections[<span class="string">&#x27;instruction&#x27;</span>],</span><br><span class="line">                target_token=instruction_budget</span><br><span class="line">            ),</span><br><span class="line">            <span class="string">&#x27;examples&#x27;</span>: self.compressor.compress(</span><br><span class="line">                sections[<span class="string">&#x27;examples&#x27;</span>],</span><br><span class="line">                target_token=example_budget</span><br><span class="line">            ),</span><br><span class="line">            <span class="string">&#x27;context&#x27;</span>: self.compressor.compress(</span><br><span class="line">                sections[<span class="string">&#x27;context&#x27;</span>],</span><br><span class="line">                target_token=context_budget</span><br><span class="line">            )</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self._merge_sections(compressed_sections)</span><br></pre></td></tr></table></figure>
<h2 id="practical-templates-patterns">Practical Templates &amp;
Patterns</h2>
<p>Theory is useful, but practitioners need ready-to-use templates. Here
are battle-tested patterns for common scenarios.</p>
<h3 id="structured-output-generation">Structured Output Generation</h3>
<p>Getting LLMs to output valid JSON, XML, or other structured formats
is notoriously tricky. Use these strategies:</p>
<p><strong>Strategy 1: Schema-first prompting:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_structured_output</span>(<span class="params">data, schema</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate JSON conforming to schema.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Generate a JSON object that follows this schema:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;json.dumps(schema, indent=<span class="number">2</span>)&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Rules:</span></span><br><span class="line"><span class="string">- All required fields must be present</span></span><br><span class="line"><span class="string">- Use correct data types (string, number, boolean, array, object)</span></span><br><span class="line"><span class="string">- Enum fields must use one of the specified values</span></span><br><span class="line"><span class="string">- No additional fields beyond the schema</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Input data:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;data&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Output valid JSON only (no explanation):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    response = llm_call(prompt, temperature=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Validate against schema</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        parsed = json.loads(response)</span><br><span class="line">        validate_against_schema(parsed, schema)</span><br><span class="line">        <span class="keyword">return</span> parsed</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="comment"># Retry with error feedback</span></span><br><span class="line">        <span class="keyword">return</span> retry_with_feedback(prompt, response, <span class="built_in">str</span>(e))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">schema = &#123;</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">    <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;age&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;number&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;hobbies&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;array&quot;</span>,</span><br><span class="line">            <span class="string">&quot;items&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;status&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;enum&quot;</span>: [<span class="string">&quot;active&quot;</span>, <span class="string">&quot;inactive&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = <span class="string">&quot;John is a 30-year-old software engineer who likes hiking and reading. He&#x27;s currently active.&quot;</span></span><br><span class="line"></span><br><span class="line">result = generate_structured_output(data, schema)</span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;name&quot;: &quot;John&quot;,</span></span><br><span class="line"><span class="comment">#   &quot;age&quot;: 30,</span></span><br><span class="line"><span class="comment">#   &quot;hobbies&quot;: [&quot;hiking&quot;, &quot;reading&quot;],</span></span><br><span class="line"><span class="comment">#   &quot;status&quot;: &quot;active&quot;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>
<p><strong>Strategy 2: Few-shot with valid examples:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">few_shot_structured_generation</span>():</span><br><span class="line">    prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Extract information in this JSON format:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Example 1:</span></span><br><span class="line"><span class="string">Text: &quot;Alice is 25 and works as a designer.&quot;</span></span><br><span class="line"><span class="string">Output: &#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25, &quot;occupation&quot;: &quot;designer&quot;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Example 2:</span></span><br><span class="line"><span class="string">Text: &quot;Bob, a 40-year-old teacher, lives in Boston.&quot;</span></span><br><span class="line"><span class="string">Output: &#123;&quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 40, &quot;occupation&quot;: &quot;teacher&quot;, &quot;location&quot;: &quot;Boston&quot;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Example 3:</span></span><br><span class="line"><span class="string">Text: &quot;Carol enjoys programming. She&#x27;s 35.&quot;</span></span><br><span class="line"><span class="string">Output: &#123;&quot;name&quot;: &quot;Carol&quot;, &quot;age&quot;: 35, &quot;interests&quot;: [&quot;programming&quot;]&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Now extract from this text:</span></span><br><span class="line"><span class="string">Text: &quot;David is a 28-year-old chef who loves traveling.&quot;</span></span><br><span class="line"><span class="string">Output:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> llm_call(prompt, temperature=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Strategy 3: Use function calling
(OpenAI-specific):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"></span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">    model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Extract person info from: David is a 28-year-old chef&quot;</span>&#125;</span><br><span class="line">    ],</span><br><span class="line">    functions=[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;extract_person_info&quot;</span>,</span><br><span class="line">            <span class="string">&quot;description&quot;</span>: <span class="string">&quot;Extract structured information about a person&quot;</span>,</span><br><span class="line">            <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">                <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;name&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;,</span><br><span class="line">                    <span class="string">&quot;age&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;integer&quot;</span>&#125;,</span><br><span class="line">                    <span class="string">&quot;occupation&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;name&quot;</span>]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    function_call=&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;extract_person_info&quot;</span>&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Guaranteed valid JSON matching schema</span></span><br><span class="line">args = json.loads(response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;message&quot;</span>][<span class="string">&quot;function_call&quot;</span>][<span class="string">&quot;arguments&quot;</span>])</span><br></pre></td></tr></table></figure>
<h3 id="code-generation-debugging">Code Generation &amp; Debugging</h3>
<p>Generating reliable code requires careful prompting:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_code</span>(<span class="params">task, language=<span class="string">&quot;python&quot;</span>, test_cases=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate and validate code for a task.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Write <span class="subst">&#123;language&#125;</span> code to solve this task:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Task: <span class="subst">&#123;task&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Requirements:</span></span><br><span class="line"><span class="string">- Include clear comments explaining the logic</span></span><br><span class="line"><span class="string">- Handle edge cases and errors</span></span><br><span class="line"><span class="string">- Follow best practices for <span class="subst">&#123;language&#125;</span></span></span><br><span class="line"><span class="string">- Include a main function demonstrating usage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;format_test_cases(test_cases) <span class="keyword">if</span> test_cases <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span>&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Provide complete, runnable code:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```<span class="subst">&#123;language&#125;</span></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    code = llm_call(prompt, temperature=<span class="number">0.3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Extract code from markdown</span></span><br><span class="line">    code = extract_code_block(code)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Validate</span></span><br><span class="line">    <span class="keyword">if</span> test_cases:</span><br><span class="line">        results = run_tests(code, test_cases, language)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>(r.passed <span class="keyword">for</span> r <span class="keyword">in</span> results):</span><br><span class="line">            <span class="comment"># Debug and retry</span></span><br><span class="line">            code = debug_and_fix(code, results, prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> code</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_test_cases</span>(<span class="params">test_cases</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Format test cases for prompt.&quot;&quot;&quot;</span></span><br><span class="line">    formatted = <span class="string">&quot;Test cases:\n&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_data, expected) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_cases, <span class="number">1</span>):</span><br><span class="line">        formatted += <span class="string">f&quot;<span class="subst">&#123;i&#125;</span>. Input: <span class="subst">&#123;input_data&#125;</span> → Expected: <span class="subst">&#123;expected&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="keyword">return</span> formatted</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">debug_and_fix</span>(<span class="params">code, test_results, original_prompt</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Iteratively fix failing code.&quot;&quot;&quot;</span></span><br><span class="line">    failures = [r <span class="keyword">for</span> r <span class="keyword">in</span> test_results <span class="keyword">if</span> <span class="keyword">not</span> r.passed]</span><br><span class="line">    </span><br><span class="line">    debug_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"><span class="subst">&#123;original_prompt&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Your previous code failed these tests:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;format_failures(failures)&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Fix the code to pass all tests. Provide corrected code:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    fixed_code = llm_call(debug_prompt, temperature=<span class="number">0.2</span>)</span><br><span class="line">    <span class="keyword">return</span> extract_code_block(fixed_code)</span><br></pre></td></tr></table></figure>
<h3 id="data-extraction-classification">Data Extraction &amp;
Classification</h3>
<p>Extract structured data from unstructured text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataExtractor</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Extract structured data using progressive refinement.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extract</span>(<span class="params">self, text, schema, confidence_threshold=<span class="number">0.8</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Extract data matching schema, with confidence scores.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Step 1: Initial extraction</span></span><br><span class="line">        extraction_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Extract all instances matching this schema:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;format_schema(schema)&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Text:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;text&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">For each instance, provide:</span></span><br><span class="line"><span class="string">1. Extracted data in JSON</span></span><br><span class="line"><span class="string">2. Confidence (0-1)</span></span><br><span class="line"><span class="string">3. Supporting quote from text</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Output:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        raw_extraction = self.model.generate(extraction_prompt)</span><br><span class="line">        candidates = self.parse_candidates(raw_extraction)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Validate and filter</span></span><br><span class="line">        validated = []</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            <span class="keyword">if</span> candidate[<span class="string">&#x27;confidence&#x27;</span>] &gt;= confidence_threshold:</span><br><span class="line">                <span class="comment"># Verify extraction is supported by text</span></span><br><span class="line">                <span class="keyword">if</span> self.verify_extraction(text, candidate):</span><br><span class="line">                    validated.append(candidate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Resolve conflicts (overlapping extractions)</span></span><br><span class="line">        resolved = self.resolve_conflicts(validated)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> resolved</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">verify_extraction</span>(<span class="params">self, text, candidate</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Verify that extraction is actually supported by source text.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        verification_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Source text:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;text&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Claimed extraction:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;json.dumps(candidate[<span class="string">&#x27;data&#x27;</span>])&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Supporting quote:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;candidate[<span class="string">&#x27;quote&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Is this extraction accurate and supported by the text?</span></span><br><span class="line"><span class="string">Answer: yes or no</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        answer = self.model.generate(verification_prompt, temperature=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;yes&#x27;</span> <span class="keyword">in</span> answer.lower()</span><br></pre></td></tr></table></figure>
<h3 id="creative-writing-content-generation">Creative Writing &amp;
Content Generation</h3>
<p>Generate high-quality creative content:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_creative_content</span>(<span class="params">topic, style, length, audience</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate creative content with stylistic control.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Write a <span class="subst">&#123;length&#125;</span>-word piece about: <span class="subst">&#123;topic&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Style: <span class="subst">&#123;style&#125;</span></span></span><br><span class="line"><span class="string">Audience: <span class="subst">&#123;audience&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Guidelines:</span></span><br><span class="line"><span class="string">- Start with a compelling hook</span></span><br><span class="line"><span class="string">- Use vivid, concrete details</span></span><br><span class="line"><span class="string">- Vary sentence structure for rhythm</span></span><br><span class="line"><span class="string">- End with a memorable conclusion</span></span><br><span class="line"><span class="string">- Avoid clichés and generic phrases</span></span><br><span class="line"><span class="string">- Show, don&#x27;t just tell</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Content:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use higher temperature for creativity</span></span><br><span class="line">    content = llm_call(prompt, temperature=<span class="number">0.8</span>, max_tokens=length*<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Post-process: check quality metrics</span></span><br><span class="line">    quality_score = evaluate_writing_quality(content)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> quality_score &lt; <span class="number">0.7</span>:</span><br><span class="line">        <span class="comment"># Refine using critique feedback</span></span><br><span class="line">        content = refine_content(content, prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_writing_quality</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Score writing quality across multiple dimensions.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    eval_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Rate this writing on a scale of 0-1 for each criterion:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Text:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;text&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Criteria:</span></span><br><span class="line"><span class="string">1. Originality: Unique perspective and fresh language</span></span><br><span class="line"><span class="string">2. Clarity: Easy to understand, well-structured</span></span><br><span class="line"><span class="string">3. Engagement: Captures and holds attention</span></span><br><span class="line"><span class="string">4. Concrete details: Specific, vivid examples</span></span><br><span class="line"><span class="string">5. Flow: Smooth transitions, varied rhythm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Provide scores:</span></span><br><span class="line"><span class="string">Originality:</span></span><br><span class="line"><span class="string">Clarity:</span></span><br><span class="line"><span class="string">Engagement:</span></span><br><span class="line"><span class="string">Concrete details:</span></span><br><span class="line"><span class="string">Flow:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    response = llm_call(eval_prompt, temperature=<span class="number">0</span>)</span><br><span class="line">    scores = parse_scores(response)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(scores.values()) / <span class="built_in">len</span>(scores)</span><br></pre></td></tr></table></figure>
<h3 id="multi-turn-conversation-management">Multi-turn Conversation
Management</h3>
<p>Build context-aware conversational agents:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConversationManager</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Manage multi-turn conversations with context tracking.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, system_prompt, max_context_tokens=<span class="number">4000</span></span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.system_prompt = system_prompt</span><br><span class="line">        self.max_context_tokens = max_context_tokens</span><br><span class="line">        self.history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, user_message</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Process user message and generate response.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Add user message to history</span></span><br><span class="line">        self.history.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_message&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compress history if needed</span></span><br><span class="line">        context = self.get_context()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate response</span></span><br><span class="line">        full_prompt = self.build_prompt(context)</span><br><span class="line">        response = self.model.generate(full_prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add response to history</span></span><br><span class="line">        self.history.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: response&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_context</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Get recent context within token budget.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Always include system prompt</span></span><br><span class="line">        context = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: self.system_prompt&#125;]</span><br><span class="line">        </span><br><span class="line">        token_count = count_tokens(self.system_prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add messages from most recent backward</span></span><br><span class="line">        <span class="keyword">for</span> message <span class="keyword">in</span> <span class="built_in">reversed</span>(self.history):</span><br><span class="line">            msg_tokens = count_tokens(message[<span class="string">&quot;content&quot;</span>])</span><br><span class="line">            <span class="keyword">if</span> token_count + msg_tokens &gt; self.max_context_tokens:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            context.insert(<span class="number">1</span>, message)  <span class="comment"># Insert after system prompt</span></span><br><span class="line">            token_count += msg_tokens</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> context</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">summarize_history</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Summarize old conversation turns to save tokens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.history) &lt; <span class="number">10</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Summarize oldest messages</span></span><br><span class="line">        old_messages = self.history[:<span class="number">6</span>]</span><br><span class="line">        summary_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Summarize this conversation concisely:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;format_messages(old_messages)&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Summary (2-3 sentences):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        summary = self.model.generate(summary_prompt, temperature=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Replace old messages with summary</span></span><br><span class="line">        self.history = [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Previous conversation: <span class="subst">&#123;summary&#125;</span>&quot;</span>&#125;</span><br><span class="line">        ] + self.history[<span class="number">6</span>:]</span><br></pre></td></tr></table></figure>
<h2 id="evaluation-debugging">Evaluation &amp; Debugging</h2>
<p>Prompt engineering is empirical. You need robust evaluation to know
what works.</p>
<h3 id="evaluation-metrics">Evaluation Metrics</h3>
<p><strong>1. Exact Match:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exact_match</span>(<span class="params">prediction, ground_truth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Strict equality after normalization.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> normalize(prediction) == normalize(ground_truth)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Lowercase, strip whitespace, remove punctuation.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">import</span> string</span><br><span class="line">    text = text.lower().strip()</span><br><span class="line">    text = text.translate(<span class="built_in">str</span>.maketrans(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>, string.punctuation))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(text.split())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage</span></span><br><span class="line">predictions = [<span class="string">&quot;The answer is 42.&quot;</span>, <span class="string">&quot;42&quot;</span>, <span class="string">&quot;forty-two&quot;</span>]</span><br><span class="line">ground_truth = <span class="string">&quot;42&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pred <span class="keyword">in</span> predictions:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;pred&#125;</span> → <span class="subst">&#123;exact_match(pred, ground_truth)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># The answer is 42. → True</span></span><br><span class="line"><span class="comment"># 42 → True</span></span><br><span class="line"><span class="comment"># forty-two → False</span></span><br></pre></td></tr></table></figure>
<p><strong>2. F1 Score (for extraction tasks):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_f1</span>(<span class="params">prediction, ground_truth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Token-level F1 score.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens = <span class="built_in">set</span>(normalize(prediction).split())</span><br><span class="line">    true_tokens = <span class="built_in">set</span>(normalize(ground_truth).split())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pred_tokens) == <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(true_tokens) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pred_tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(true_tokens) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    common = pred_tokens &amp; true_tokens</span><br><span class="line">    precision = <span class="built_in">len</span>(common) / <span class="built_in">len</span>(pred_tokens)</span><br><span class="line">    recall = <span class="built_in">len</span>(common) / <span class="built_in">len</span>(true_tokens)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> precision + recall == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">    <span class="keyword">return</span> f1</span><br></pre></td></tr></table></figure>
<p><strong>3. Semantic Similarity:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">semantic_similarity</span>(<span class="params">prediction, ground_truth, model=<span class="string">&quot;sentence-transformers&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Measure semantic similarity using embeddings.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, util</span><br><span class="line">    </span><br><span class="line">    model = SentenceTransformer(<span class="string">&#x27;all-MiniLM-L6-v2&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    emb1 = model.encode(prediction, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line">    emb2 = model.encode(ground_truth, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    cosine_sim = util.cos_sim(emb1, emb2).item()</span><br><span class="line">    <span class="keyword">return</span> cosine_sim</span><br></pre></td></tr></table></figure>
<p><strong>4. LLM-as-Judge:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">llm_as_judge</span>(<span class="params">prediction, ground_truth, criteria</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Use an LLM to evaluate quality.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    judge_prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Evaluate this model output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Task: <span class="subst">&#123;criteria[<span class="string">&#x27;task&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Expected: <span class="subst">&#123;ground_truth&#125;</span></span></span><br><span class="line"><span class="string">Actual: <span class="subst">&#123;prediction&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Criteria:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;format_criteria(criteria[<span class="string">&#x27;rubric&#x27;</span>])&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Provide:</span></span><br><span class="line"><span class="string">1. Score (0-10)</span></span><br><span class="line"><span class="string">2. Brief justification</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Evaluation:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    evaluation = llm_call(judge_prompt, temperature=<span class="number">0</span>)</span><br><span class="line">    score = extract_score(evaluation)</span><br><span class="line">    justification = extract_justification(evaluation)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> score, justification</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example</span></span><br><span class="line">criteria = &#123;</span><br><span class="line">    <span class="string">&quot;task&quot;</span>: <span class="string">&quot;Summarize the article&quot;</span>,</span><br><span class="line">    <span class="string">&quot;rubric&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Accuracy&quot;</span>: <span class="string">&quot;Factually correct, no hallucinations&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Completeness&quot;</span>: <span class="string">&quot;Covers key points&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Conciseness&quot;</span>: <span class="string">&quot;No unnecessary details&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Fluency&quot;</span>: <span class="string">&quot;Clear, grammatical prose&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">score, reasoning = llm_as_judge(</span><br><span class="line">    prediction=<span class="string">&quot;The article discusses climate change impacts...&quot;</span>,</span><br><span class="line">    ground_truth=<span class="string">&quot;Summary should cover: causes, impacts, solutions&quot;</span>,</span><br><span class="line">    criteria=criteria</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="systematic-ab-testing">Systematic A/B Testing</h3>
<p>Compare prompt variants rigorously:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PromptExperiment</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Run A/B tests on prompt variants.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, test_set, metrics</span>):</span><br><span class="line">        self.test_set = test_set  <span class="comment"># List of (input, ground_truth)</span></span><br><span class="line">        self.metrics = metrics  <span class="comment"># List of metric functions</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_prompt</span>(<span class="params">self, prompt_fn</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Evaluate a prompt on test set.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            prompt_fn: Function that takes input and returns prompt</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Dict of metric name -&gt; score</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        results = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> metric <span class="keyword">in</span> self.metrics:</span><br><span class="line">            scores = []</span><br><span class="line">            <span class="keyword">for</span> input_data, ground_truth <span class="keyword">in</span> self.test_set:</span><br><span class="line">                prompt = prompt_fn(input_data)</span><br><span class="line">                prediction = llm_call(prompt, temperature=<span class="number">0</span>)</span><br><span class="line">                score = metric(prediction, ground_truth)</span><br><span class="line">                scores.append(score)</span><br><span class="line">            results[metric.__name__] = <span class="built_in">sum</span>(scores) / <span class="built_in">len</span>(scores)</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compare</span>(<span class="params">self, prompts_dict</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compare multiple prompt variants.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            prompts_dict: &#123;name: prompt_fn&#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            DataFrame with results</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">        </span><br><span class="line">        results = []</span><br><span class="line">        <span class="keyword">for</span> name, prompt_fn <span class="keyword">in</span> prompts_dict.items():</span><br><span class="line">            scores = self.evaluate_prompt(prompt_fn)</span><br><span class="line">            scores[<span class="string">&#x27;prompt_name&#x27;</span>] = name</span><br><span class="line">            results.append(scores)</span><br><span class="line">        </span><br><span class="line">        df = pd.DataFrame(results)</span><br><span class="line">        <span class="keyword">return</span> df.sort_values(by=df.columns[<span class="number">0</span>], ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">test_set = [</span><br><span class="line">    (<span class="string">&quot;Classify: This movie was great!&quot;</span>, <span class="string">&quot;positive&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Classify: Terrible experience&quot;</span>, <span class="string">&quot;negative&quot;</span>),</span><br><span class="line">    <span class="comment"># ... more examples</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">metrics = [exact_match, semantic_similarity]</span><br><span class="line"></span><br><span class="line">experiment = PromptExperiment(test_set, metrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define variants</span></span><br><span class="line">prompts = &#123;</span><br><span class="line">    <span class="string">&quot;baseline&quot;</span>: <span class="keyword">lambda</span> x: <span class="string">f&quot;Classify sentiment: <span class="subst">&#123;x&#125;</span>\nSentiment:&quot;</span>,</span><br><span class="line">    <span class="string">&quot;with_instructions&quot;</span>: <span class="keyword">lambda</span> x: <span class="string">f&quot;Classify as positive/negative/neutral.\nText: <span class="subst">&#123;x&#125;</span>\nSentiment:&quot;</span>,</span><br><span class="line">    <span class="string">&quot;few_shot&quot;</span>: <span class="keyword">lambda</span> x: few_shot_template(x),</span><br><span class="line">    <span class="string">&quot;cot&quot;</span>: <span class="keyword">lambda</span> x: <span class="string">f&quot;Classify sentiment step by step: <span class="subst">&#123;x&#125;</span>\nThinking:&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">results = experiment.compare(prompts)</span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="comment">#        exact_match  semantic_similarity    prompt_name</span></span><br><span class="line"><span class="comment"># 2         0.87            0.94             few_shot</span></span><br><span class="line"><span class="comment"># 3         0.84            0.91             cot</span></span><br><span class="line"><span class="comment"># 1         0.79            0.89             with_instructions</span></span><br><span class="line"><span class="comment"># 0         0.73            0.85             baseline</span></span><br></pre></td></tr></table></figure>
<h3 id="debugging-failed-prompts">Debugging Failed Prompts</h3>
<p>When prompts fail, diagnose systematically:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PromptDebugger</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Diagnose why a prompt fails.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">debug</span>(<span class="params">self, prompt, expected, actual</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Run diagnostic checks and suggest fixes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        issues = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check 1: Is the prompt clear?</span></span><br><span class="line">        <span class="keyword">if</span> self.check_ambiguity(prompt):</span><br><span class="line">            issues.append(&#123;</span><br><span class="line">                <span class="string">&quot;issue&quot;</span>: <span class="string">&quot;Ambiguous instructions&quot;</span>,</span><br><span class="line">                <span class="string">&quot;evidence&quot;</span>: self.find_ambiguous_phrases(prompt),</span><br><span class="line">                <span class="string">&quot;fix&quot;</span>: <span class="string">&quot;Add specific constraints and examples&quot;</span></span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check 2: Are there contradictions?</span></span><br><span class="line">        <span class="keyword">if</span> self.check_contradictions(prompt):</span><br><span class="line">            issues.append(&#123;</span><br><span class="line">                <span class="string">&quot;issue&quot;</span>: <span class="string">&quot;Contradictory requirements&quot;</span>,</span><br><span class="line">                <span class="string">&quot;evidence&quot;</span>: self.find_contradictions(prompt),</span><br><span class="line">                <span class="string">&quot;fix&quot;</span>: <span class="string">&quot;Resolve conflicting instructions&quot;</span></span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check 3: Is context sufficient?</span></span><br><span class="line">        <span class="keyword">if</span> self.check_missing_context(prompt, expected):</span><br><span class="line">            issues.append(&#123;</span><br><span class="line">                <span class="string">&quot;issue&quot;</span>: <span class="string">&quot;Insufficient context&quot;</span>,</span><br><span class="line">                <span class="string">&quot;evidence&quot;</span>: <span class="string">&quot;Expected output requires information not in prompt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;fix&quot;</span>: <span class="string">&quot;Provide additional background or examples&quot;</span></span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check 4: Format issues?</span></span><br><span class="line">        <span class="keyword">if</span> self.check_format_mismatch(expected, actual):</span><br><span class="line">            issues.append(&#123;</span><br><span class="line">                <span class="string">&quot;issue&quot;</span>: <span class="string">&quot;Output format mismatch&quot;</span>,</span><br><span class="line">                <span class="string">&quot;evidence&quot;</span>: <span class="string">f&quot;Expected format: <span class="subst">&#123;self.infer_format(expected)&#125;</span>, Got: <span class="subst">&#123;self.infer_format(actual)&#125;</span>&quot;</span>,</span><br><span class="line">                <span class="string">&quot;fix&quot;</span>: <span class="string">&quot;Explicitly specify output format with examples&quot;</span></span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check 5: Is it too complex?</span></span><br><span class="line">        <span class="keyword">if</span> self.check_complexity(prompt):</span><br><span class="line">            issues.append(&#123;</span><br><span class="line">                <span class="string">&quot;issue&quot;</span>: <span class="string">&quot;Task too complex for single prompt&quot;</span>,</span><br><span class="line">                <span class="string">&quot;evidence&quot;</span>: <span class="string">f&quot;Prompt asks for <span class="subst">&#123;self.count_subtasks(prompt)&#125;</span> subtasks&quot;</span>,</span><br><span class="line">                <span class="string">&quot;fix&quot;</span>: <span class="string">&quot;Decompose into multiple steps&quot;</span></span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.generate_report(issues)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_ambiguity</span>(<span class="params">self, prompt</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Detect vague language.&quot;&quot;&quot;</span></span><br><span class="line">        ambiguous_phrases = [</span><br><span class="line">            <span class="string">&quot;relevant&quot;</span>, <span class="string">&quot;appropriate&quot;</span>, <span class="string">&quot;good&quot;</span>, <span class="string">&quot;bad&quot;</span>, </span><br><span class="line">            <span class="string">&quot;some&quot;</span>, <span class="string">&quot;few&quot;</span>, <span class="string">&quot;many&quot;</span>, <span class="string">&quot;stuff&quot;</span>, <span class="string">&quot;things&quot;</span></span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">any</span>(phrase <span class="keyword">in</span> prompt.lower() <span class="keyword">for</span> phrase <span class="keyword">in</span> ambiguous_phrases)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_contradictions</span>(<span class="params">self, prompt</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Check for conflicting instructions.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Simple heuristic: look for &quot;but&quot;, &quot;however&quot;, &quot;although&quot;</span></span><br><span class="line">        contradiction_markers = [<span class="string">&quot;but&quot;</span>, <span class="string">&quot;however&quot;</span>, <span class="string">&quot;although&quot;</span>, <span class="string">&quot;except&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(marker <span class="keyword">in</span> prompt.lower() <span class="keyword">for</span> marker <span class="keyword">in</span> contradiction_markers) &gt;= <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_format_mismatch</span>(<span class="params">self, expected, actual</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Check if output format matches expectation.&quot;&quot;&quot;</span></span><br><span class="line">        expected_fmt = self.infer_format(expected)</span><br><span class="line">        actual_fmt = self.infer_format(actual)</span><br><span class="line">        <span class="keyword">return</span> expected_fmt != actual_fmt</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">infer_format</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Infer output format (json, list, prose, etc).&quot;&quot;&quot;</span></span><br><span class="line">        text = text.strip()</span><br><span class="line">        <span class="keyword">if</span> text.startswith(<span class="string">&#x27;&#123;&#x27;</span>) <span class="keyword">and</span> text.endswith(<span class="string">&#x27;&#125;&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;json_object&quot;</span></span><br><span class="line">        <span class="keyword">if</span> text.startswith(<span class="string">&#x27;[&#x27;</span>) <span class="keyword">and</span> text.endswith(<span class="string">&#x27;]&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;json_array&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;\n-&#x27;</span> <span class="keyword">in</span> text <span class="keyword">or</span> <span class="string">&#x27;\n*&#x27;</span> <span class="keyword">in</span> text <span class="keyword">or</span> <span class="string">&#x27;\n1.&#x27;</span> <span class="keyword">in</span> text:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;list&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;prose&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_subtasks</span>(<span class="params">self, prompt</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Estimate number of subtasks in prompt.&quot;&quot;&quot;</span></span><br><span class="line">        task_markers = [</span><br><span class="line">            <span class="string">&quot;first&quot;</span>, <span class="string">&quot;then&quot;</span>, <span class="string">&quot;next&quot;</span>, <span class="string">&quot;finally&quot;</span>, <span class="string">&quot;also&quot;</span>, <span class="string">&quot;additionally&quot;</span>,</span><br><span class="line">            <span class="string">&quot;1.&quot;</span>, <span class="string">&quot;2.&quot;</span>, <span class="string">&quot;3.&quot;</span>, <span class="string">&quot;step 1&quot;</span>, <span class="string">&quot;step 2&quot;</span></span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(marker <span class="keyword">in</span> prompt.lower() <span class="keyword">for</span> marker <span class="keyword">in</span> task_markers)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_report</span>(<span class="params">self, issues</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Format diagnosis report.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> issues:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;No obvious issues detected. Try:\n- Adding more examples\n- Adjusting temperature\n- Using a different model&quot;</span></span><br><span class="line">        </span><br><span class="line">        report = <span class="string">&quot;Diagnosis:\n\n&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i, issue <span class="keyword">in</span> <span class="built_in">enumerate</span>(issues, <span class="number">1</span>):</span><br><span class="line">            report += <span class="string">f&quot;<span class="subst">&#123;i&#125;</span>. <span class="subst">&#123;issue[<span class="string">&#x27;issue&#x27;</span>]&#125;</span>\n&quot;</span></span><br><span class="line">            report += <span class="string">f&quot;   Evidence: <span class="subst">&#123;issue[<span class="string">&#x27;evidence&#x27;</span>]&#125;</span>\n&quot;</span></span><br><span class="line">            report += <span class="string">f&quot;   Fix: <span class="subst">&#123;issue[<span class="string">&#x27;fix&#x27;</span>]&#125;</span>\n\n&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage</span></span><br><span class="line">debugger = PromptDebugger()</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;Classify the sentiment of this review.&quot;</span></span><br><span class="line">expected = <span class="string">&quot;positive&quot;</span></span><br><span class="line">actual = <span class="string">&quot;The review expresses a generally favorable opinion with some minor criticisms...&quot;</span></span><br><span class="line"></span><br><span class="line">diagnosis = debugger.debug(prompt, expected, actual)</span><br><span class="line"><span class="built_in">print</span>(diagnosis)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># Diagnosis:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 1. Ambiguous instructions</span></span><br><span class="line"><span class="comment">#    Evidence: [&#x27;relevant&#x27;, &#x27;good&#x27;]</span></span><br><span class="line"><span class="comment">#    Fix: Add specific constraints and examples</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 2. Output format mismatch</span></span><br><span class="line"><span class="comment">#    Evidence: Expected format: prose, Got: prose</span></span><br><span class="line"><span class="comment">#    Fix: Explicitly specify output format with examples</span></span><br></pre></td></tr></table></figure>
<h3 id="error-analysis">Error Analysis</h3>
<p>Systematically analyze failure modes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">analyze_errors</span>(<span class="params">predictions, ground_truths, inputs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Categorize and analyze prediction errors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    errors = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> pred, truth, inp <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, ground_truths, inputs):</span><br><span class="line">        <span class="keyword">if</span> normalize(pred) != normalize(truth):</span><br><span class="line">            error = &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: inp,</span><br><span class="line">                <span class="string">&quot;prediction&quot;</span>: pred,</span><br><span class="line">                <span class="string">&quot;ground_truth&quot;</span>: truth,</span><br><span class="line">                <span class="string">&quot;error_type&quot;</span>: categorize_error(pred, truth),</span><br><span class="line">                <span class="string">&quot;severity&quot;</span>: compute_severity(pred, truth)</span><br><span class="line">            &#125;</span><br><span class="line">            errors.append(error)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Group by error type</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">    by_type = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    <span class="keyword">for</span> error <span class="keyword">in</span> errors:</span><br><span class="line">        by_type[error[<span class="string">&quot;error_type&quot;</span>]].append(error)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate report</span></span><br><span class="line">    report = <span class="string">f&quot;Total errors: <span class="subst">&#123;<span class="built_in">len</span>(errors)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(predictions)&#125;</span> (<span class="subst">&#123;<span class="number">100</span>*<span class="built_in">len</span>(errors)/<span class="built_in">len</span>(predictions):<span class="number">.1</span>f&#125;</span>%)\n\n&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> error_type, instances <span class="keyword">in</span> <span class="built_in">sorted</span>(by_type.items(), key=<span class="keyword">lambda</span> x: -<span class="built_in">len</span>(x[<span class="number">1</span>])):</span><br><span class="line">        report += <span class="string">f&quot;<span class="subst">&#123;error_type&#125;</span>: <span class="subst">&#123;<span class="built_in">len</span>(instances)&#125;</span> instances\n&quot;</span></span><br><span class="line">        report += <span class="string">&quot;Examples:\n&quot;</span></span><br><span class="line">        <span class="keyword">for</span> instance <span class="keyword">in</span> instances[:<span class="number">3</span>]:  <span class="comment"># Show up to 3 examples</span></span><br><span class="line">            report += <span class="string">f&quot;  Input: <span class="subst">&#123;instance[<span class="string">&#x27;input&#x27;</span>][:<span class="number">50</span>]&#125;</span>...\n&quot;</span></span><br><span class="line">            report += <span class="string">f&quot;  Predicted: <span class="subst">&#123;instance[<span class="string">&#x27;prediction&#x27;</span>]&#125;</span>\n&quot;</span></span><br><span class="line">            report += <span class="string">f&quot;  Expected: <span class="subst">&#123;instance[<span class="string">&#x27;ground_truth&#x27;</span>]&#125;</span>\n\n&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> report</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">categorize_error</span>(<span class="params">prediction, ground_truth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Classify type of error.&quot;&quot;&quot;</span></span><br><span class="line">    pred_norm = normalize(prediction)</span><br><span class="line">    truth_norm = normalize(ground_truth)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pred_norm) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;empty_output&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> pred_norm <span class="keyword">in</span> truth_norm <span class="keyword">or</span> truth_norm <span class="keyword">in</span> pred_norm:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;partial_match&quot;</span></span><br><span class="line">    </span><br><span class="line">    pred_words = <span class="built_in">set</span>(pred_norm.split())</span><br><span class="line">    truth_words = <span class="built_in">set</span>(truth_norm.split())</span><br><span class="line">    overlap = <span class="built_in">len</span>(pred_words &amp; truth_words) / <span class="built_in">max</span>(<span class="built_in">len</span>(pred_words), <span class="built_in">len</span>(truth_words))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> overlap &gt; <span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;semantic_error&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> overlap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;partial_hallucination&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;complete_hallucination&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_severity</span>(<span class="params">prediction, ground_truth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Score error severity (0=minor, 1=critical).&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Use semantic similarity as proxy</span></span><br><span class="line">    similarity = semantic_similarity(prediction, ground_truth)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - similarity</span><br></pre></td></tr></table></figure>
<h2 id="common-pitfalls-solutions">Common Pitfalls &amp; Solutions</h2>
<p>Learn from common mistakes:</p>
<h3 id="pitfall-1-vague-instructions">Pitfall 1: Vague Instructions</h3>
<p><strong>Problem:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;Make this better: &#123;text&#125;&quot;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Why it fails:</strong> "Better" is subjective. The model
doesn't know what dimensions to optimize.</p>
<p><strong>Solution:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Improve this text for clarity and conciseness:</span></span><br><span class="line"><span class="string">- Remove redundant words</span></span><br><span class="line"><span class="string">- Use active voice</span></span><br><span class="line"><span class="string">- Break up long sentences</span></span><br><span class="line"><span class="string">- Preserve all key information</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original: <span class="subst">&#123;text&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Improved version:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="pitfall-2-assuming-knowledge">Pitfall 2: Assuming Knowledge</h3>
<p><strong>Problem:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;What&#x27;s the bug in this code?&quot;</span>  <span class="comment"># No code provided</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Why it fails:</strong> Model needs context you forgot to
include.</p>
<p><strong>Solution:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Find bugs in this Python code:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;code&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Check for:</span></span><br><span class="line"><span class="string">- Syntax errors</span></span><br><span class="line"><span class="string">- Logic errors</span></span><br><span class="line"><span class="string">- Edge cases not handled</span></span><br><span class="line"><span class="string">- Potential runtime errors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Bugs found:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="pitfall-3-overly-long-prompts">Pitfall 3: Overly Long
Prompts</h3>
<p><strong>Problem:</strong> 15,000-word prompt with every possible
instruction.</p>
<p><strong>Why it fails:</strong> Models struggle to attend to all
information (the "lost in the middle" problem). Also expensive.</p>
<p><strong>Solution:</strong> Decompose or use hierarchical prompting:
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 1: Extract key info</span></span><br><span class="line">key_info = extract_key_information(long_document)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Focused task with only relevant info</span></span><br><span class="line">result = perform_task(key_info)</span><br></pre></td></tr></table></figure></p>
<h3 id="pitfall-4-ignoring-output-format">Pitfall 4: Ignoring Output
Format</h3>
<p><strong>Problem:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;Extract the dates.&quot;</span></span><br><span class="line"><span class="comment"># Gets: &quot;The dates are March 3 and April 15.&quot;</span></span><br><span class="line"><span class="comment"># Want: [&quot;2024-03-03&quot;, &quot;2024-04-15&quot;]</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Solution:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Extract all dates in ISO format (YYYY-MM-DD).</span></span><br><span class="line"><span class="string">Output as JSON array of strings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Text: &#123;text&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Output (JSON only):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="pitfall-5-no-validation">Pitfall 5: No Validation</h3>
<p><strong>Problem:</strong> Accepting model output without
checking.</p>
<p><strong>Solution:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">validated_generation</span>(<span class="params">prompt, validator, max_retries=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate with validation loop.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> attempt <span class="keyword">in</span> <span class="built_in">range</span>(max_retries):</span><br><span class="line">        output = llm_call(prompt)</span><br><span class="line">        </span><br><span class="line">        is_valid, error = validator(output)</span><br><span class="line">        <span class="keyword">if</span> is_valid:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retry with feedback</span></span><br><span class="line">        prompt += <span class="string">f&quot;\n\nError: <span class="subst">&#123;error&#125;</span>\nTry again:&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Failed validation after max retries&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example validator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validate_json</span>(<span class="params">output</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        json.loads(output)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">except</span> json.JSONDecodeError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>, <span class="string">f&quot;Invalid JSON: <span class="subst">&#123;e&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<h3 id="q1-should-i-use-higher-or-lower-temperature">Q1: Should I use
higher or lower temperature?</h3>
<p><strong>Answer:</strong> It depends on your task:</p>
<ul>
<li><p><strong>Temperature = 0 (or very low):</strong> Use for tasks
requiring consistency and correctness (classification, extraction, math,
code generation). The model always picks the most likely token, giving
deterministic outputs.</p></li>
<li><p><strong>Temperature = 0.7-0.8:</strong> Good for creative tasks
where you want diversity but still coherent outputs (writing,
brainstorming, marketing copy).</p></li>
<li><p><strong>Temperature = 1.0+:</strong> Maximum randomness. Rarely
useful except for exploration or artistic generation.</p></li>
</ul>
<p><strong>Rule of thumb:</strong> Start with 0 for structured tasks,
0.7 for creative ones.</p>
<h3 id="q2-how-many-examples-should-i-include-in-few-shot-prompts">Q2:
How many examples should I include in few-shot prompts?</h3>
<p><strong>Answer:</strong> Research shows diminishing returns:</p>
<ul>
<li><strong>2-3 examples:</strong> Often sufficient for simple
tasks</li>
<li><strong>5-7 examples:</strong> Sweet spot for most tasks</li>
<li><strong>10+ examples:</strong> Only helps if examples are diverse
and cover edge cases</li>
<li><strong>50+ examples:</strong> Approaches few-shot learning ceiling;
consider fine-tuning instead</li>
</ul>
<p>Quality matters more than quantity. Choose diverse, representative
examples.</p>
<h3 id="q3-when-should-i-fine-tune-instead-of-prompt-engineering">Q3:
When should I fine-tune instead of prompt engineering?</h3>
<p><strong>Answer:</strong> Fine-tune when:</p>
<ol type="1">
<li><strong>You have &gt;1,000 high-quality examples</strong>
(preferably 10,000+)</li>
<li><strong>Task is specialized</strong> with domain jargon or
style</li>
<li><strong>Latency/cost matters</strong> and you make millions of
requests</li>
<li><strong>Prompt engineering plateaus</strong> below acceptable
accuracy</li>
</ol>
<p>Stick with prompting when: 1. Task changes frequently (prompts are
easier to iterate) 2. You lack training data or labeling budget 3. You
need interpretability (prompts are transparent) 4. You want to leverage
the latest models without retraining</p>
<h3 id="q4-how-do-i-prevent-hallucinations">Q4: How do I prevent
hallucinations?</h3>
<p><strong>Strategies:</strong></p>
<ol type="1">
<li><p><strong>Ground with context:</strong> Provide relevant facts in
the prompt <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">f&quot;Based on this context: <span class="subst">&#123;facts&#125;</span>\n\nAnswer: <span class="subst">&#123;question&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Explicit instructions:</strong> Tell the model to say "I
don't know" when uncertain <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;If the answer isn&#x27;t in the context, respond: &#x27;Information not available&#x27;&quot;</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Request citations:</strong> Ask the model to quote
sources <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Quote the exact sentence from the text that supports your answer&quot;</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Lower temperature:</strong> Reduces random guessing
(temperature=0)</p></li>
<li><p><strong>Validation loop:</strong> Check outputs against ground
truth <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> can_verify(output, source_text):</span><br><span class="line">    output = <span class="string">&quot;Cannot verify answer&quot;</span></span><br></pre></td></tr></table></figure></p></li>
</ol>
<h3 id="q5-whats-the-best-way-to-handle-long-documents">Q5: What's the
best way to handle long documents?</h3>
<p><strong>Approaches:</strong></p>
<ol type="1">
<li><p><strong>Chunking + Map-Reduce:</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Process each chunk</span></span><br><span class="line">summaries = [summarize(chunk) <span class="keyword">for</span> chunk <span class="keyword">in</span> chunks]</span><br><span class="line"><span class="comment"># Combine results</span></span><br><span class="line">final = synthesize(summaries)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Retrieval-Augmented Generation (RAG):</strong>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find relevant chunks</span></span><br><span class="line">relevant = semantic_search(query, document_chunks, k=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Use only relevant context</span></span><br><span class="line">answer = llm_call(<span class="string">f&quot;Context: <span class="subst">&#123;relevant&#125;</span>\n\nQuestion: <span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Hierarchical summarization:</strong>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layer 1: Summarize paragraphs</span></span><br><span class="line"><span class="comment"># Layer 2: Summarize summaries</span></span><br><span class="line"><span class="comment"># Layer 3: Final synthesis</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Use models with large context windows:</strong></p>
<ul>
<li>Claude 3: 200K tokens</li>
<li>GPT-4 Turbo: 128K tokens</li>
<li>Gemini 1.5: 1M tokens</li>
</ul></li>
</ol>
<h3 id="q6-how-do-i-make-prompts-work-across-different-models">Q6: How
do I make prompts work across different models?</h3>
<p><strong>Answer:</strong> Some techniques transfer well, others
don't:</p>
<p><strong>Universal techniques:</strong> - Clear instructions -
Few-shot examples - Output format specification - Chain-of-thought
reasoning</p>
<p><strong>Model-specific:</strong> - Exact phrasing sensitivity varies
- Some models need explicit format instructions (XML, JSON markers) -
Function calling is API-specific (OpenAI vs. others)</p>
<p><strong>Best practice:</strong> Test on your target model. Don't
assume prompts transfer perfectly.</p>
<h3 id="q7-can-i-automate-prompt-optimization">Q7: Can I automate prompt
optimization?</h3>
<p><strong>Yes, several approaches:</strong></p>
<ol type="1">
<li><strong>APE (Automatic Prompt Engineering):</strong> Generate and
test candidate prompts</li>
<li><strong>DSPy:</strong> Framework for programmatic prompt
optimization</li>
<li><strong>Genetic algorithms:</strong> Evolve prompts through mutation
and selection</li>
<li><strong>Reinforcement learning:</strong> Optimize prompts using
reward signals</li>
</ol>
<p><strong>Practical recommendation:</strong> Start with manual
engineering to understand the task, then automate optimization for
production.</p>
<h3 id="q8-how-do-i-evaluate-prompt-quality-without-ground-truth">Q8:
How do I evaluate prompt quality without ground truth?</h3>
<p><strong>Methods:</strong></p>
<ol type="1">
<li><p><strong>LLM-as-Judge:</strong> Use a stronger model to evaluate
outputs <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score = judge_llm(<span class="string">f&quot;Rate this output (0-10): <span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Consistency checking:</strong> Generate multiple outputs,
check agreement <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = [llm_call(prompt) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]</span><br><span class="line">consistency = compute_agreement(outputs)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Human eval on samples:</strong> Evaluate 100-200 examples
manually <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample = random.sample(outputs, <span class="number">200</span>)</span><br><span class="line">human_scores = get_human_ratings(sample)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Proxy metrics:</strong> Measure related properties</p>
<ul>
<li>Length (for summaries)</li>
<li>Readability scores</li>
<li>Semantic similarity to reference texts</li>
</ul></li>
</ol>
<h3 id="q9-whats-the-roi-of-investing-in-prompt-engineering">Q9: What's
the ROI of investing in prompt engineering?</h3>
<p><strong>Quantifiable benefits:</strong></p>
<ul>
<li><strong>Accuracy improvements:</strong> 20-50% gains on complex
tasks (see CoT benchmarks)</li>
<li><strong>Cost reduction:</strong> Better prompts can reduce API calls
through higher success rates</li>
<li><strong>Latency:</strong> Shorter, optimized prompts generate
faster</li>
<li><strong>Maintenance:</strong> Good prompts need less frequent
updates</li>
</ul>
<p><strong>Example calculation:</strong> - Baseline: 70% accuracy
requires 1.4× API calls for retries - Optimized: 90% accuracy reduces to
1.1× API calls - <strong>Savings: 22% fewer API calls</strong></p>
<p>At scale (1M requests/month), this saves thousands of dollars.</p>
<h3 id="q10-how-do-i-debug-a-prompt-that-sometimes-works-and-sometimes-fails">Q10:
How do I debug a prompt that sometimes works and sometimes fails?</h3>
<p><strong>Diagnosis steps:</strong></p>
<ol type="1">
<li><p><strong>Check for non-determinism:</strong> Set temperature=0 to
eliminate randomness <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = llm_call(prompt, temperature=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Identify failure patterns:</strong> Analyze errors for
commonalities <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">failures = [x <span class="keyword">for</span> x <span class="keyword">in</span> test_set <span class="keyword">if</span> <span class="keyword">not</span> is_correct(x)]</span><br><span class="line">patterns = cluster_similar_inputs(failures)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Add explicit edge case handling:</strong> Update prompt
with failure examples <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prompt += <span class="string">&quot;\n\nEdge case examples:\n&#123;edge_cases&#125;&quot;</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Use self-consistency:</strong> Vote across multiple
generations <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = [llm_call(prompt) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]</span><br><span class="line">final = majority_vote(outputs)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Incremental refinement:</strong> Test each change on
known failure cases</p></li>
</ol>
<h3 id="q11-should-i-use-xml-json-or-plain-text-for-structured-prompts">Q11:
Should I use XML, JSON, or plain text for structured prompts?</h3>
<p><strong>Comparison:</strong></p>
<p><strong>XML:</strong> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">instruction</span>&gt;</span>Classify sentiment<span class="tag">&lt;/<span class="name">instruction</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">example</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">text</span>&gt;</span>Great product<span class="tag">&lt;/<span class="name">text</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">label</span>&gt;</span>positive<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">example</span>&gt;</span></span><br></pre></td></tr></table></figure> - Pro: Clear structure, Claude
prefers XML - Con: Verbose</p>
<p><strong>JSON:</strong> <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Classify sentiment&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;example&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Great product&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;label&quot;</span><span class="punctuation">:</span> <span class="string">&quot;positive&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure> - Pro: Compact, easy to parse
programmatically - Con: Models sometimes generate invalid JSON</p>
<p><strong>Plain text:</strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Classify sentiment:</span><br><span class="line">Example: &quot;Great product&quot; → positive</span><br></pre></td></tr></table></figure> - Pro: Natural, easy to
read/write - Con: Less structure for complex data</p>
<p><strong>Recommendation:</strong> Use plain text for simple prompts,
JSON for structured I/O, XML for complex multi-part prompts (especially
with Claude).</p>
<h3 id="q12-how-do-i-balance-cost-vs.-quality">Q12: How do I balance
cost vs. quality?</h3>
<p><strong>Strategies:</strong></p>
<ol type="1">
<li><p><strong>Tiered approach:</strong> Use cheaper models for simple
tasks, expensive ones for hard tasks <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> is_simple(task):</span><br><span class="line">    <span class="keyword">return</span> gpt_3_5(task)  <span class="comment"># Cheaper</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> gpt_4(task)    <span class="comment"># More expensive but accurate</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Caching:</strong> Reuse responses for repeated inputs
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@lru_cache(<span class="params">maxsize=<span class="number">10000</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cached_llm_call</span>(<span class="params">prompt</span>):</span><br><span class="line">    <span class="keyword">return</span> llm_call(prompt)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Prompt compression:</strong> Use LLMLingua to reduce
token count <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">compressed = compress_prompt(long_prompt, target_ratio=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Cascade:</strong> Try simple prompts first, escalate if
they fail <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = zero_shot(task)</span><br><span class="line"><span class="keyword">if</span> confidence(result) &lt; <span class="number">0.7</span>:</span><br><span class="line">    result = few_shot(task)  <span class="comment"># More expensive</span></span><br></pre></td></tr></table></figure></p></li>
<li><p><strong>Batch processing:</strong> Process multiple requests
together to amortize costs</p></li>
</ol>
<h3 id="q13-whats-the-difference-between-cot-tot-and-got">Q13: What's
the difference between CoT, ToT, and GoT?</h3>
<p><strong>Quick comparison:</strong></p>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 31%">
</colgroup>
<thead>
<tr>
<th>Technique</th>
<th>Structure</th>
<th>When to use</th>
<th>Cost multiplier</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CoT</strong></td>
<td>Linear chain</td>
<td>Multi-step reasoning, math, logic</td>
<td>1-2× (longer outputs)</td>
</tr>
<tr>
<td><strong>ToT</strong></td>
<td>Tree search</td>
<td>Problems with multiple solution paths</td>
<td>5-50× (explores branches)</td>
</tr>
<tr>
<td><strong>GoT</strong></td>
<td>Arbitrary graph</td>
<td>Parallel processing, merging insights</td>
<td>Varies (more efficient than ToT)</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong> "Plan a trip to Europe"</p>
<ul>
<li><strong>CoT:</strong> Day 1 → Day 2 → Day 3... (sequential)</li>
<li><strong>ToT:</strong> Explore multiple itineraries, backtrack if
hotel unavailable</li>
<li><strong>GoT:</strong> Research destinations in parallel, merge into
cohesive plan</li>
</ul>
<h3 id="q14-how-do-i-handle-rate-limits-and-api-errors">Q14: How do I
handle rate limits and API errors?</h3>
<p><strong>Robust implementation:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retry_with_exponential_backoff</span>(<span class="params"></span></span><br><span class="line"><span class="params">    max_retries=<span class="number">5</span>,</span></span><br><span class="line"><span class="params">    initial_delay=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    backoff_factor=<span class="number">2</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Decorator to retry with exponential backoff.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">        @wraps(<span class="params">func</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            delay = initial_delay</span><br><span class="line">            <span class="keyword">for</span> attempt <span class="keyword">in</span> <span class="built_in">range</span>(max_retries):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">                <span class="keyword">except</span> RateLimitError:</span><br><span class="line">                    <span class="keyword">if</span> attempt == max_retries - <span class="number">1</span>:</span><br><span class="line">                        <span class="keyword">raise</span></span><br><span class="line">                    time.sleep(delay)</span><br><span class="line">                    delay *= backoff_factor</span><br><span class="line">                <span class="keyword">except</span> APIError <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&quot;server_error&quot;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                        <span class="comment"># Retry on server errors</span></span><br><span class="line">                        time.sleep(delay)</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># Don&#x27;t retry on client errors</span></span><br><span class="line">                        <span class="keyword">raise</span></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@retry_with_exponential_backoff()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">llm_call_robust</span>(<span class="params">prompt, **kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> llm_call(prompt, **kwargs)</span><br></pre></td></tr></table></figure>
<h3 id="q15-whats-the-future-of-prompt-engineering">Q15: What's the
future of prompt engineering?</h3>
<p><strong>Emerging trends:</strong></p>
<ol type="1">
<li><strong>Multimodal prompting:</strong> Text + images + audio in one
prompt</li>
<li><strong>Automated optimization:</strong> Tools that auto-tune
prompts (DSPy, APE)</li>
<li><strong>Prompt compression:</strong> Fit more context into token
limits</li>
<li><strong>Meta-prompting:</strong> Prompts that generate prompts</li>
<li><strong>Embodied agents:</strong> Prompts controlling robots and
virtual agents</li>
<li><strong>Fine-tuning obsolescence?</strong> As models improve, less
need for complex prompts</li>
</ol>
<p><strong>Prediction:</strong> Prompt engineering will remain valuable
but become more automated. Focus will shift from manual crafting to: -
<strong>Designing prompt optimization objectives</strong> -
<strong>Building evaluation frameworks</strong> - <strong>Integrating
prompts into larger systems</strong></p>
<p>The skill won't disappear—it will evolve toward higher-level
orchestration.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Prompt engineering transforms how we interact with AI systems. What
began as trial-and-error has matured into a discipline backed by
rigorous research and practical frameworks.</p>
<p>The fundamentals—clear instructions, well-chosen examples, structured
outputs—apply universally. Advanced techniques like chain-of-thought
reasoning and tree search unlock capabilities that seemed impossible
with naive prompting. Optimization methods like automatic prompt
engineering and DSPy scale these practices to production systems.</p>
<p>But techniques alone aren't enough. Effective prompt engineering
requires:</p>
<ol type="1">
<li><strong>Empiricism:</strong> Test everything. What works for one
model or task may fail for another.</li>
<li><strong>Iteration:</strong> Your first prompt will rarely be your
best. Expect to refine based on failures.</li>
<li><strong>Evaluation:</strong> Measure rigorously. Without metrics,
you're flying blind.</li>
<li><strong>Contextual thinking:</strong> Understand your model's
strengths, your task's requirements, and the trade-offs between cost,
latency, and quality.</li>
</ol>
<p>As models grow more powerful, the nature of prompt engineering
evolves. Simple tasks that once required careful prompting now work
zero-shot. But complex reasoning, specialized domains, and production
constraints ensure that prompt engineering remains essential.</p>
<p>The future belongs to those who can orchestrate AI systems
effectively—not just by writing clever prompts, but by building
frameworks that optimize, validate, and scale prompt-based solutions.
Whether you're a researcher pushing the boundaries of what LLMs can do
or a practitioner building real-world applications, the principles in
this guide provide a foundation for success.</p>
<p>Start simple. Measure constantly. Iterate relentlessly. And remember:
the best prompt is the one that reliably solves your problem, not the
cleverest one.</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>Post title：Prompt Engineering Complete Guide: From Zero to Advanced Optimization</li>
        <li>Post author：Chen Kai</li>
        <li>Create time：2025-04-01 00:00:00</li>
        <li>
            Post link：https://www.chenk.top/en/prompt-engineering-complete-guide/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/en/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/Prompt-Engineering/">#Prompt Engineering</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/reaction-diffusion-systems-and-graph-neural-networks/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Reaction-Diffusion Systems and Graph Neural Networks</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/en/prefix-tuning/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Prefix-Tuning: Optimizing Continuous Prompts for Generation</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#why-prompt-engineering-matters"><span class="nav-number">1.</span> <span class="nav-text">Why Prompt Engineering
Matters</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fundamental-techniques"><span class="nav-number">2.</span> <span class="nav-text">Fundamental Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zero-shot-prompting"><span class="nav-number">2.1.</span> <span class="nav-text">Zero-Shot Prompting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#few-shot-prompting"><span class="nav-number">2.2.</span> <span class="nav-text">Few-Shot Prompting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#many-shot-prompting"><span class="nav-number">2.3.</span> <span class="nav-text">Many-Shot Prompting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#task-decomposition"><span class="nav-number">2.4.</span> <span class="nav-text">Task Decomposition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-reasoning-techniques"><span class="nav-number">3.</span> <span class="nav-text">Advanced Reasoning
Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chain-of-thought-cot-prompting"><span class="nav-number">3.1.</span> <span class="nav-text">Chain-of-Thought (CoT)
Prompting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-consistency"><span class="nav-number">3.2.</span> <span class="nav-text">Self-Consistency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tree-of-thoughts-tot"><span class="nav-number">3.3.</span> <span class="nav-text">Tree of Thoughts (ToT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#graph-of-thoughts-got"><span class="nav-number">3.4.</span> <span class="nav-text">Graph of Thoughts (GoT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#react-reasoning-acting"><span class="nav-number">3.5.</span> <span class="nav-text">ReAct (Reasoning + Acting)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization-automation"><span class="nav-number">4.</span> <span class="nav-text">Optimization &amp; Automation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#automatic-prompt-engineering-ape"><span class="nav-number">4.1.</span> <span class="nav-text">Automatic Prompt Engineering
(APE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dspy-declarative-self-improving-prompts"><span class="nav-number">4.2.</span> <span class="nav-text">DSPy: Declarative
Self-improving Prompts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#llmlingua-prompt-compression"><span class="nav-number">4.3.</span> <span class="nav-text">LLMLingua: Prompt Compression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#practical-templates-patterns"><span class="nav-number">5.</span> <span class="nav-text">Practical Templates &amp;
Patterns</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#structured-output-generation"><span class="nav-number">5.1.</span> <span class="nav-text">Structured Output Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-generation-debugging"><span class="nav-number">5.2.</span> <span class="nav-text">Code Generation &amp; Debugging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data-extraction-classification"><span class="nav-number">5.3.</span> <span class="nav-text">Data Extraction &amp;
Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#creative-writing-content-generation"><span class="nav-number">5.4.</span> <span class="nav-text">Creative Writing &amp;
Content Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-turn-conversation-management"><span class="nav-number">5.5.</span> <span class="nav-text">Multi-turn Conversation
Management</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#evaluation-debugging"><span class="nav-number">6.</span> <span class="nav-text">Evaluation &amp; Debugging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluation-metrics"><span class="nav-number">6.1.</span> <span class="nav-text">Evaluation Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#systematic-ab-testing"><span class="nav-number">6.2.</span> <span class="nav-text">Systematic A&#x2F;B Testing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#debugging-failed-prompts"><span class="nav-number">6.3.</span> <span class="nav-text">Debugging Failed Prompts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#error-analysis"><span class="nav-number">6.4.</span> <span class="nav-text">Error Analysis</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#common-pitfalls-solutions"><span class="nav-number">7.</span> <span class="nav-text">Common Pitfalls &amp; Solutions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pitfall-1-vague-instructions"><span class="nav-number">7.1.</span> <span class="nav-text">Pitfall 1: Vague Instructions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pitfall-2-assuming-knowledge"><span class="nav-number">7.2.</span> <span class="nav-text">Pitfall 2: Assuming Knowledge</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pitfall-3-overly-long-prompts"><span class="nav-number">7.3.</span> <span class="nav-text">Pitfall 3: Overly Long
Prompts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pitfall-4-ignoring-output-format"><span class="nav-number">7.4.</span> <span class="nav-text">Pitfall 4: Ignoring Output
Format</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pitfall-5-no-validation"><span class="nav-number">7.5.</span> <span class="nav-text">Pitfall 5: No Validation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#frequently-asked-questions"><span class="nav-number">8.</span> <span class="nav-text">Frequently Asked Questions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-should-i-use-higher-or-lower-temperature"><span class="nav-number">8.1.</span> <span class="nav-text">Q1: Should I use
higher or lower temperature?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-how-many-examples-should-i-include-in-few-shot-prompts"><span class="nav-number">8.2.</span> <span class="nav-text">Q2:
How many examples should I include in few-shot prompts?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-when-should-i-fine-tune-instead-of-prompt-engineering"><span class="nav-number">8.3.</span> <span class="nav-text">Q3:
When should I fine-tune instead of prompt engineering?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-how-do-i-prevent-hallucinations"><span class="nav-number">8.4.</span> <span class="nav-text">Q4: How do I prevent
hallucinations?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-whats-the-best-way-to-handle-long-documents"><span class="nav-number">8.5.</span> <span class="nav-text">Q5: What&#39;s the
best way to handle long documents?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-how-do-i-make-prompts-work-across-different-models"><span class="nav-number">8.6.</span> <span class="nav-text">Q6: How
do I make prompts work across different models?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q7-can-i-automate-prompt-optimization"><span class="nav-number">8.7.</span> <span class="nav-text">Q7: Can I automate prompt
optimization?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q8-how-do-i-evaluate-prompt-quality-without-ground-truth"><span class="nav-number">8.8.</span> <span class="nav-text">Q8:
How do I evaluate prompt quality without ground truth?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q9-whats-the-roi-of-investing-in-prompt-engineering"><span class="nav-number">8.9.</span> <span class="nav-text">Q9: What&#39;s
the ROI of investing in prompt engineering?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q10-how-do-i-debug-a-prompt-that-sometimes-works-and-sometimes-fails"><span class="nav-number">8.10.</span> <span class="nav-text">Q10:
How do I debug a prompt that sometimes works and sometimes fails?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q11-should-i-use-xml-json-or-plain-text-for-structured-prompts"><span class="nav-number">8.11.</span> <span class="nav-text">Q11:
Should I use XML, JSON, or plain text for structured prompts?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q12-how-do-i-balance-cost-vs.-quality"><span class="nav-number">8.12.</span> <span class="nav-text">Q12: How do I balance
cost vs. quality?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q13-whats-the-difference-between-cot-tot-and-got"><span class="nav-number">8.13.</span> <span class="nav-text">Q13: What&#39;s
the difference between CoT, ToT, and GoT?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q14-how-do-i-handle-rate-limits-and-api-errors"><span class="nav-number">8.14.</span> <span class="nav-text">Q14: How do I
handle rate limits and API errors?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q15-whats-the-future-of-prompt-engineering"><span class="nav-number">8.15.</span> <span class="nav-text">Q15: What&#39;s the
future of prompt engineering?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conclusion"><span class="nav-number">9.</span> <span class="nav-text">Conclusion</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
