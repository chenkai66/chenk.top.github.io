<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            NLP (12): Frontiers and Practical Applications |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">NLP (12): Frontiers and Practical Applications</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-03-06 00:00:00</span>
        <span class="mobile">2025-03-06 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/Applications/">Applications</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>3.5k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>22 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>As large language models continue to improve in capability, the NLP field is undergoing unprecedented transformation. From Agents and tool use (Function Calling, ReAct) to code generation and understanding (CodeLlama, StarCoder), to long-context modeling (LongLoRA, LongLLaMA), these cutting-edge technologies are redefining the boundaries of AI capabilities.</p>
<p>However, as model capabilities increase, challenges such as hallucination, safety, and alignment are becoming increasingly prominent. How to evaluate model performance and how to build complete NLP project deployment systems have become critical issues in practical applications. From FastAPI service deployment to Docker containerization, from monitoring systems to performance optimization, production-grade NLP applications require comprehensive consideration of technical, engineering, and business dimensions.</p>
<p>This article delves into frontier technologies in NLP, including Agents and tool use, code generation, long-context modeling, hallucination mitigation, safety and alignment, model evaluation systems, and demonstrates how to build and deploy production-grade NLP application systems through complete practical examples.</p>
<span id="more"></span>
<h2 id="Agents-and-Tool-Use"><a class="header-anchor" href="#Agents-and-Tool-Use">¶</a>Agents and Tool Use</h2>
<h3 id="Function-Calling-Enabling-LLMs-to-Call-External-Tools"><a class="header-anchor" href="#Function-Calling-Enabling-LLMs-to-Call-External-Tools">¶</a>Function Calling: Enabling LLMs to Call External Tools</h3>
<p>Function Calling allows large language models to call external functions and APIs, enabling interaction with external systems.</p>
<p><strong>Core Concept</strong>:</p>
<p>The model receives function definitions and user queries, decides whether to call functions, and generates parameters conforming to function signatures.</p>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define available functions</span></span><br><span class="line">functions = [</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_weather"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"Get weather information for a specified city"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: {</span><br><span class="line">                <span class="string">"location"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"City name, e.g., Beijing, Shanghai"</span></span><br><span class="line">                },</span><br><span class="line">                <span class="string">"unit"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"enum"</span>: [<span class="string">"celsius"</span>, <span class="string">"fahrenheit"</span>],</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"Temperature unit"</span></span><br><span class="line">                }</span><br><span class="line">            },</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"location"</span>]</span><br><span class="line">        }</span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"send_email"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"Send an email"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: {</span><br><span class="line">                <span class="string">"to"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"description"</span>: <span class="string">"Recipient email"</span>},</span><br><span class="line">                <span class="string">"subject"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"description"</span>: <span class="string">"Email subject"</span>},</span><br><span class="line">                <span class="string">"body"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"description"</span>: <span class="string">"Email body"</span>}</span><br><span class="line">            },</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"to"</span>, <span class="string">"subject"</span>, <span class="string">"body"</span>]</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weather</span>(<span class="params">location, unit=<span class="string">"celsius"</span></span>):</span><br><span class="line">    <span class="string">"""Simulate weather query function"""</span></span><br><span class="line">    <span class="comment"># In practice, call real weather API</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f"Weather in <span class="subst">{location}</span>: 25 degrees <span class="subst">{unit}</span>, sunny"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">send_email</span>(<span class="params">to, subject, body</span>):</span><br><span class="line">    <span class="string">"""Simulate email sending function"""</span></span><br><span class="line">    <span class="comment"># In practice, call email service</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f"Email sent to <span class="subst">{to}</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat_with_functions</span>(<span class="params">user_message</span>):</span><br><span class="line">    <span class="string">"""Chat using Function Calling"""</span></span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">"gpt-4"</span>,</span><br><span class="line">        messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: user_message}],</span><br><span class="line">        functions=functions,</span><br><span class="line">        function_call=<span class="string">"auto"</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    message = response.choices[<span class="number">0</span>].message</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check if function call is needed</span></span><br><span class="line">    <span class="keyword">if</span> message.function_call:</span><br><span class="line">        function_name = message.function_call.name</span><br><span class="line">        function_args = json.loads(message.function_call.arguments)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Call corresponding function</span></span><br><span class="line">        <span class="keyword">if</span> function_name == <span class="string">"get_weather"</span>:</span><br><span class="line">            result = get_weather(**function_args)</span><br><span class="line">        <span class="keyword">elif</span> function_name == <span class="string">"send_email"</span>:</span><br><span class="line">            result = send_email(**function_args)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result = <span class="string">"Unknown function"</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Return function result to model</span></span><br><span class="line">        second_response = client.chat.completions.create(</span><br><span class="line">            model=<span class="string">"gpt-4"</span>,</span><br><span class="line">            messages=[</span><br><span class="line">                {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: user_message},</span><br><span class="line">                message,</span><br><span class="line">                {</span><br><span class="line">                    <span class="string">"role"</span>: <span class="string">"function"</span>,</span><br><span class="line">                    <span class="string">"name"</span>: function_name,</span><br><span class="line">                    <span class="string">"content"</span>: result</span><br><span class="line">                }</span><br><span class="line">            ],</span><br><span class="line">            functions=functions</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> second_response.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> message.content</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">response = chat_with_functions(<span class="string">"What's the weather in Beijing today?"</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<h3 id="ReAct-Combining-Reasoning-and-Acting"><a class="header-anchor" href="#ReAct-Combining-Reasoning-and-Acting">¶</a>ReAct: Combining Reasoning and Acting</h3>
<p>The ReAct (Reasoning + Acting) framework enables models to reason and execute actions, completing tasks through iterative loops.</p>
<p><strong>ReAct Loop</strong>:</p>
<ol>
<li><strong>Thought</strong>: Analyze current state and operations to execute</li>
<li><strong>Action</strong>: Call tools or execute operations</li>
<li><strong>Observation</strong>: Get action results</li>
<li>Repeat until task is complete</li>
</ol>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> initialize_agent, Tool</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> SerpAPIWrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReActAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define tools</span></span><br><span class="line">        search = SerpAPIWrapper()</span><br><span class="line">        self.tools = [</span><br><span class="line">            Tool(</span><br><span class="line">                name=<span class="string">"Search"</span>,</span><br><span class="line">                func=search.run,</span><br><span class="line">                description=<span class="string">"Used to search for latest information, input should be a search query"</span></span><br><span class="line">            ),</span><br><span class="line">            Tool(</span><br><span class="line">                name=<span class="string">"Calculator"</span>,</span><br><span class="line">                func=self.calculator,</span><br><span class="line">                description=<span class="string">"Used to perform mathematical calculations, input should be a mathematical expression"</span></span><br><span class="line">            )</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        self.agent = initialize_agent(</span><br><span class="line">            self.tools,</span><br><span class="line">            self.llm,</span><br><span class="line">            agent=<span class="string">"react-docstore"</span>,</span><br><span class="line">            verbose=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculator</span>(<span class="params">self, expression</span>):</span><br><span class="line">        <span class="string">"""Calculator tool"""</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result = <span class="built_in">eval</span>(expression)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">str</span>(result)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Calculation error"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="string">"""Execute query"""</span></span><br><span class="line">        <span class="keyword">return</span> self.agent.run(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">agent = ReActAgent()</span><br><span class="line">result = agent.run(<span class="string">"Search for OpenAI's latest model, then calculate the result of 2024 minus 2015"</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p><strong>Custom ReAct Implementation</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomReActAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm, tools</span>):</span><br><span class="line">        self.llm = llm</span><br><span class="line">        self.tools = {tool.name: tool <span class="keyword">for</span> tool <span class="keyword">in</span> tools}</span><br><span class="line">        self.max_iterations = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="string">"""Execute ReAct loop"""</span></span><br><span class="line">        history = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iterations):</span><br><span class="line">            <span class="comment"># Build prompt</span></span><br><span class="line">            prompt = self.build_prompt(query, history)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Get model response</span></span><br><span class="line">            response = self.llm(prompt)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Parse response</span></span><br><span class="line">            thought, action, action_input = self.parse_response(response)</span><br><span class="line">            </span><br><span class="line">            history.append({</span><br><span class="line">                <span class="string">"thought"</span>: thought,</span><br><span class="line">                <span class="string">"action"</span>: action,</span><br><span class="line">                <span class="string">"action_input"</span>: action_input</span><br><span class="line">            })</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if complete</span></span><br><span class="line">            <span class="keyword">if</span> action == <span class="string">"Final Answer"</span>:</span><br><span class="line">                <span class="keyword">return</span> action_input</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Execute action</span></span><br><span class="line">            <span class="keyword">if</span> action <span class="keyword">in</span> self.tools:</span><br><span class="line">                observation = self.tools[action].run(action_input)</span><br><span class="line">                history.append({<span class="string">"observation"</span>: observation})</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                history.append({<span class="string">"observation"</span>: <span class="string">f"Unknown tool: <span class="subst">{action}</span>"</span>})</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Reached maximum iterations, task not completed"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_prompt</span>(<span class="params">self, query, history</span>):</span><br><span class="line">        <span class="string">"""Build prompt"""</span></span><br><span class="line">        prompt = <span class="string">f"Question: <span class="subst">{query}</span>\n\n"</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> history:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"thought"</span> <span class="keyword">in</span> step:</span><br><span class="line">                prompt += <span class="string">f"Thought: <span class="subst">{step[<span class="string">'thought'</span>]}</span>\n"</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"action"</span> <span class="keyword">in</span> step:</span><br><span class="line">                prompt += <span class="string">f"Action: <span class="subst">{step[<span class="string">'action'</span>]}</span>(<span class="subst">{step[<span class="string">'action_input'</span>]}</span>)\n"</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"observation"</span> <span class="keyword">in</span> step:</span><br><span class="line">                prompt += <span class="string">f"Observation: <span class="subst">{step[<span class="string">'observation'</span>]}</span>\n"</span></span><br><span class="line">        </span><br><span class="line">        prompt += <span class="string">"\nAvailable tools: "</span> + <span class="string">", "</span>.join(self.tools.keys())</span><br><span class="line">        prompt += <span class="string">"\nPlease continue in the format 'Thought -&gt; Action -&gt; Observation'."</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> prompt</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_response</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">"""Parse model response"""</span></span><br><span class="line">        <span class="comment"># Simplified implementation, should be more robust in practice</span></span><br><span class="line">        lines = response.strip().split(<span class="string">"\n"</span>)</span><br><span class="line">        thought = <span class="string">""</span></span><br><span class="line">        action = <span class="string">""</span></span><br><span class="line">        action_input = <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">"Thought:"</span>):</span><br><span class="line">                thought = line.replace(<span class="string">"Thought:"</span>, <span class="string">""</span>).strip()</span><br><span class="line">            <span class="keyword">elif</span> line.startswith(<span class="string">"Action:"</span>):</span><br><span class="line">                parts = line.replace(<span class="string">"Action:"</span>, <span class="string">""</span>).strip().split(<span class="string">"("</span>)</span><br><span class="line">                action = parts[<span class="number">0</span>].strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(parts) &gt; <span class="number">1</span>:</span><br><span class="line">                    action_input = parts[<span class="number">1</span>].rstrip(<span class="string">")"</span>).strip()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> thought, action, action_input</span><br></pre></td></tr></table></figure>
<h2 id="Code-Generation-and-Understanding"><a class="header-anchor" href="#Code-Generation-and-Understanding">¶</a>Code Generation and Understanding</h2>
<h3 id="CodeLlama-Code-Specialized-Large-Model"><a class="header-anchor" href="#CodeLlama-Code-Specialized-Large-Model">¶</a>CodeLlama: Code-Specialized Large Model</h3>
<p>CodeLlama is a code generation and understanding model developed by Meta based on LLaMA 2.</p>
<p><strong>Features</strong>:</p>
<ul>
<li><strong>Multilingual Support</strong>: Python, C++, Java, PHP, TypeScript, C#, Bash, etc.</li>
<li><strong>Multiple Variants</strong>: Base model, Python-specific, instruction-tuned versions</li>
<li><strong>Long Context</strong>: Supports up to 100K tokens of context</li>
</ul>
<p><strong>Usage Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CodeLlamaGenerator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"codellama/CodeLlama-7b-Instruct-hf"</span></span>):</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">            model_name,</span><br><span class="line">            torch_dtype=torch.float16,</span><br><span class="line">            device_map=<span class="string">"auto"</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_code</span>(<span class="params">self, prompt, max_length=<span class="number">512</span>, temperature=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="string">"""Generate code"""</span></span><br><span class="line">        <span class="comment"># Build prompt format</span></span><br><span class="line">        formatted_prompt = <span class="string">f"&lt;s&gt;[INST] <span class="subst">{prompt}</span> [/INST]"</span></span><br><span class="line">        </span><br><span class="line">        inputs = self.tokenizer(formatted_prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=max_length,</span><br><span class="line">                temperature=temperature,</span><br><span class="line">                top_p=<span class="number">0.9</span>,</span><br><span class="line">                do_sample=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        generated_text = self.tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Extract generated code portion</span></span><br><span class="line">        code = generated_text.split(<span class="string">"[/INST]"</span>)[-<span class="number">1</span>].strip()</span><br><span class="line">        <span class="keyword">return</span> code</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">complete_code</span>(<span class="params">self, code_context, language=<span class="string">"python"</span></span>):</span><br><span class="line">        <span class="string">"""Code completion"""</span></span><br><span class="line">        prompt = <span class="string">f"Complete the following <span class="subst">{language}</span> code:\n\n<span class="subst">{code_context}</span>"</span></span><br><span class="line">        <span class="keyword">return</span> self.generate_code(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">explain_code</span>(<span class="params">self, code</span>):</span><br><span class="line">        <span class="string">"""Code explanation"""</span></span><br><span class="line">        prompt = <span class="string">f"Explain what the following code does:\n\n<span class="subst">{code}</span>"</span></span><br><span class="line">        <span class="keyword">return</span> self.generate_code(prompt, max_length=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">generator = CodeLlamaGenerator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate code</span></span><br><span class="line">code = generator.generate_code(<span class="string">"Write a Python function to calculate fibonacci numbers"</span>)</span><br><span class="line"><span class="built_in">print</span>(code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Code completion</span></span><br><span class="line">partial_code = <span class="string">"""</span></span><br><span class="line"><span class="string">def quicksort(arr):</span></span><br><span class="line"><span class="string">    if len(arr) &lt;= 1:</span></span><br><span class="line"><span class="string">        return arr</span></span><br><span class="line"><span class="string">    pivot = arr[len(arr) // 2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">completed = generator.complete_code(partial_code)</span><br><span class="line"><span class="built_in">print</span>(completed)</span><br></pre></td></tr></table></figure>
<h3 id="StarCoder-Trained-on-GitHub-Code"><a class="header-anchor" href="#StarCoder-Trained-on-GitHub-Code">¶</a>StarCoder: Trained on GitHub Code</h3>
<p>StarCoder is a code model developed by the BigCode project, trained on GitHub code.</p>
<p><strong>Features</strong>:</p>
<ul>
<li><strong>Large-Scale Training</strong>: Trained on code in 80+ programming languages</li>
<li><strong>Long Context</strong>: Supports 8K tokens</li>
<li><strong>Code Completion</strong>: Specifically optimized code completion capabilities</li>
</ul>
<p><strong>Usage Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StarCoderGenerator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"bigcode/starcoder"</span></span>):</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">            model_name,</span><br><span class="line">            device_map=<span class="string">"auto"</span>,</span><br><span class="line">            trust_remote_code=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">complete</span>(<span class="params">self, code, max_new_tokens=<span class="number">256</span></span>):</span><br><span class="line">        <span class="string">"""Code completion"""</span></span><br><span class="line">        inputs = self.tokenizer(code, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=max_new_tokens,</span><br><span class="line">                temperature=<span class="number">0.2</span>,</span><br><span class="line">                top_p=<span class="number">0.95</span>,</span><br><span class="line">                do_sample=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        completed_code = self.tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> completed_code</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">starcoder = StarCoderGenerator()</span><br><span class="line">code = <span class="string">"def binary_search(arr, target):"</span></span><br><span class="line">completed = starcoder.complete(code)</span><br><span class="line"><span class="built_in">print</span>(completed)</span><br></pre></td></tr></table></figure>
<h3 id="Code-Understanding-and-Q-A"><a class="header-anchor" href="#Code-Understanding-and-Q-A">¶</a>Code Understanding and Q&amp;A</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CodeUnderstandingSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"codellama/CodeLlama-7b-Instruct-hf"</span></span>):</span><br><span class="line">        self.generator = CodeLlamaGenerator(model_name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">self, code, question</span>):</span><br><span class="line">        <span class="string">"""Answer questions about code"""</span></span><br><span class="line">        prompt = <span class="string">f"""Given the following code:</span></span><br><span class="line"><span class="string"><span class="subst">{code}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: <span class="subst">{question}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Please provide a detailed answer."""</span></span><br><span class="line">        <span class="keyword">return</span> self.generator.generate_code(prompt, max_length=<span class="number">256</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find_bugs</span>(<span class="params">self, code</span>):</span><br><span class="line">        <span class="string">"""Find bugs in code"""</span></span><br><span class="line">        prompt = <span class="string">f"""Analyze the following code and identify any bugs or potential issues:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">{code}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">List all bugs found:"""</span></span><br><span class="line">        <span class="keyword">return</span> self.generator.generate_code(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">refactor_code</span>(<span class="params">self, code, instructions</span>):</span><br><span class="line">        <span class="string">"""Refactor code"""</span></span><br><span class="line">        prompt = <span class="string">f"""Refactor the following code according to these instructions: <span class="subst">{instructions}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original code:</span></span><br><span class="line"><span class="string"><span class="subst">{code}</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Refactored code:"""</span></span><br><span class="line">        <span class="keyword">return</span> self.generator.generate_code(prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">code_understanding = CodeUnderstandingSystem()</span><br><span class="line"></span><br><span class="line">code = <span class="string">"""</span></span><br><span class="line"><span class="string">def calculate_total(items):</span></span><br><span class="line"><span class="string">    total = 0</span></span><br><span class="line"><span class="string">    for item in items:</span></span><br><span class="line"><span class="string">        total += item.price</span></span><br><span class="line"><span class="string">    return total</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Answer questions</span></span><br><span class="line">answer = code_understanding.answer_question(code, <span class="string">"What does this function do?"</span>)</span><br><span class="line"><span class="built_in">print</span>(answer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find bugs</span></span><br><span class="line">bugs = code_understanding.find_bugs(code)</span><br><span class="line"><span class="built_in">print</span>(bugs)</span><br></pre></td></tr></table></figure>
<h2 id="Long-Context-Modeling"><a class="header-anchor" href="#Long-Context-Modeling">¶</a>Long-Context Modeling</h2>
<h3 id="Challenges-and-Solutions"><a class="header-anchor" href="#Challenges-and-Solutions">¶</a>Challenges and Solutions</h3>
<p>Traditional Transformer attention mechanisms have <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.832ex" height="2.452ex" role="img" focusable="false" viewbox="0 -833.9 2577.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> complexity, where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container> is the sequence length, limiting the model’s ability to handle long contexts.</p>
<p><strong>Main Challenges</strong>:</p>
<ol>
<li><strong>Computational Complexity</strong>: Attention matrix grows quadratically with sequence length</li>
<li><strong>Memory Usage</strong>: Need to store complete attention matrix</li>
<li><strong>Position Encoding</strong>: Need to handle positions beyond maximum training length</li>
</ol>
<h3 id="LongLoRA-Efficient-Long-Context-Fine-Tuning"><a class="header-anchor" href="#LongLoRA-Efficient-Long-Context-Fine-Tuning">¶</a>LongLoRA: Efficient Long-Context Fine-Tuning</h3>
<p>LongLoRA achieves efficient long-context fine-tuning through sparse attention mechanisms.</p>
<p><strong>Core Idea</strong>:</p>
<ul>
<li><strong>Shifted Sparse Attention</strong>: Only compute local and global attention, reducing complexity</li>
<li><strong>LoRA Fine-Tuning</strong>: Only fine-tune a small number of parameters, maintaining efficiency</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LongLoRAModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_model_name, max_length=<span class="number">8192</span></span>):</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">            base_model_name,</span><br><span class="line">            torch_dtype=torch.float16,</span><br><span class="line">            device_map=<span class="string">"auto"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Configure LoRA</span></span><br><span class="line">        lora_config = LoraConfig(</span><br><span class="line">            r=<span class="number">16</span>,</span><br><span class="line">            lora_alpha=<span class="number">32</span>,</span><br><span class="line">            target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],</span><br><span class="line">            lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">            bias=<span class="string">"none"</span>,</span><br><span class="line">            task_type=<span class="string">"CAUSAL_LM"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.model = get_peft_model(self.model, lora_config)</span><br><span class="line">        self.max_length = max_length</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, prompt, max_new_tokens=<span class="number">256</span></span>):</span><br><span class="line">        <span class="string">"""Generate long text"""</span></span><br><span class="line">        inputs = self.tokenizer(</span><br><span class="line">            prompt,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_length</span><br><span class="line">        ).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=max_new_tokens,</span><br><span class="line">                temperature=<span class="number">0.7</span>,</span><br><span class="line">                top_p=<span class="number">0.9</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        generated_text = self.tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> generated_text</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">long_model = LongLoRAModel(<span class="string">"meta-llama/Llama-2-7b-hf"</span>, max_length=<span class="number">8192</span>)</span><br><span class="line">long_prompt = <span class="string">"..."</span>  <span class="comment"># Very long prompt</span></span><br><span class="line">result = long_model.generate(long_prompt)</span><br></pre></td></tr></table></figure>
<h3 id="LongLLaMA-Extending-Context-Window"><a class="header-anchor" href="#LongLLaMA-Extending-Context-Window">¶</a>LongLLaMA: Extending Context Window</h3>
<p>LongLLaMA extends the context window through the FoT (Focus on Transformer) mechanism.</p>
<p><strong>FoT Mechanism</strong>:</p>
<ul>
<li><strong>Memory Layer</strong>: Stores long-term memory</li>
<li><strong>Attention Mechanism</strong>: Establishes connections between memory layer and current context</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LongLLaMAModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_model, memory_size=<span class="number">4096</span></span>):</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">        self.memory_size = memory_size</span><br><span class="line">        self.memory = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_with_memory</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="string">"""Forward pass with memory"""</span></span><br><span class="line">        <span class="comment"># Update memory</span></span><br><span class="line">        <span class="keyword">if</span> self.memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Combine current input with memory</span></span><br><span class="line">            extended_input = torch.cat([self.memory, input_ids], dim=<span class="number">1</span>)</span><br><span class="line">            extended_mask = torch.cat([</span><br><span class="line">                torch.ones_like(self.memory),</span><br><span class="line">                attention_mask</span><br><span class="line">            ], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            extended_input = input_ids</span><br><span class="line">            extended_mask = attention_mask</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        outputs = self.base_model(</span><br><span class="line">            input_ids=extended_input,</span><br><span class="line">            attention_mask=extended_mask</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update memory (keep last memory_size tokens)</span></span><br><span class="line">        <span class="keyword">if</span> extended_input.size(<span class="number">1</span>) &gt; self.memory_size:</span><br><span class="line">            self.memory = extended_input[:, -self.memory_size:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.memory = extended_input</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h2 id="Hallucination-and-Mitigation"><a class="header-anchor" href="#Hallucination-and-Mitigation">¶</a>Hallucination and Mitigation</h2>
<h3 id="Definition-and-Types-of-Hallucination"><a class="header-anchor" href="#Definition-and-Types-of-Hallucination">¶</a>Definition and Types of Hallucination</h3>
<p><strong>Types of Hallucination</strong>:</p>
<ol>
<li><strong>Factual Hallucination</strong>: Generate content inconsistent with facts</li>
<li><strong>Logical Hallucination</strong>: Incorrect reasoning process</li>
<li><strong>Consistency Hallucination</strong>: Contradictory statements</li>
</ol>
<h3 id="Mitigation-Strategies"><a class="header-anchor" href="#Mitigation-Strategies">¶</a>Mitigation Strategies</h3>
<p><strong>Retrieval-Augmented Generation (RAG)</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RAGSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, documents, llm_model=<span class="string">"gpt-3.5-turbo"</span></span>):</span><br><span class="line">        <span class="comment"># Text splitting</span></span><br><span class="line">        text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">            chunk_size=<span class="number">1000</span>,</span><br><span class="line">            chunk_overlap=<span class="number">200</span></span><br><span class="line">        )</span><br><span class="line">        texts = text_splitter.split_documents(documents)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create vector store</span></span><br><span class="line">        embeddings = OpenAIEmbeddings()</span><br><span class="line">        self.vectorstore = FAISS.from_documents(texts, embeddings)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create retrieval chain</span></span><br><span class="line">        self.qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">            llm=OpenAI(model_name=llm_model),</span><br><span class="line">            chain_type=<span class="string">"stuff"</span>,</span><br><span class="line">            retriever=self.vectorstore.as_retriever(),</span><br><span class="line">            return_source_documents=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">self, question</span>):</span><br><span class="line">        <span class="string">"""Query and return answer and sources"""</span></span><br><span class="line">        result = self.qa_chain({<span class="string">"query"</span>: question})</span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">"answer"</span>: result[<span class="string">"result"</span>],</span><br><span class="line">            <span class="string">"sources"</span>: result[<span class="string">"source_documents"</span>]</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">documents = [...]  <span class="comment"># Document list</span></span><br><span class="line">rag = RAGSystem(documents)</span><br><span class="line">result = rag.query(<span class="string">"What is machine learning?"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Answer: <span class="subst">{result[<span class="string">'answer'</span>]}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Sources: <span class="subst">{result[<span class="string">'sources'</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Confidence Scoring</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConfidenceScorer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score_response</span>(<span class="params">self, question, answer, context=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""Score answer confidence"""</span></span><br><span class="line">        <span class="comment"># Method 1: Probability-based confidence</span></span><br><span class="line">        probs = self.get_token_probabilities(question, answer)</span><br><span class="line">        avg_prob = probs.mean()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Method 2: Consistency-based confidence</span></span><br><span class="line">        consistency = self.check_consistency(answer, context)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Method 3: Factuality-based confidence</span></span><br><span class="line">        factuality = self.check_factuality(answer, context)</span><br><span class="line">        </span><br><span class="line">        confidence = (avg_prob + consistency + factuality) / <span class="number">3</span></span><br><span class="line">        <span class="keyword">return</span> confidence</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_token_probabilities</span>(<span class="params">self, question, answer</span>):</span><br><span class="line">        <span class="string">"""Get token probabilities"""</span></span><br><span class="line">        <span class="comment"># Implement token probability calculation</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_consistency</span>(<span class="params">self, answer, context</span>):</span><br><span class="line">        <span class="string">"""Check consistency"""</span></span><br><span class="line">        <span class="comment"># Implement consistency check</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_factuality</span>(<span class="params">self, answer, context</span>):</span><br><span class="line">        <span class="string">"""Check factuality"""</span></span><br><span class="line">        <span class="comment"># Implement factuality check</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="Safety-and-Alignment"><a class="header-anchor" href="#Safety-and-Alignment">¶</a>Safety and Alignment</h2>
<h3 id="Safety-Challenges"><a class="header-anchor" href="#Safety-Challenges">¶</a>Safety Challenges</h3>
<p><strong>Main Risks</strong>:</p>
<ol>
<li><strong>Harmful Content Generation</strong>: Generate violent, discriminatory content</li>
<li><strong>Privacy Leakage</strong>: Leak sensitive information from training data</li>
<li><strong>Misuse</strong>: Used for malicious purposes</li>
</ol>
<h3 id="Alignment-Techniques"><a class="header-anchor" href="#Alignment-Techniques">¶</a>Alignment Techniques</h3>
<p><strong>RLHF (Reinforcement Learning from Human Feedback)</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RLHFTrainer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, reward_model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.reward_model = reward_model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, prompts, responses</span>):</span><br><span class="line">        <span class="string">"""RLHF training step"""</span></span><br><span class="line">        <span class="comment"># 1. Generate responses</span></span><br><span class="line">        logits = self.model(prompts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Compute rewards</span></span><br><span class="line">        rewards = self.reward_model(prompts, responses)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Compute policy gradient</span></span><br><span class="line">        loss = self.compute_policy_gradient(logits, rewards)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. Update model</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">return</span> loss.item()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_policy_gradient</span>(<span class="params">self, logits, rewards</span>):</span><br><span class="line">        <span class="string">"""Compute policy gradient"""</span></span><br><span class="line">        <span class="comment"># PPO or other RL algorithms</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>Safety Filters</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SafetyFilter</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.harmful_patterns = [</span><br><span class="line">            <span class="string">r"violence"</span>,</span><br><span class="line">            <span class="string">r"discrimination"</span>,</span><br><span class="line">            <span class="comment"># ... more patterns</span></span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">"""Filter harmful content"""</span></span><br><span class="line">        <span class="keyword">for</span> pattern <span class="keyword">in</span> self.harmful_patterns:</span><br><span class="line">            <span class="keyword">if</span> re.search(pattern, text, re.IGNORECASE):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span>, <span class="string">"Harmful content detected"</span></span><br><span class="line">        <span class="keyword">return</span> text, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Model-Evaluation-System"><a class="header-anchor" href="#Model-Evaluation-System">¶</a>Model Evaluation System</h2>
<h3 id="Evaluation-Dimensions"><a class="header-anchor" href="#Evaluation-Dimensions">¶</a>Evaluation Dimensions</h3>
<p><strong>1. Capability Evaluation</strong>:</p>
<ul>
<li><strong>Language Understanding</strong>: GLUE, SuperGLUE</li>
<li><strong>Language Generation</strong>: BLEU, ROUGE, METEOR</li>
<li><strong>Reasoning Ability</strong>: GSM8K, HellaSwag</li>
</ul>
<p><strong>2. Safety Evaluation</strong>:</p>
<ul>
<li><strong>Harmful Content Detection</strong></li>
<li><strong>Bias Detection</strong></li>
<li><strong>Privacy Risk Assessment</strong></li>
</ul>
<p><strong>3. Efficiency Evaluation</strong>:</p>
<ul>
<li><strong>Inference Speed</strong></li>
<li><strong>Memory Usage</strong></li>
<li><strong>Energy Consumption</strong></li>
</ul>
<h3 id="Evaluation-Framework"><a class="header-anchor" href="#Evaluation-Framework">¶</a>Evaluation Framework</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ModelEvaluator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, tokenizer</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_glue</span>(<span class="params">self, dataset</span>):</span><br><span class="line">        <span class="string">"""Evaluate GLUE tasks"""</span></span><br><span class="line">        results = {}</span><br><span class="line">        <span class="keyword">for</span> task_name, task_data <span class="keyword">in</span> dataset.items():</span><br><span class="line">            accuracy = self.evaluate_task(task_name, task_data)</span><br><span class="line">            results[task_name] = accuracy</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_generation</span>(<span class="params">self, test_set</span>):</span><br><span class="line">        <span class="string">"""Evaluate generation quality"""</span></span><br><span class="line">        bleu_scores = []</span><br><span class="line">        rouge_scores = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> example <span class="keyword">in</span> test_set:</span><br><span class="line">            generated = self.generate(example[<span class="string">"input"</span>])</span><br><span class="line">            bleu = self.compute_bleu(example[<span class="string">"reference"</span>], generated)</span><br><span class="line">            rouge = self.compute_rouge(example[<span class="string">"reference"</span>], generated)</span><br><span class="line">            bleu_scores.append(bleu)</span><br><span class="line">            rouge_scores.append(rouge)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">"bleu"</span>: np.mean(bleu_scores),</span><br><span class="line">            <span class="string">"rouge"</span>: np.mean(rouge_scores)</span><br><span class="line">        }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_safety</span>(<span class="params">self, test_prompts</span>):</span><br><span class="line">        <span class="string">"""Evaluate safety"""</span></span><br><span class="line">        harmful_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> prompt <span class="keyword">in</span> test_prompts:</span><br><span class="line">            response = self.generate(prompt)</span><br><span class="line">            <span class="keyword">if</span> self.is_harmful(response):</span><br><span class="line">                harmful_count += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">"harmful_rate"</span>: harmful_count / <span class="built_in">len</span>(test_prompts),</span><br><span class="line">            <span class="string">"safe_rate"</span>: <span class="number">1</span> - harmful_count / <span class="built_in">len</span>(test_prompts)</span><br><span class="line">        }</span><br></pre></td></tr></table></figure>
<h2 id="Practical-Complete-NLP-Project-Deployment"><a class="header-anchor" href="#Practical-Complete-NLP-Project-Deployment">¶</a>Practical: Complete NLP Project Deployment</h2>
<h3 id="Project-Structure"><a class="header-anchor" href="#Project-Structure">¶</a>Project Structure</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">nlp-service/</span><br><span class="line">├── app/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── main.py          # FastAPI application</span><br><span class="line">│   ├── models.py         # Model loading</span><br><span class="line">│   ├── routers/         # API routes</span><br><span class="line">│   │   ├── chat.py</span><br><span class="line">│   │   ├── embedding.py</span><br><span class="line">│   │   └── generation.py</span><br><span class="line">│   └── utils/</span><br><span class="line">│       ├── logging.py</span><br><span class="line">│       └── monitoring.py</span><br><span class="line">├── tests/</span><br><span class="line">├── docker/</span><br><span class="line">│   └── Dockerfile</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── docker-compose.yml</span><br><span class="line">└── README.md</span><br></pre></td></tr></table></figure>
<h3 id="FastAPI-Application"><a class="header-anchor" href="#FastAPI-Application">¶</a>FastAPI Application</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/main.py</span></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, HTTPException</span><br><span class="line"><span class="keyword">from</span> fastapi.middleware.cors <span class="keyword">import</span> CORSMiddleware</span><br><span class="line"><span class="keyword">from</span> app.models <span class="keyword">import</span> ModelManager</span><br><span class="line"><span class="keyword">from</span> app.routers <span class="keyword">import</span> chat, embedding, generation</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure logging</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span></span><br><span class="line">)</span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">app = FastAPI(title=<span class="string">"NLP Service API"</span>, version=<span class="string">"1.0.0"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CORS configuration</span></span><br><span class="line">app.add_middleware(</span><br><span class="line">    CORSMiddleware,</span><br><span class="line">    allow_origins=[<span class="string">"*"</span>],</span><br><span class="line">    allow_credentials=<span class="literal">True</span>,</span><br><span class="line">    allow_methods=[<span class="string">"*"</span>],</span><br><span class="line">    allow_headers=[<span class="string">"*"</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Global model manager</span></span><br><span class="line">model_manager = ModelManager()</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.on_event(<span class="params"><span class="string">"startup"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">startup_event</span>():</span><br><span class="line">    <span class="string">"""Load models on startup"""</span></span><br><span class="line">    logger.info(<span class="string">"Loading models..."</span>)</span><br><span class="line">    <span class="keyword">await</span> model_manager.load_models()</span><br><span class="line">    logger.info(<span class="string">"Models loaded successfully"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.on_event(<span class="params"><span class="string">"shutdown"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">shutdown_event</span>():</span><br><span class="line">    <span class="string">"""Cleanup resources on shutdown"""</span></span><br><span class="line">    logger.info(<span class="string">"Shutting down..."</span>)</span><br><span class="line">    <span class="keyword">await</span> model_manager.cleanup()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Register routes</span></span><br><span class="line">app.include_router(chat.router, prefix=<span class="string">"/api/v1"</span>, tags=[<span class="string">"chat"</span>])</span><br><span class="line">app.include_router(embedding.router, prefix=<span class="string">"/api/v1"</span>, tags=[<span class="string">"embedding"</span>])</span><br><span class="line">app.include_router(generation.router, prefix=<span class="string">"/api/v1"</span>, tags=[<span class="string">"generation"</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/health"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">health_check</span>():</span><br><span class="line">    <span class="string">"""Health check"""</span></span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"status"</span>: <span class="string">"healthy"</span>, <span class="string">"models_loaded"</span>: model_manager.models_loaded}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app, host=<span class="string">"0.0.0.0"</span>, port=<span class="number">8000</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Model-Management"><a class="header-anchor" href="#Model-Management">¶</a>Model Management</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/models.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelManager</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.models = {}</span><br><span class="line">        self.tokenizers = {}</span><br><span class="line">        self.models_loaded = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">load_models</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Asynchronously load models"""</span></span><br><span class="line">        <span class="comment"># Chat model</span></span><br><span class="line">        <span class="keyword">await</span> self._load_model_async(</span><br><span class="line">            <span class="string">"chat"</span>,</span><br><span class="line">            <span class="string">"meta-llama/Llama-2-7b-chat-hf"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Embedding model</span></span><br><span class="line">        <span class="keyword">await</span> self._load_model_async(</span><br><span class="line">            <span class="string">"embedding"</span>,</span><br><span class="line">            <span class="string">"sentence-transformers/all-MiniLM-L6-v2"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.models_loaded = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">_load_model_async</span>(<span class="params">self, name, model_path</span>):</span><br><span class="line">        <span class="string">"""Asynchronously load a single model"""</span></span><br><span class="line">        loop = asyncio.get_event_loop()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Execute CPU-intensive operations in thread pool</span></span><br><span class="line">        tokenizer = <span class="keyword">await</span> loop.run_in_executor(</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            AutoTokenizer.from_pretrained,</span><br><span class="line">            model_path</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        model = <span class="keyword">await</span> loop.run_in_executor(</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            AutoModelForCausalLM.from_pretrained,</span><br><span class="line">            model_path,</span><br><span class="line">            {<span class="string">"torch_dtype"</span>: torch.float16, <span class="string">"device_map"</span>: <span class="string">"auto"</span>}</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.tokenizers[name] = tokenizer</span><br><span class="line">        self.models[name] = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">cleanup</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""Cleanup resources"""</span></span><br><span class="line">        <span class="keyword">for</span> model <span class="keyword">in</span> self.models.values():</span><br><span class="line">            <span class="keyword">del</span> model</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_model</span>(<span class="params">self, name: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="string">"""Get model"""</span></span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> self.models:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Model <span class="subst">{name}</span> not loaded"</span>)</span><br><span class="line">        <span class="keyword">return</span> self.models[name]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_tokenizer</span>(<span class="params">self, name: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="string">"""Get tokenizer"""</span></span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> self.tokenizers:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Tokenizer <span class="subst">{name}</span> not loaded"</span>)</span><br><span class="line">        <span class="keyword">return</span> self.tokenizers[name]</span><br></pre></td></tr></table></figure>
<h3 id="API-Routes"><a class="header-anchor" href="#API-Routes">¶</a>API Routes</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/routers/chat.py</span></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> APIRouter, HTTPException</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"><span class="keyword">from</span> app.models <span class="keyword">import</span> ModelManager</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">router = APIRouter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatRequest</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    message: <span class="built_in">str</span></span><br><span class="line">    max_tokens: <span class="built_in">int</span> = <span class="number">256</span></span><br><span class="line">    temperature: <span class="built_in">float</span> = <span class="number">0.7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatResponse</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    response: <span class="built_in">str</span></span><br><span class="line">    tokens_used: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@router.post(<span class="params"><span class="string">"/chat"</span>, response_model=ChatResponse</span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">request: ChatRequest, model_manager: ModelManager</span>):</span><br><span class="line">    <span class="string">"""Chat endpoint"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        model = model_manager.get_model(<span class="string">"chat"</span>)</span><br><span class="line">        tokenizer = model_manager.get_tokenizer(<span class="string">"chat"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Encode input</span></span><br><span class="line">        inputs = tokenizer(request.message, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_new_tokens=request.max_tokens,</span><br><span class="line">                temperature=request.temperature,</span><br><span class="line">                do_sample=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Decode</span></span><br><span class="line">        response_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        tokens_used = outputs.shape[<span class="number">1</span>] - inputs.input_ids.shape[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ChatResponse(</span><br><span class="line">            response=response_text,</span><br><span class="line">            tokens_used=tokens_used</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="built_in">str</span>(e))</span><br></pre></td></tr></table></figure>
<h3 id="Docker-Configuration"><a class="header-anchor" href="#Docker-Configuration">¶</a>Docker Configuration</h3>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker/Dockerfile</span></span><br><span class="line"><span class="keyword">FROM</span> nvidia/cuda:<span class="number">11.8</span>.<span class="number">0</span>-cudnn8-runtime-ubuntu22.<span class="number">04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install Python</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get update &amp;&amp; apt-get install -y \</span></span><br><span class="line"><span class="language-bash">    python3.9 \</span></span><br><span class="line"><span class="language-bash">    python3-pip \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install dependencies</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> requirements.txt .</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip3 install --no-cache-dir -r requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Copy application code</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> app/ ./app/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set environment variables</span></span><br><span class="line"><span class="keyword">ENV</span> PYTHONUNBUFFERED=<span class="number">1</span></span><br><span class="line"><span class="keyword">ENV</span> CUDA_VISIBLE_DEVICES=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Expose port</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start command</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">"uvicorn"</span>, <span class="string">"app.main:app"</span>, <span class="string">"--host"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"--port"</span>, <span class="string">"8000"</span>, <span class="string">"--workers"</span>, <span class="string">"1"</span>]</span></span><br></pre></td></tr></table></figure>
<h3 id="Docker-Compose-v2"><a class="header-anchor" href="#Docker-Compose-v2">¶</a>Docker Compose</h3>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker-compose.yml</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3.8'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nlp-service:</span></span><br><span class="line">    <span class="attr">build:</span></span><br><span class="line">      <span class="attr">context:</span> <span class="string">.</span></span><br><span class="line">      <span class="attr">dockerfile:</span> <span class="string">docker/Dockerfile</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"8000:8000"</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">CUDA_VISIBLE_DEVICES=0</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">reservations:</span></span><br><span class="line">          <span class="attr">devices:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">driver:</span> <span class="string">nvidia</span></span><br><span class="line">              <span class="attr">count:</span> <span class="number">1</span></span><br><span class="line">              <span class="attr">capabilities:</span> [<span class="string">gpu</span>]</span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./models:/app/models</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br><span class="line">  </span><br><span class="line">  <span class="attr">monitoring:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">prometheus/prometheus</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"9090:9090"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml</span></span><br></pre></td></tr></table></figure>
<h3 id="Monitoring-System"><a class="header-anchor" href="#Monitoring-System">¶</a>Monitoring System</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/utils/monitoring.py</span></span><br><span class="line"><span class="keyword">from</span> prometheus_client <span class="keyword">import</span> Counter, Histogram, Gauge</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Metric definitions</span></span><br><span class="line">request_count = Counter(</span><br><span class="line">    <span class="string">'nlp_requests_total'</span>,</span><br><span class="line">    <span class="string">'Total number of requests'</span>,</span><br><span class="line">    [<span class="string">'endpoint'</span>, <span class="string">'status'</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">request_duration = Histogram(</span><br><span class="line">    <span class="string">'nlp_request_duration_seconds'</span>,</span><br><span class="line">    <span class="string">'Request duration in seconds'</span>,</span><br><span class="line">    [<span class="string">'endpoint'</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model_memory = Gauge(</span><br><span class="line">    <span class="string">'nlp_model_memory_bytes'</span>,</span><br><span class="line">    <span class="string">'Model memory usage in bytes'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">track_request</span>(<span class="params">endpoint, status</span>):</span><br><span class="line">    <span class="string">"""Track request"""</span></span><br><span class="line">    request_count.labels(endpoint=endpoint, status=status).inc()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">track_duration</span>(<span class="params">endpoint, duration</span>):</span><br><span class="line">    <span class="string">"""Track request duration"""</span></span><br><span class="line">    request_duration.labels(endpoint=endpoint).observe(duration)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_memory_usage</span>():</span><br><span class="line">    <span class="string">"""Update memory usage"""</span></span><br><span class="line">    <span class="keyword">import</span> torch</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        memory = torch.cuda.memory_allocated()</span><br><span class="line">        model_memory.<span class="built_in">set</span>(memory)</span><br></pre></td></tr></table></figure>
<h3 id="Deployment-Script"><a class="header-anchor" href="#Deployment-Script">¶</a>Deployment Script</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># deploy.sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Building Docker image..."</span></span><br><span class="line">docker build -t nlp-service:latest -f docker/Dockerfile .</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Stopping existing containers..."</span></span><br><span class="line">docker-compose down</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Starting services..."</span></span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Waiting for services to be ready..."</span></span><br><span class="line"><span class="built_in">sleep</span> 10</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Checking health..."</span></span><br><span class="line">curl http://localhost:8000/health</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Deployment complete!"</span></span><br></pre></td></tr></table></figure>
<h2 id="❓-Q-A-Common-Questions-About-Frontiers-and-Practical-Applications"><a class="header-anchor" href="#❓-Q-A-Common-Questions-About-Frontiers-and-Practical-Applications">¶</a>❓ Q&amp;A: Common Questions About Frontiers and Practical Applications</h2>
<p><strong>Q1: What’s the difference between Function Calling and ReAct?</strong></p>
<p>A: Function Calling is a static tool calling mechanism where the model decides whether to call based on function definitions. ReAct is a dynamic reasoning-action loop where the model can autonomously plan steps and execute iteratively. Function Calling is better suited for structured tool calls, while ReAct is better for complex task planning.</p>
<p><strong>Q2: How to choose between CodeLlama and StarCoder?</strong></p>
<p>A: CodeLlama is based on LLaMA 2 with stronger instruction-following capabilities, suitable for code generation and Q&amp;A. StarCoder is trained on larger-scale code with stronger code completion capabilities. The choice depends on specific needs: choose CodeLlama for conversational code generation, choose StarCoder for code completion.</p>
<p><strong>Q3: What are the practical application scenarios for long-context models?</strong></p>
<p>A: Main scenarios: 1) Long document Q&amp;A and summarization; 2) Codebase understanding and generation; 3) Multi-turn dialogue history maintenance; 4) Long text analysis. Note that long contexts increase computational costs and require trade-offs.</p>
<p><strong>Q4: How to effectively mitigate model hallucination?</strong></p>
<p>A: Comprehensive strategies: 1) Use RAG to provide external knowledge; 2) Implement confidence scoring and uncertainty quantification; 3) Add fact-checking steps; 4) Use more reliable models; 5) Human review of critical outputs.</p>
<p><strong>Q5: How much human annotation is needed for RLHF training?</strong></p>
<p>A: Typically requires thousands to tens of thousands of human feedback data points. Can use semi-automatic methods: first generate initial feedback using rules or models, then human review and correction to improve efficiency.</p>
<p><strong>Q6: Key considerations for deploying NLP models in production?</strong></p>
<p>A: Key factors: 1) Model size and inference speed; 2) GPU memory and cost; 3) Concurrent processing capability; 4) Error handling and fallback strategies; 5) Monitoring and logging; 6) Security and access control.</p>
<p><strong>Q7: How to optimize NLP service performance?</strong></p>
<p>A: Optimization strategies: 1) Model quantization (INT8/INT4); 2) Batch request processing; 3) Use KV caching; 4) Model distillation; 5) Use smaller model variants; 6) Asynchronous processing; 7) CDN caching for static content.</p>
<p><strong>Q8: How to handle large model files in Docker deployment?</strong></p>
<p>A: Solutions: 1) Use Docker volumes to mount model directories; 2) Use model caching services (e.g., HuggingFace Cache); 3) Pre-download models during build; 4) Use model servers (e.g., TensorRT Inference Server).</p>
<p><strong>Q9: How to monitor NLP service health?</strong></p>
<p>A: Monitoring metrics: 1) Request volume and response time; 2) Error rate and anomalies; 3) GPU usage and memory; 4) Model output quality (sampling evaluation); 5) User feedback. Use Prometheus + Grafana for visualization.</p>
<p><strong>Q10: How to manage resources for multi-model services?</strong></p>
<p>A: Strategies: 1) Use model queues and priority scheduling; 2) Dynamic model loading/unloading; 3) Use model servers (e.g., TorchServe, Triton); 4) Implement resource quotas and rate limiting; 5) Use Kubernetes for resource management and auto-scaling.</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>Post title：NLP (12): Frontiers and Practical Applications</li>
        <li>Post author：Chen Kai</li>
        <li>Create time：2025-03-06 00:00:00</li>
        <li>
            Post link：https://www.chenk.top/en/nlp-frontiers-applications/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/en/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/Applications/">#Applications</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/en/nlp-word-embeddings-lm/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">NLP (2): Word Embeddings and Language Models</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/en/nlp-multimodal-nlp/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">NLP (11): Multimodal Large Language Models</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Agents-and-Tool-Use"><span class="nav-number">1.</span> <span class="nav-text">Agents and Tool Use</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Function-Calling-Enabling-LLMs-to-Call-External-Tools"><span class="nav-number">1.1.</span> <span class="nav-text">Function Calling: Enabling LLMs to Call External Tools</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReAct-Combining-Reasoning-and-Acting"><span class="nav-number">1.2.</span> <span class="nav-text">ReAct: Combining Reasoning and Acting</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Code-Generation-and-Understanding"><span class="nav-number">2.</span> <span class="nav-text">Code Generation and Understanding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CodeLlama-Code-Specialized-Large-Model"><span class="nav-number">2.1.</span> <span class="nav-text">CodeLlama: Code-Specialized Large Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StarCoder-Trained-on-GitHub-Code"><span class="nav-number">2.2.</span> <span class="nav-text">StarCoder: Trained on GitHub Code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Code-Understanding-and-Q-A"><span class="nav-number">2.3.</span> <span class="nav-text">Code Understanding and Q&amp;A</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Long-Context-Modeling"><span class="nav-number">3.</span> <span class="nav-text">Long-Context Modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Challenges-and-Solutions"><span class="nav-number">3.1.</span> <span class="nav-text">Challenges and Solutions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LongLoRA-Efficient-Long-Context-Fine-Tuning"><span class="nav-number">3.2.</span> <span class="nav-text">LongLoRA: Efficient Long-Context Fine-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LongLLaMA-Extending-Context-Window"><span class="nav-number">3.3.</span> <span class="nav-text">LongLLaMA: Extending Context Window</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hallucination-and-Mitigation"><span class="nav-number">4.</span> <span class="nav-text">Hallucination and Mitigation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition-and-Types-of-Hallucination"><span class="nav-number">4.1.</span> <span class="nav-text">Definition and Types of Hallucination</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mitigation-Strategies"><span class="nav-number">4.2.</span> <span class="nav-text">Mitigation Strategies</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Safety-and-Alignment"><span class="nav-number">5.</span> <span class="nav-text">Safety and Alignment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Safety-Challenges"><span class="nav-number">5.1.</span> <span class="nav-text">Safety Challenges</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Alignment-Techniques"><span class="nav-number">5.2.</span> <span class="nav-text">Alignment Techniques</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Evaluation-System"><span class="nav-number">6.</span> <span class="nav-text">Model Evaluation System</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-Dimensions"><span class="nav-number">6.1.</span> <span class="nav-text">Evaluation Dimensions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-Framework"><span class="nav-number">6.2.</span> <span class="nav-text">Evaluation Framework</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Practical-Complete-NLP-Project-Deployment"><span class="nav-number">7.</span> <span class="nav-text">Practical: Complete NLP Project Deployment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Project-Structure"><span class="nav-number">7.1.</span> <span class="nav-text">Project Structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FastAPI-Application"><span class="nav-number">7.2.</span> <span class="nav-text">FastAPI Application</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Management"><span class="nav-number">7.3.</span> <span class="nav-text">Model Management</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#API-Routes"><span class="nav-number">7.4.</span> <span class="nav-text">API Routes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Docker-Configuration"><span class="nav-number">7.5.</span> <span class="nav-text">Docker Configuration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Docker-Compose-v2"><span class="nav-number">7.6.</span> <span class="nav-text">Docker Compose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monitoring-System"><span class="nav-number">7.7.</span> <span class="nav-text">Monitoring System</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deployment-Script"><span class="nav-number">7.8.</span> <span class="nav-text">Deployment Script</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%9D%93-Q-A-Common-Questions-About-Frontiers-and-Practical-Applications"><span class="nav-number">8.</span> <span class="nav-text">❓ Q&amp;A: Common Questions About Frontiers and Practical Applications</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
