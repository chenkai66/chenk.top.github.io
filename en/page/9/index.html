<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="home-content-container fade-in-down-animation">
    
    
    

    <ul class="home-article-list">
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/reinforcement-learning-3-policy-gradient-and-actor-critic/">
                        Reinforcement Learning (3) — Policy Gradient and Actor-Critic Methods
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>If value function methods learn policies indirectly by "evaluating
action quality," then policy gradient methods directly optimize the
policy itself. DQN's success proved deep learning's tremendous potential
in reinforcement learning, but its limitations are also obvious—it can
only handle discrete action spaces and struggles with continuous control
tasks like robot control and autonomous driving. Policy Gradient methods
parameterize policies as neural networks <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.875ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 3038.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"/></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g></g><g data-mml-node="mo" transform="translate(984.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1373.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(1902.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(2180.6,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mo" transform="translate(2649.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> and use gradient ascent
to directly maximize expected returns, naturally supporting continuous
actions. From the earliest REINFORCE algorithm to Actor-Critic
architectures combining value functions, from asynchronous parallel A3C
to breakthrough DDPG, from sample-efficient TD3 to industrially
widespread PPO, to SAC under the maximum entropy framework—policy
gradient methods have become the mainstream technical approach in deep
reinforcement learning. This chapter systematically traces this
evolution path, deeply analyzing each algorithm's design motivations,
mathematical principles, and implementation details.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Jul 05 2025 10:00:00 GMT+0800">2025-07-05</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Artificial-Intelligence/">Artificial Intelligence</a>&nbsp;
                        </li>
                    
                    
                        <li>
                            &gt; 
                            <a href="/en/categories/Artificial-Intelligence/Reinforcement-Learning/">Reinforcement Learning</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Policy-Gradient/">Policy Gradient</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Actor-Critic/">Actor-Critic</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/REINFORCE/">REINFORCE</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/A3C/">A3C</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/DDPG/">DDPG</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/reinforcement-learning-3-policy-gradient-and-actor-critic/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/recommendation-systems-5-embedding-techniques/">
                        Recommendation Systems (5): Embedding and Representation Learning
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>When you browse Netflix, each movie recommendation feels
personalized—not just because the algorithm knows your viewing history,
but because it has learned dense vector representations (embeddings)
that capture subtle relationships between movies, genres, and your
preferences. These embeddings transform sparse, high-dimensional
user-item interactions into compact, semantically rich vectors that
enable efficient similarity search and recommendation.</p>
<p>Embedding techniques form the backbone of modern recommendation
systems, from Word2Vec-inspired Item2Vec that treats user sequences as
"sentences," to graph-based Node2Vec that captures complex item
relationships, to deep two-tower architectures like DSSM and YouTube DNN
that learn separate user and item embeddings. These methods solve
fundamental challenges: how to represent items and users in a way that
preserves their relationships, how to handle millions of items
efficiently, and how to learn from implicit feedback signals like clicks
and views.</p>
<p>This article provides a comprehensive exploration of embedding
techniques for recommendation systems, covering theoretical foundations,
sequence-based methods (Item2Vec, Word2Vec), graph-based approaches
(Node2Vec), two-tower architectures (DSSM, YouTube DNN), negative
sampling strategies, approximate nearest neighbor search (FAISS, Annoy,
HNSW), embedding quality evaluation, and practical implementation with
10+ code examples and detailed Q&amp;A sections.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Thu Jun 26 2025 00:00:00 GMT+0800">2025-06-26</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Embedding/">Embedding</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/recommendation-systems-5-embedding-techniques/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/reinforcement-learning-2-q-learning-and-dqn/">
                        Reinforcement Learning (2) — Q-Learning and Deep Q-Networks (DQN)
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>From board games to Atari video games, value function methods have
been a cornerstone of reinforcement learning. Q-Learning learns to
select optimal actions by iteratively updating state-action values, but
faces the curse of dimensionality when dealing with high-dimensional
state spaces (like an 84x84 pixel game screen). DeepMind's Deep
Q-Network (DQN), proposed in 2013, broke through this barrier by using
neural networks as function approximators, combined with two key
innovations: experience replay and target networks. This enabled
computers to achieve superhuman performance on multiple Atari games for
the first time. This breakthrough not only accelerated the development
of deep reinforcement learning but also spawned a series of improvements
like Double DQN, Dueling DQN, and Prioritized Experience Replay,
culminating in the Rainbow algorithm. This chapter starts from the
mathematical foundations of Q-Learning, progressively deconstructs DQN's
core mechanisms, and analyzes the design motivations and implementation
details of various variants.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Wed Jun 18 2025 10:00:00 GMT+0800">2025-06-18</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Artificial-Intelligence/">Artificial Intelligence</a>&nbsp;
                        </li>
                    
                    
                        <li>
                            &gt; 
                            <a href="/en/categories/Artificial-Intelligence/Reinforcement-Learning/">Reinforcement Learning</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Q-Learning/">Q-Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/DQN/">DQN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Deep-Q-Network/">Deep Q-Network</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Rainbow/">Rainbow</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Experience-Replay/">Experience Replay</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/reinforcement-learning-2-q-learning-and-dqn/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/recommendation-systems-4-ctr-prediction/">
                        Recommendation Systems (4): CTR Prediction and Click-Through Rate Modeling
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>When you scroll through your social media feed, click on a product
recommendation, or watch a suggested video, you're interacting with one
of the most critical components of modern recommendation systems: the
CTR (Click-Through Rate) prediction model. These models answer a
deceptively simple question: "What's the probability this user will
click on this item?" But behind this simplicity lies a complex machine
learning challenge that directly impacts billions of dollars in revenue
for platforms like Facebook, Google, Amazon, and Alibaba.</p>
<p>CTR prediction sits at the heart of the ranking stage in
recommendation systems. After candidate generation retrieves thousands
of potential items, CTR models score each candidate to determine the
final ranking order. A 1% improvement in CTR prediction accuracy can
translate to millions of dollars in additional revenue for large-scale
platforms. This makes CTR prediction one of the most researched and
optimized problems in machine learning.</p>
<p>This article takes you on a journey through the evolution of CTR
prediction models, from the foundational Logistic Regression baseline to
state-of-the-art deep learning architectures like DeepFM, xDeepFM, DCN,
AutoInt, and FiBiNet. We'll explore not just how these models work
mathematically, but why they were designed the way they were, what
problems they solve, and how to implement them from scratch. Along the
way, we'll cover feature engineering techniques, training strategies,
and practical considerations that separate academic prototypes from
production-ready systems.</p>
<p>Whether you're building a recommendation system for the first time or
optimizing an existing one, understanding CTR prediction models is
essential. These models have evolved dramatically over the past decade,
incorporating insights from factorization machines, deep learning,
attention mechanisms, and feature interaction modeling. By the end of
this article, you'll have a comprehensive understanding of the field and
the practical skills to implement these models yourself.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Jun 07 2025 00:00:00 GMT+0800">2025-06-07</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/CTR-Prediction/">CTR Prediction</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/recommendation-systems-4-ctr-prediction/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/reinforcement-learning-1-fundamentals-and-core-concepts/">
                        Reinforcement Learning (Part 1): Fundamentals and Core Concepts
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>How should an intelligent agent learn optimal behavior in an
environment? When AlphaGo defeats world champions on the Go board, when
robots learn to walk and grasp objects, when recommendation systems
continuously optimize suggestions based on user feedback—all of these
share a common mathematical framework: reinforcement learning.</p>
<p>Reinforcement learning differs from supervised learning's "train with
labeled data" approach and unsupervised learning's "discover inherent
structure" paradigm. It addresses a problem closer to the real world:
<strong>learning optimal decision-making strategies for long-term
rewards through trial-and-error and reward feedback, without explicit
correct answers</strong>.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sun Jun 01 2025 10:30:00 GMT+0800">2025-06-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Machine-Learning/">Machine Learning</a>&nbsp;
                        </li>
                    
                    
                        <li>
                            &gt; 
                            <a href="/en/categories/Machine-Learning/Reinforcement-Learning/">Reinforcement Learning</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/MDP/">MDP</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Q-Learning/">Q-Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Dynamic-Programming/">Dynamic Programming</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Reinforcement-Learning/">Reinforcement Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Bellman-Equation/">Bellman Equation</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/reinforcement-learning-1-fundamentals-and-core-concepts/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
    </ul>

    <div class="home-paginator">
        <div class="paginator">
    
        <a class="prev btn"
           href="/en/page/8/"
        >Prev</a>
    

    
        <a class="next btn"
           href="/en/page/10/"
        >Next</a>
    
</div>

    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>





<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
