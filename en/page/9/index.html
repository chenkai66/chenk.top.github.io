<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="home-content-container fade-in-down-animation">
    
    
    

    <ul class="home-article-list">
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/gnn-equivariant-representations/">
                        Graph Neural Networks for Learning Equivariant Representations of Neural Networks
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Neural network parameters live in a space with strong permutation symmetries: you can reorder hidden units without changing the function, yet the raw weight tensors look completely different. If a representation ignores this, it ends up learning spurious differences and struggles to generalize across architectures or widths. This paper proposes representing a neural network as a <strong>neural graph</strong> (nodes as neurons/bias features, edges as weights) and then using a GNN to produce <strong>equivariant</strong> representations that respect these symmetries. This enables tasks like predicting generalization, classifying networks by behavior, retrieving similar architectures, and meta-learning over model populations.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/GNN/">GNN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Representation-Learning/">Representation Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Meta-Learning/">Meta-Learning</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/gnn-equivariant-representations/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/hcgr/">
                        HCGR: Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Session-based recommendation often hides a hierarchical structure: users start with a coarse intent (e.g., “running shoes”), then narrow down to brand, style, size, and price. Euclidean embeddings are good at “flat similarity”, but they are not a natural geometry for tree-like growth. HCGR’s core idea is to model session graphs in <strong>hyperbolic space</strong> (specifically the Lorentz model) and use <strong>contrastive learning</strong> to make the representations more robust and discriminative.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/GNN/">GNN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Recommender-Systems/">Recommender Systems</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Contrastive-Learning/">Contrastive Learning</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/hcgr/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/llmgr/">
                        LLMGR: Integrating Large Language Models with Graphical Session-Based Recommendation
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Session-based recommendation (SBR) is a “short-history” problem: given a short click sequence in a session (typically 3–20 clicks), predict the next item without relying on a stable long-term user profile. The difficulty is not conceptual but practical: sessions are short, long-tail items are abundant, cold-start is frequent, and <strong>relying purely on interaction graphs (IDs + transition edges) often fails to learn stable representations</strong>—new items have almost no edges, long-tail items have very sparse edges, and user exploration introduces significant noise.</p>
<p>However, real-world systems often have a wealth of underutilized textual side information (titles, descriptions, attributes, reviews). If this semantic information could be leveraged, it could theoretically alleviate cold-start and long-tail problems: even if a new item has no interactions, it still has a title and description; even if a long-tail item has few interactions, its semantic information is still available. The challenge is that traditional GNN-SBR methods struggle to effectively inject textual semantics into session graph modeling—graph models excel at learning structure, LLMs excel at understanding semantics, but their representation spaces are naturally incompatible, and simply concatenating them often fails to train stably.</p>
<p>LLMGR’s core approach is to treat a large language model as a “semantic engine” that converts text into representations alignable with graph nodes; then use a hybrid encoding layer to fuse semantics and graph structure into the same representation space; finally, use a two-stage prompt tuning strategy to first align “node–text” (teaching the model “which description corresponds to which item”) and then align “session–behavior patterns” (teaching the model “how to predict next-item intent from session graphs”). This note explains why it is designed this way, what bottlenecks each stage of training solves, how the fusion layer combines semantics with transition patterns, and why it can more stably widen the gap in sparse and cold-start settings. I’ll also preserve the key experimental details and numbers from the paper (e.g., on Amazon Music/Beauty/Pantry datasets, compared to the strongest baseline, HR@20 improves by ~8.68%, NDCG@20 by 10.71%, MRR@20 by 11.75%) to help you evaluate whether this method is worth trying.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/GNN/">GNN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/LLM/">LLM</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Recommender-Systems/">Recommender Systems</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/llmgr/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/moslora/">
                        MoSLoRA: Mixture-of-Subspaces in Low-Rank Adaptation
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>LoRA is a simple and effective parameter-efficient fine-tuning (PEFT) method, but a single low-rank subspace can be too restrictive for complex tasks or heterogeneous domains. MoSLoRA increases LoRA’s expressivity by using a <strong>mixture of low-rank subspaces</strong> while keeping the operational simplicity of LoRA: small trainable parameters, low inference overhead, and practical deployability. The main idea is to represent the adaptation as multiple low-rank “experts” and combine them with a learnable mixer—without turning the model into a full Mixture-of-Experts (MoE) system with routing complexity.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/LLM/">LLM</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/PEFT/">PEFT</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/moslora/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/prefix-tuning/">
                        Prefix-Tuning: Optimizing Continuous Prompts for Generation
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Prefix-Tuning is a parameter-efficient way to adapt a frozen language model: instead of updating model weights, you learn a small set of continuous vectors (“prefixes”) that steer the model’s generation. A key practical variant injects learned prefixes into the attention mechanism as per-layer key/value prefixes. This note explains the method, why reparameterization helps optimization stability, how Prefix-Tuning compares to prompt tuning and LoRA, and what implementation details matter in real training.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/LLM/">LLM</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/PEFT/">PEFT</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/prefix-tuning/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/paper2repo%EF%BC%9AGitHub%20Repository%20Recommendation%20for%20Academic%20Papers/">
                        paper2repo：GitHub Repository Recommendation for Academic Papers
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Finding the code behind a paper is often the most frustrating part of reproducing results: links are missing, names drift, and keyword search is noisy. <strong>paper2repo</strong> frames this as a cross-platform recommendation problem—matching academic papers to relevant GitHub repositories by aligning them in a shared embedding space. It combines text encoders with graph-based signals (e.g., citation/context relations and repository-side structure) via a constrained GCN to learn comparable representations and rank candidate repos. This note summarizes the motivation, how the joint graph is built, what the “constrained” alignment is doing, and which components seem to drive improvements in Hit@K / MAP / MRR.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Recommend-System/">Recommend System</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/paper2repo%EF%BC%9AGitHub%20Repository%20Recommendation%20for%20Academic%20Papers/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
    </ul>

    <div class="home-paginator">
        <div class="paginator">
    
        <a class="prev btn"
           href="/en/page/8/"
        >Prev</a>
    

    
        <a class="next btn"
           href="/en/page/10/"
        >Next</a>
    
</div>

    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>





<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
