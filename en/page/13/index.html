<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="home-content-container fade-in-down-animation">
    
    
    

    <ul class="home-article-list">
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/nlp-attention-transformer/">
                        NLP (4): Attention Mechanism and Transformer
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>The Transformer architecture revolutionized natural language
processing by introducing a mechanism that allows models to focus on
relevant parts of the input when processing each element. Unlike
recurrent networks that process sequences step-by-step, Transformers use
attention to capture dependencies regardless of distance, making them
both more powerful and more parallelizable. This article explores the
evolution from basic sequence-to-sequence models to the full Transformer
architecture, diving deep into attention mechanisms, multi-head
attention, positional encoding, and providing complete PyTorch
implementations that you can run and modify.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Nov 12 2024 00:00:00 GMT+0800">2024-11-12</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/NLP/">NLP</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Transformer/">Transformer</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Attention/">Attention</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/nlp-attention-transformer/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/nlp-rnn-sequence-modeling/">
                        NLP (3): RNN and Sequence Modeling
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Sequence data is everywhere in natural language processing—from
sentences and documents to time-series conversations. Unlike feedforward
networks that treat inputs as independent fixed-size vectors, Recurrent
Neural Networks (RNNs) maintain an internal state that evolves as they
process sequences step by step. This recurrent connection allows the
network to capture temporal dependencies and context, making RNNs a
natural choice for language modeling, machine translation, and text
generation. However, vanilla RNNs struggle with long-range dependencies
due to vanishing gradients. This challenge led to the development of
gated architectures like LSTM and GRU, which selectively control
information flow and maintain long-term memory. In this article, we'll
explore the core mechanics of RNN architectures, understand why gradient
issues arise during backpropagation through time, and dive into
practical implementations using PyTorch for text generation and
sequence-to-sequence tasks.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sun Oct 20 2024 00:00:00 GMT+0800">2024-10-20</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/NLP/">NLP</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/RNN/">RNN</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/nlp-rnn-sequence-modeling/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/symplectic-geometry-and-structure-preserving-neural-networks/">
                        Symplectic Geometry and Structure-Preserving Neural Networks
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Traditional neural networks often fail to preserve the intrinsic
structure of physical systems when predicting their evolution—energy
conservation, angular momentum conservation, symplectic structure, and
more. A simple example: using a standard neural network to predict the
motion of a harmonic oscillator, even with small training error, the
energy gradually drifts after long-term evolution, and the trajectory
deviates from the true orbit. This is because standard neural networks
do not encode the geometric structure of physical systems.</p>
<p><strong>Structure-preserving learning</strong> addresses this by
enabling neural networks to learn the geometric structure of physical
systems, not just fit the data. For Hamiltonian systems, this means
learning dynamics on symplectic manifolds; for Lagrangian systems, this
means learning extremal paths of action functionals. These geometric
constraints not only improve long-term prediction accuracy but also
endow models with interpretability and physical meaning.</p>
<p>This article systematically introduces the mathematical foundations
and practical methods of structure-preserving learning. Starting from
Hamiltonian mechanics and symplectic geometry, we introduce core
concepts such as phase space, Poisson brackets, and symplectic
manifolds; then we analyze in depth the energy-preserving properties of
symplectic integrators (Verlet, symplectic Runge-Kutta); finally, we
focus on three main structure-preserving neural network architectures:
Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and
Symplectic Neural Networks (SympNet), validated through four classical
experiments.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Oct 05 2024 00:00:00 GMT+0800">2024-10-05</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/PDE-and-Machine-Learning/">PDE and Machine Learning</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/PDE/">PDE</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Machine-Learning/">Machine Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Symplectic-Geometry/">Symplectic Geometry</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Hamiltonian-Systems/">Hamiltonian Systems</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Structure-Preserving-Learning/">Structure-Preserving Learning</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/symplectic-geometry-and-structure-preserving-neural-networks/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/gnn-equivariant-representations/">
                        Graph Neural Networks for Learning Equivariant Representations of Neural Networks
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Neural network parameters live in a space with strong permutation
symmetries: you can reorder hidden units without changing the function,
yet the raw weight tensors look completely different. If a
representation ignores this, it ends up learning spurious differences
and struggles to generalize across architectures or widths. This paper
proposes representing a neural network as a <strong>neural
graph</strong> (nodes as neurons/bias features, edges as weights) and
then using a GNN to produce <strong>equivariant</strong> representations
that respect these symmetries.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/GNN/">GNN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Representation-Learning/">Representation Learning</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/gnn-equivariant-representations/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/gcsan/">
                        Graph Contextualized Self-Attention Network (GC-SAN) for Session-based Recommendation
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Session-based recommendation predicts the next clicked item from a
short session sequence when long-term user history is missing or
unreliable (e.g., anonymous traffic, cold-start users, multi-device
sessions). GC-SAN is a hybrid approach: it uses a <strong>session graph
+ GNN</strong> to capture local transition patterns and uses
<strong>self-attention</strong> to capture global, long-range
dependencies within the same session. The key insight is that “sequence”
and “graph” are complementary views of session intent: the sequence
expresses order, while the graph exposes repeated transitions and
multi-hop relations.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/GNN/">GNN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Attention/">Attention</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Recommender-Systems/">Recommender Systems</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/gcsan/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/Session-based%20Recommendation%20with%20Graph%20Neural%20Networks/">
                        Session-based Recommendation with Graph Neural Networks (SR-GNN)
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Session-based recommendation is challenging when you only observe a
short click sequence and have little or no long-term user profile.
<strong>SR-GNN</strong> tackles this by turning each session into a
directed graph, where repeated items and multi-step transitions form
richer structure than a plain sequence. A gated GNN propagates
information over this session graph to learn item representations, and
the model then aggregates them into a session representation to score
next-item candidates. This note explains the session-graph construction,
the gated message passing update, and how SR-GNN produces the final
ranking—highlighting why this graph view often outperforms purely
sequential baselines on standard SBR benchmarks.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/GNN/">GNN</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Recommend-System/">Recommend System</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/Session-based%20Recommendation%20with%20Graph%20Neural%20Networks/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/Solving%20Constrained%20Mean-Variance%20Portfolio%20Optimization%20Problems%20Using%20Spiral%20Optimization%20Algorithm/">
                        Solving Constrained Mean-Variance Portfolio Optimization Problems Using Spiral Optimization Algorithm
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>The classic mean–variance portfolio model is elegant, but real
trading constraints (buy-in thresholds, cardinality limits, min/max
position sizes) quickly turn it into a hard mixed-integer nonlinear
problem. This paper tackles that constrained setting with a modified
<strong>Spiral Optimization Algorithm (SOA)</strong>—a metaheuristic
designed to search complex feasible regions where convex solvers or
gradient methods are not directly applicable. This note focuses on the
formulation (what constraints are added and how), how SOA explores the
search space, and what the reported results say about solution quality
under practical constraints.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Portfolio-optimization/">Portfolio optimization</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/Solving%20Constrained%20Mean-Variance%20Portfolio%20Optimization%20Problems%20Using%20Spiral%20Optimization%20Algorithm/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/paper2repo%EF%BC%9AGitHub%20Repository%20Recommendation%20for%20Academic%20Papers/">
                        paper2repo：GitHub Repository Recommendation for Academic Papers
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Finding the code behind a paper is often the most frustrating part of
reproducing results: links are missing, names drift, and keyword search
is noisy. <strong>paper2repo</strong> frames this as a cross-platform
recommendation problem—matching academic papers to relevant GitHub
repositories by aligning them in a shared embedding space. It combines
text encoders with graph-based signals (e.g., citation/context relations
and repository-side structure) via a constrained GCN to learn comparable
representations and rank candidate repos. This note summarizes the
motivation, how the joint graph is built, what the “constrained”
alignment is doing, and which components seem to drive improvements in
Hit@K / MAP / MRR.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 01 2024 00:00:00 GMT+0800">2024-10-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Paper/">Paper</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Recommend-System/">Recommend System</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/paper2repo%EF%BC%9AGitHub%20Repository%20Recommendation%20for%20Academic%20Papers/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
    </ul>

    <div class="home-paginator">
        <div class="paginator">
    
        <a class="prev btn"
           href="/en/page/12/"
        >Prev</a>
    

    
        <a class="next btn"
           href="/en/page/14/"
        >Next</a>
    
</div>

    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>





<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
