<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="home-content-container fade-in-down-animation">
    
    
    

    <ul class="home-article-list">
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/nlp-prompt-engineering-icl/">
                        NLP (7): Prompt Engineering and In-Context Learning
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>In the era of large language models, how to "converse" with models
has become an art form. The same model can produce dramatically
different results depending on the prompt used. Prompt engineering is
the discipline of designing effective inputs to unlock the best
performance from models. From simple zero-shot prompts to complex
chain-of-thought reasoning, from role assignment to template design,
prompt engineering has become a core skill for working with large
models.</p>
<p>In-Context Learning (ICL) is the theoretical foundation of prompt
engineering. It reveals how models learn from examples, how they
dynamically adjust behavior during inference, and why few-shot prompts
often outperform zero-shot prompts. Understanding these mechanisms not
only helps us write better prompts but also deepens our understanding of
how large language models work.</p>
<p>This article systematically introduces the core concepts and
practical techniques of prompt engineering, including zero-shot,
few-shot, and chain-of-thought prompting, role assignment and formatting
techniques, prompt template design, advanced techniques like
Self-Consistency and ReAct, and demonstrates how to build efficient
prompt systems through practical examples.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sun Jan 19 2025 00:00:00 GMT+0800">2025-01-19</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/LLM/">LLM</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Prompt-Engineering/">Prompt Engineering</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/NLP/">NLP</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/nlp-prompt-engineering-icl/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/nlp-gpt-generative-models/">
                        NLP (6): GPT and Generative Language Models
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>If BERT opened the golden age of understanding-based NLP, then the
GPT series represents the pinnacle of generative NLP. From GPT-1 in 2018
to GPT-4 in 2023, OpenAI has demonstrated through continuously scaling
model size and optimizing training strategies that autoregressive
language models can serve as the foundation for artificial general
intelligence. GPT's success lies not only in its powerful text
generation capabilities but also in demonstrating the magical power of
<strong>In-Context Learning</strong>: models can learn new tasks with
just a few examples without updating parameters.</p>
<p>GPT's core is autoregressive language modeling: given previous
tokens, predict the next token. This seemingly simple objective,
combined with Transformer decoder architecture and large-scale data
training, produces astonishing emergent capabilities. Understanding GPT
is not just key to understanding modern large language models—it's the
starting point for exploring AI general intelligence.</p>
<p>This article provides an in-depth exploration of the GPT series
evolution, principles of autoregressive language modeling, various
decoding strategies, in-context learning mechanisms, and how to evaluate
generation quality. We'll also build a dialogue system through practical
code, demonstrating GPT's powerful capabilities in real
applications.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Dec 28 2024 00:00:00 GMT+0800">2024-12-28</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/NLP/">NLP</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/GPT/">GPT</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/nlp-gpt-generative-models/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/nlp-bert-pretrained-models/">
                        NLP (5): BERT and Pretrained Models
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>In 2018, Google released BERT (Bidirectional Encoder Representations
from Transformers), which fundamentally transformed the field of natural
language processing. Prior to BERT, pretrained models primarily used
unidirectional language modeling (like GPT), which could only leverage
context in one direction. BERT revolutionized NLP by introducing
bidirectional encoder architecture and masked language modeling (MLM),
achieving state-of-the-art performance on 11 NLP tasks and ushering in
the golden age of the "pretrain-finetune" paradigm.</p>
<p>BERT's success lies not only in its architectural innovations but
also in demonstrating that large-scale pretrained models can serve as
universal foundations for NLP tasks. Since BERT, variants like RoBERTa,
ALBERT, and ELECTRA have continuously emerged, each optimizing BERT's
design in different dimensions. Understanding BERT is not just key to
understanding modern NLP—it's the starting point for diving into the era
of large language models.</p>
<p>This article provides an in-depth analysis of BERT's architecture,
training strategies, and finetuning methods, demonstrates practical
usage through HuggingFace code examples, and compares various BERT
variants and their improvements.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Thu Dec 05 2024 00:00:00 GMT+0800">2024-12-05</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/NLP/">NLP</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/BERT/">BERT</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/nlp-bert-pretrained-models/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/pde-theory-of-variational-inference-from-fokker-planck-to-langevin-dynamics/">
                        PDE Theory of Variational Inference: From Fokker-Planck to Langevin Dynamics
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>Probabilistic inference is one of the core problems in machine
learning. Given observed data, we wish to infer the posterior
distribution of latent variables or sample from complex high-dimensional
distributions. Traditional methods fall into two main categories:
<strong>Variational Inference (VI)</strong> approximates the posterior
by optimizing a variational lower bound, while <strong>Markov Chain
Monte Carlo (MCMC)</strong> samples by constructing Markov chains. These
seemingly different approaches reveal profound unity when viewed through
the lens of partial differential equations.</p>
<p>When we use Langevin dynamics for MCMC sampling, particle motion in a
potential field is described by stochastic differential equations, with
probability density evolution governed by the Fokker-Planck equation.
When we optimize the variational lower bound using gradient descent, the
evolution of parameter distributions in Wasserstein space can similarly
be viewed as gradient flows of energy functionals. More remarkably,
<strong>the KL divergence minimization process itself is the solution to
the Fokker-Planck equation</strong>—variational inference and Langevin
MCMC are completely equivalent in the continuous-time limit. This PDE
perspective not only reveals the mathematical essence of probabilistic
inference but also provides a unified theoretical framework for
designing new inference algorithms such as Stein Variational Gradient
Descent.</p>
<p>This article systematically establishes this theoretical framework.
We begin with the Fokker-Planck equation, showing how to formalize the
probability density evolution of stochastic processes as partial
differential equations. We then delve into Langevin dynamics, discussing
overdamped and underdamped cases, and the distinction between Itô and
Stratonovich integrals. Next, we establish the gradient flow
interpretation of KL divergence, proving the equivalence between
variational inference and Langevin MCMC. Finally, we focus on advanced
methods like Stein Variational Gradient Descent, demonstrating how to
solve variational inference problems using particle systems, and
validate theoretical predictions through four complete experiments.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Mon Nov 18 2024 00:00:00 GMT+0800">2024-11-18</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Mathematics-and-Machine-Learning/">Mathematics and Machine Learning</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/PDE/">PDE</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Fokker-Planck/">Fokker-Planck</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/MCMC/">MCMC</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Variational-Inference/">Variational Inference</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Langevin-Dynamics/">Langevin Dynamics</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/pde-theory-of-variational-inference-from-fokker-planck-to-langevin-dynamics/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/en/nlp-attention-transformer/">
                        NLP (4): Attention Mechanism and Transformer
                    </a>
                </h3>

                <div class="home-article-content markdown-body">
                    
                        <p>The Transformer architecture revolutionized natural language
processing by introducing a mechanism that allows models to focus on
relevant parts of the input when processing each element. Unlike
recurrent networks that process sequences step-by-step, Transformers use
attention to capture dependencies regardless of distance, making them
both more powerful and more parallelizable. This article explores the
evolution from basic sequence-to-sequence models to the full Transformer
architecture, diving deep into attention mechanisms, multi-head
attention, positional encoding, and providing complete PyTorch
implementations that you can run and modify.</p>
                    
                </div>

                <div class="home-article-meta-info-container">
    
    
    
    

    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Nov 12 2024 00:00:00 GMT+0800">2024-11-12</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/en/tags/NLP/">NLP</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Transformer/">Transformer</a>&nbsp;
                        </li>
                    
                        <li>
                            | 
                            <a href="/en/tags/Attention/">Attention</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/en/nlp-attention-transformer/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
    </ul>

    <div class="home-paginator">
        <div class="paginator">
    
        <a class="prev btn"
           href="/en/page/5/"
        >Prev</a>
    

    
        <a class="next btn"
           href="/en/page/7/"
        >Next</a>
    
</div>

    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>





<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
