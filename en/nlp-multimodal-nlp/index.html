<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            NLP (11): Multimodal Large Language Models |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">NLP (11): Multimodal Large Language Models</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-03-30 00:00:00</span>
        <span class="mobile">2025-03-30 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/Multimodal/">Multimodal</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>4.2k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>26 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>Humans perceive the world multimodally: we see images, hear sounds,
read text, and these information streams fuse in the brain to form
unified understanding. However, traditional NLP models can only process
text, limiting AI's ability to understand the real world.</p>
<p>Multimodal Large Language Models (MLLMs) attempt to break this
limitation, enabling AI to understand images, audio, video, and text
simultaneously, like humans. But multimodal fusion is far from trivial:
different modalities have vastly different data distributions—how to
align them into a unified representation space? How to design efficient
cross-modal attention mechanisms? How to pretrain multimodal models on
large-scale data?</p>
<p>From CLIP's contrastive learning achieving vision-language alignment,
to BLIP-2's Q-Former enabling parameter-efficient multimodal
pretraining, to GPT-4V demonstrating general visual understanding
capabilities, multimodal technology is rapidly evolving. Audio-text
models like Whisper achieve near-human-level speech recognition, while
video understanding models can analyze complex temporal information.
These technologies not only achieve breakthroughs in academic research
but also demonstrate enormous potential in practical applications—from
intelligent customer service to content creation, from medical diagnosis
to autonomous driving.</p>
<p>This article dives deep into core technologies of multimodal large
language models: from mathematical principles of vision-language
alignment to data strategies for multimodal pretraining, from
implementation details of image captioning and visual question answering
to architectural designs of cutting-edge models like GPT-4V, from
audio-text alignment to video temporal modeling. Each technique includes
runnable code examples, helping readers not only understand principles
but also implement them.</p>
<span id="more"></span>
<h2 id="vision-language-model-fundamentals">Vision-Language Model
Fundamentals</h2>
<h3 id="clip-contrastive-learning-for-vision-language-alignment">CLIP:
Contrastive Learning for Vision-Language Alignment</h3>
<p>CLIP (Contrastive Language-Image Pre-training) is a vision-language
model proposed by OpenAI in 2021. Its core innovation lies in using
large-scale contrastive learning to achieve unified representation of
images and text. CLIP was trained on 400 million image-text pairs,
demonstrating powerful zero-shot capabilities.</p>
<p><strong>Core Idea</strong>:</p>
<p>CLIP's core assumption is: matching image-text pairs should be
semantically similar, hence close in vector space; non-matching pairs
should be far apart. Through contrastive learning, CLIP learns to map
images and text into the same high-dimensional space, bringing
semantically similar image and text vectors close together.</p>
<p><strong>Architecture Design</strong>:</p>
<ul>
<li><strong>Image Encoder</strong>: Vision Transformer (ViT) or ResNet,
encoding images into fixed-dimensional vectors</li>
<li><strong>Text Encoder</strong>: Transformer, encoding text into
vectors of the same dimension</li>
<li><strong>Contrastive Loss</strong>: InfoNCE Loss, maximizing
similarity of matching pairs while minimizing similarity of non-matching
pairs</li>
</ul>
<p><strong>Mathematical Principle</strong>:</p>
<p>Given <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g></svg></mjx-container></span> image-text pairs in a
batch, CLIP first computes similarity matrix between all images and
texts:</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.271ex;" xmlns="http://www.w3.org/2000/svg" width="32.973ex" height="5.523ex" role="img" focusable="false" viewbox="0 -1437.2 14574.1 2441"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mo" transform="translate(1509.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mtext" transform="translate(2564.8,0)"><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(444,0)"/><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(944,0)"/><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(1338,0)"/><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1616,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2172,0)"/></g><g data-mml-node="mo" transform="translate(5180.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(5569.8,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(473,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(6336.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(6781.5,0)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mi" transform="translate(617,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g><g data-mml-node="mo" transform="translate(7739.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(8406.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(9462.3,0)"><g data-mml-node="mrow" transform="translate(1332,754.2)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(473,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(989.2,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="msub" transform="translate(1489.4,0)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mi" transform="translate(617,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mrow" transform="translate(220,-709.5)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(278,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(556,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(473,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1323,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(1601,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(2101.2,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="mo" transform="translate(2601.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(2879.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(3157.4,0)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mi" transform="translate(617,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g><g data-mml-node="mo" transform="translate(4115.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(4393.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g></g><rect width="4871.7" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p>
<p>where <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.902ex" role="img" focusable="false" viewbox="0 -683 767 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(473,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span> is the embedding of
the <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewbox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span>-th image and <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="2.168ex" height="2.197ex" role="img" focusable="false" viewbox="0 -677 958.3 971.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mi" transform="translate(617,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></svg></mjx-container></span> is the embedding of the <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewbox="0 -661 412 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></svg></mjx-container></span>-th text. Diagonal elements <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.679ex" height="1.952ex" role="img" focusable="false" viewbox="0 -705 1183.9 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></g></svg></mjx-container></span> are similarities of matching
pairs, off-diagonal elements are similarities of non-matching pairs.</p>
<p>The contrastive loss function contains two symmetric terms:</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.053ex;" xmlns="http://www.w3.org/2000/svg" width="63.499ex" height="6.974ex" role="img" focusable="false" viewbox="0 -1733 28066.5 3082.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"/></g></g><g data-mml-node="mo" transform="translate(967.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mo" transform="translate(2023.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mfrac" transform="translate(2801.6,0)"><g data-mml-node="mn" transform="translate(414,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><rect width="1088" height="60" x="120" y="220"/></g><g data-mml-node="munderover" transform="translate(4296.2,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(408,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g><g data-mml-node="mi" transform="translate(5906.9,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(7184.9,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(7351.6,0)"><g data-mml-node="mrow" transform="translate(1494,710)"><g data-mml-node="mi"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"/><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"/></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1917,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3100.9,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mi" transform="translate(3600.9,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g><g data-mml-node="mo" transform="translate(4117.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-920)"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g><g data-mml-node="mo" transform="translate(412,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1190,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mi" transform="translate(2500.7,0)"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"/><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"/></g><g data-mml-node="mo" transform="translate(4028.7,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(4028.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(4417.7,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5649,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mi" transform="translate(6149,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g><g data-mml-node="mo" transform="translate(6666,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><rect width="7255" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(15068.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mfrac" transform="translate(16069,0)"><g data-mml-node="mn" transform="translate(414,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><rect width="1088" height="60" x="120" y="220"/></g><g data-mml-node="munderover" transform="translate(17563.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(124.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g><g data-mml-node="mo" transform="translate(412,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1190,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(408,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g><g data-mml-node="mi" transform="translate(19174.3,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(20452.3,0)"><path data-c="2061" d=""/></g><g data-mml-node="mfrac" transform="translate(20619,0)"><g data-mml-node="mrow" transform="translate(1423,754.2)"><g data-mml-node="mi"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"/><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"/></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1917,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g><g data-mml-node="mi" transform="translate(412,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3195.7,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mi" transform="translate(3695.7,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g><g data-mml-node="mo" transform="translate(4212.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-920)"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mi" transform="translate(2453.3,0)"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"/><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"/></g><g data-mml-node="mo" transform="translate(3981.3,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(3981.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(4370.3,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5601.6,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mi" transform="translate(6101.6,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g><g data-mml-node="mo" transform="translate(6618.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><rect width="7207.6" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p>
<p>The first term is image-to-text contrastive loss, the second is
text-to-image contrastive loss. Temperature parameter <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex;" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewbox="0 -431 517 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g></g></g></svg></mjx-container></span> controls distribution sharpness,
typically set to 0.07.</p>
<p><strong>Why It Works</strong>:</p>
<p>CLIP's success lies in combining large-scale data with contrastive
learning. Training on massive image-text pairs, the model learns rich
vision-language correspondences. Contrastive learning avoids expensive
manual annotation costs, requiring only image-text pairs for training.
This enables CLIP to handle unseen tasks during training, demonstrating
powerful zero-shot capabilities.</p>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPProcessor, CLIPModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load pretrained CLIP model</span></span><br><span class="line">model = CLIPModel.from_pretrained(<span class="string">"openai/clip-vit-base-patch32"</span>)</span><br><span class="line">processor = CLIPProcessor.from_pretrained(<span class="string">"openai/clip-vit-base-patch32"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Image and texts</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">"cat.jpg"</span>)</span><br><span class="line">texts = [<span class="string">"a photo of a cat"</span>, <span class="string">"a photo of a dog"</span>, <span class="string">"a photo of a bird"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process inputs</span></span><br><span class="line">inputs = processor(</span><br><span class="line">    text=texts,</span><br><span class="line">    images=image,</span><br><span class="line">    return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">    padding=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get features</span></span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">image_embeds = outputs.image_embeds</span><br><span class="line">text_embeds = outputs.text_embeds</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute similarity</span></span><br><span class="line">similarities = (image_embeds @ text_embeds.T).softmax(dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Most matching text index: <span class="subst">{similarities.argmax().item()}</span>"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Applications</strong>:</p>
<ul>
<li><strong>Zero-shot Image Classification</strong>: Classify images
without training</li>
<li><strong>Image Retrieval</strong>: Retrieve relevant images based on
text descriptions</li>
<li><strong>Image Generation Guidance</strong>: Provide text-image
alignment for generative models</li>
</ul>
<h3 id="blip-unified-vision-language-understanding-and-generation">BLIP:
Unified Vision-Language Understanding and Generation</h3>
<p>BLIP (Bootstrapping Language-Image Pre-training) is a unified
vision-language model proposed by Salesforce that can simultaneously
perform understanding and generation tasks.</p>
<p><strong>Architecture Features</strong>:</p>
<p>BLIP uses a multi-task learning framework with three modules:</p>
<ol type="1">
<li><strong>Unimodal Encoders</strong>: Encode images and text
separately</li>
<li><strong>Image-Text Cross-Attention Encoder</strong>: Fuse multimodal
information</li>
<li><strong>Image-Text Decoder</strong>: Generate text descriptions</li>
</ol>
<p><strong>Pretraining Tasks</strong>:</p>
<ul>
<li><strong>Image-Text Contrastive Learning</strong> (ITC): Align image
and text representations</li>
<li><strong>Image-Text Matching</strong> (ITM): Determine if image-text
pairs match</li>
<li><strong>Image-Conditioned Language Modeling</strong> (ITG): Generate
image descriptions</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForConditionalGeneration</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load BLIP model</span></span><br><span class="line">processor = BlipProcessor.from_pretrained(<span class="string">"Salesforce/blip-image-captioning-base"</span>)</span><br><span class="line">model = BlipForConditionalGeneration.from_pretrained(<span class="string">"Salesforce/blip-image-captioning-base"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Image captioning</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">"scene.jpg"</span>)</span><br><span class="line">inputs = processor(image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">out = model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">caption = processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Generated caption: <span class="subst">{caption}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visual Question Answering</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipForQuestionAnswering</span><br><span class="line"></span><br><span class="line">qa_model = BlipForQuestionAnswering.from_pretrained(<span class="string">"Salesforce/blip-vqa-base"</span>)</span><br><span class="line">question = <span class="string">"What is in the image?"</span></span><br><span class="line">inputs = processor(image, question, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">out = qa_model.generate(**inputs)</span><br><span class="line">answer = processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Answer: <span class="subst">{answer}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="blip-2-parameter-efficient-multimodal-pretraining">BLIP-2:
Parameter-Efficient Multimodal Pretraining</h3>
<p>BLIP-2 is an upgraded version of BLIP, with core innovation in
introducing Q-Former (Query Transformer) as a "bridge" between image
encoder and language model. This design enables BLIP-2 to freeze
pretrained image encoders and LLMs, training only about 188M parameters
in Q-Former, dramatically reducing training costs.</p>
<p><strong>Core Innovation</strong>:</p>
<ol type="1">
<li><p><strong>Frozen Pretrained Models</strong>: Image encoders (e.g.,
ViT) and language models (e.g., OPT, LLaMA) remain frozen, parameters
not updated. This avoids degrading pretrained model performance while
significantly reducing trainable parameters.</p></li>
<li><p><strong>Two-Stage Pretraining Strategy</strong>:</p>
<ul>
<li><strong>Stage 1: Vision-Language Representation Learning</strong>:
Q-Former learns to extract most relevant visual features from frozen
image encoder. Through three tasks—image-text contrastive learning,
image-text matching, and image-conditioned language modeling—Q-Former
learns to convert image features into formats understandable by language
models.</li>
<li><strong>Stage 2: Vision-to-Language Generation Learning</strong>:
Q-Former learns to align with frozen LLM, using extracted visual
features as "soft prompts" input to LLM, enabling LLM to generate text
based on visual information.</li>
</ul></li>
</ol>
<p><strong>Q-Former Architecture Explained</strong>:</p>
<p>Q-Former contains a set of learnable query embeddings, typically 32
in number. These query vectors work through the following
mechanisms:</p>
<ul>
<li><strong>Cross-Attention</strong>: Query vectors interact with image
features through cross-attention, extracting relevant information from
images</li>
<li><strong>Self-Attention</strong>: Query vectors learn relationships
between queries through self-attention, forming global understanding of
images</li>
<li><strong>Feedforward Network</strong>: Non-linear transformation of
query vectors, enhancing representation capability</li>
</ul>
<p>The advantage of this design: the number of query vectors is far
fewer than image patches (e.g., 32 vs 256), dramatically reducing
feature dimensions to process, improving efficiency.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Blip2Processor, Blip2ForConditionalGeneration</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">processor = Blip2Processor.from_pretrained(<span class="string">"Salesforce/blip2-opt-2.7b"</span>)</span><br><span class="line">model = Blip2ForConditionalGeneration.from_pretrained(</span><br><span class="line">    <span class="string">"Salesforce/blip2-opt-2.7b"</span>,</span><br><span class="line">    torch_dtype=torch.float16,</span><br><span class="line">    device_map=<span class="string">"auto"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">"image.jpg"</span>)</span><br><span class="line">prompt = <span class="string">"Question: What is this? Answer:"</span></span><br><span class="line">inputs = processor(image, text=prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line">generated_ids = model.generate(**inputs, max_new_tokens=<span class="number">50</span>)</span><br><span class="line">generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(generated_text)</span><br></pre></td></tr></table></figure>
<p><strong>Advantages</strong>:</p>
<ul>
<li><strong>Parameter Efficiency</strong>: Only need to train Q-Former
(~188M parameters), while image encoder and LLM remain frozen</li>
<li><strong>Flexibility</strong>: Can easily adapt to different LLMs
(e.g., OPT, Flan-T5, LLaMA)</li>
<li><strong>Performance</strong>: Achieves SOTA on multiple
vision-language tasks</li>
</ul>
<h2 id="multimodal-pretraining-strategies">Multimodal Pretraining
Strategies</h2>
<h3 id="data-construction">Data Construction</h3>
<p>Multimodal pretraining requires large-scale image-text pair data:</p>
<p><strong>Common Datasets</strong>:</p>
<ul>
<li><strong>LAION</strong>: Billions of image-text pairs</li>
<li><strong>CC (Common Crawl)</strong>: Image-text pairs scraped from
web pages</li>
<li><strong>COCO</strong>: High-quality annotated image captioning
dataset</li>
<li><strong>Visual Genome</strong>: Image dataset with detailed visual
relationships</li>
</ul>
<p><strong>Data Cleaning Strategy</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPProcessor, CLIPModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use CLIP for data filtering</span></span><br><span class="line">clip_model = CLIPModel.from_pretrained(<span class="string">"openai/clip-vit-base-patch32"</span>)</span><br><span class="line">clip_processor = CLIPProcessor.from_pretrained(<span class="string">"openai/clip-vit-base-patch32"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">filter_image_text_pair</span>(<span class="params">image_path, text, threshold=<span class="number">0.25</span></span>):</span><br><span class="line">    <span class="string">"""Filter low-quality image-text pairs"""</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">    inputs = clip_processor(</span><br><span class="line">        text=[text],</span><br><span class="line">        images=image,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span></span><br><span class="line">    )</span><br><span class="line">    outputs = clip_model(**inputs)</span><br><span class="line">    similarity = (outputs.image_embeds @ outputs.text_embeds.T).item()</span><br><span class="line">    <span class="keyword">return</span> similarity &gt;= threshold</span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch filtering</span></span><br><span class="line">filtered_pairs = []</span><br><span class="line"><span class="keyword">for</span> img_path, text <span class="keyword">in</span> image_text_pairs:</span><br><span class="line">    <span class="keyword">if</span> filter_image_text_pair(img_path, text):</span><br><span class="line">        filtered_pairs.append((img_path, text))</span><br></pre></td></tr></table></figure>
<h3 id="pretraining-objectives">Pretraining Objectives</h3>
<p><strong>Multi-Task Learning</strong>:</p>
<p>Simultaneously optimize multiple objectives to learn richer
representations:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultimodalPretrainingLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, temperature=<span class="number">0.07</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.temperature = temperature</span><br><span class="line">        self.itc_loss = nn.CrossEntropyLoss()</span><br><span class="line">        self.itm_loss = nn.CrossEntropyLoss()</span><br><span class="line">        self.lm_loss = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image_embeds, text_embeds, itm_logits, lm_logits, labels</span>):</span><br><span class="line">        <span class="comment"># Image-text contrastive loss</span></span><br><span class="line">        logits = image_embeds @ text_embeds.T / self.temperature</span><br><span class="line">        labels = torch.arange(logits.size(<span class="number">0</span>), device=logits.device)</span><br><span class="line">        itc_loss = (self.itc_loss(logits, labels) + </span><br><span class="line">                   self.itc_loss(logits.T, labels)) / <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Image-text matching loss</span></span><br><span class="line">        itm_loss = self.itm_loss(itm_logits, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Language modeling loss</span></span><br><span class="line">        lm_loss = self.lm_loss(lm_logits.view(-<span class="number">1</span>, lm_logits.size(-<span class="number">1</span>)), </span><br><span class="line">                               labels.view(-<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        total_loss = itc_loss + itm_loss + lm_loss</span><br><span class="line">        <span class="keyword">return</span> total_loss, {</span><br><span class="line">            <span class="string">"itc_loss"</span>: itc_loss.item(),</span><br><span class="line">            <span class="string">"itm_loss"</span>: itm_loss.item(),</span><br><span class="line">            <span class="string">"lm_loss"</span>: lm_loss.item()</span><br><span class="line">        }</span><br></pre></td></tr></table></figure>
<p><strong>Curriculum Learning</strong>:</p>
<p>Train progressively from simple to complex:</p>
<ol type="1">
<li><strong>Stage 1</strong>: Image-text alignment (ITC)</li>
<li><strong>Stage 2</strong>: Image-text matching (ITM)</li>
<li><strong>Stage 3</strong>: Image-conditioned generation (ITG)</li>
</ol>
<h2 id="image-captioning-and-visual-question-answering">Image Captioning
and Visual Question Answering</h2>
<h3 id="image-captioning">Image Captioning</h3>
<p>Image captioning is a fundamental task in multimodal
understanding.</p>
<p><strong>Evaluation Metrics</strong>:</p>
<ul>
<li><strong>BLEU</strong>: Based on n-gram overlap</li>
<li><strong>METEOR</strong>: Considers synonyms and word order</li>
<li><strong>CIDEr</strong>: Specifically designed for image
captioning</li>
<li><strong>SPICE</strong>: Semantic similarity based on scene
graphs</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForConditionalGeneration</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageCaptioner</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"Salesforce/blip-image-captioning-large"</span></span>):</span><br><span class="line">        self.processor = BlipProcessor.from_pretrained(model_name)</span><br><span class="line">        self.model = BlipForConditionalGeneration.from_pretrained(model_name)</span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_caption</span>(<span class="params">self, image_path, max_length=<span class="number">50</span>, num_beams=<span class="number">3</span></span>):</span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        inputs = self.processor(image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_length=max_length,</span><br><span class="line">                num_beams=num_beams,</span><br><span class="line">                early_stopping=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        caption = self.processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> caption</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_diverse_captions</span>(<span class="params">self, image_path, num_return_sequences=<span class="number">3</span></span>):</span><br><span class="line">        <span class="string">"""Generate diverse captions"""</span></span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        inputs = self.processor(image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = self.model.generate(</span><br><span class="line">                **inputs,</span><br><span class="line">                max_length=<span class="number">50</span>,</span><br><span class="line">                num_beams=<span class="number">5</span>,</span><br><span class="line">                num_return_sequences=num_return_sequences,</span><br><span class="line">                do_sample=<span class="literal">True</span>,</span><br><span class="line">                temperature=<span class="number">0.8</span></span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        captions = [self.processor.decode(o, skip_special_tokens=<span class="literal">True</span>) </span><br><span class="line">                   <span class="keyword">for</span> o <span class="keyword">in</span> out]</span><br><span class="line">        <span class="keyword">return</span> captions</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">captioner = ImageCaptioner()</span><br><span class="line">caption = captioner.generate_caption(<span class="string">"image.jpg"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Caption: <span class="subst">{caption}</span>"</span>)</span><br><span class="line"></span><br><span class="line">diverse_captions = captioner.generate_diverse_captions(<span class="string">"image.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> i, cap <span class="keyword">in</span> <span class="built_in">enumerate</span>(diverse_captions, <span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Caption <span class="subst">{i}</span>: <span class="subst">{cap}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="visual-question-answering-vqa">Visual Question Answering
(VQA)</h3>
<p>Visual Question Answering requires models to understand image content
and answer natural language questions.</p>
<p><strong>Datasets</strong>:</p>
<ul>
<li><strong>VQA v2</strong>: Contains 200K+ images, 1.1M+ questions</li>
<li><strong>GQA</strong>: Scene graph-enhanced visual question
answering</li>
<li><strong>TextVQA</strong>: Visual question answering with text</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForQuestionAnswering</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VQASystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"Salesforce/blip-vqa-base"</span></span>):</span><br><span class="line">        self.processor = BlipProcessor.from_pretrained(model_name)</span><br><span class="line">        self.model = BlipForQuestionAnswering.from_pretrained(model_name)</span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">self, image_path, question</span>):</span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        inputs = self.processor(image, question, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = self.model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">        </span><br><span class="line">        answer = self.processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> answer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">batch_answer</span>(<span class="params">self, image_path, questions</span>):</span><br><span class="line">        <span class="string">"""Batch answer questions"""</span></span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        answers = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> question <span class="keyword">in</span> questions:</span><br><span class="line">            inputs = self.processor(image, question, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                out = self.model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">            answer = self.processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">            answers.append(answer)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> answers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">vqa = VQASystem()</span><br><span class="line">answer = vqa.answer_question(<span class="string">"image.jpg"</span>, <span class="string">"What color is the car?"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Answer: <span class="subst">{answer}</span>"</span>)</span><br><span class="line"></span><br><span class="line">questions = [</span><br><span class="line">    <span class="string">"What is in the image?"</span>,</span><br><span class="line">    <span class="string">"How many people are there?"</span>,</span><br><span class="line">    <span class="string">"What is the weather like?"</span></span><br><span class="line">]</span><br><span class="line">answers = vqa.batch_answer(<span class="string">"image.jpg"</span>, questions)</span><br><span class="line"><span class="keyword">for</span> q, a <span class="keyword">in</span> <span class="built_in">zip</span>(questions, answers):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Q: <span class="subst">{q}</span>\nA: <span class="subst">{a}</span>\n"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="gpt-4v-and-multimodal-llms">GPT-4V and Multimodal LLMs</h2>
<h3 id="gpt-4v-architecture">GPT-4V Architecture</h3>
<p>GPT-4V (GPT-4 Vision) is OpenAI's multimodal large language model
capable of understanding images and generating text responses.</p>
<p><strong>Core Capabilities</strong>:</p>
<ul>
<li><strong>Image Understanding</strong>: Recognize objects, scenes,
text, charts, etc.</li>
<li><strong>Multi-turn Dialogue</strong>: Support mixed image and text
inputs</li>
<li><strong>Complex Reasoning</strong>: Capable of visual reasoning and
logical analysis</li>
</ul>
<p><strong>Usage Example</strong> (API call):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt4v_chat</span>(<span class="params">image_path, text_prompt</span>):</span><br><span class="line">    <span class="string">"""Use GPT-4V for multimodal dialogue"""</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(image_path, <span class="string">"rb"</span>) <span class="keyword">as</span> image_file:</span><br><span class="line">        response = client.chat.completions.create(</span><br><span class="line">            model=<span class="string">"gpt-4-vision-preview"</span>,</span><br><span class="line">            messages=[</span><br><span class="line">                {</span><br><span class="line">                    <span class="string">"role"</span>: <span class="string">"user"</span>,</span><br><span class="line">                    <span class="string">"content"</span>: [</span><br><span class="line">                        {<span class="string">"type"</span>: <span class="string">"text"</span>, <span class="string">"text"</span>: text_prompt},</span><br><span class="line">                        {</span><br><span class="line">                            <span class="string">"type"</span>: <span class="string">"image_url"</span>,</span><br><span class="line">                            <span class="string">"image_url"</span>: {</span><br><span class="line">                                <span class="string">"url"</span>: <span class="string">f"data:image/jpeg;base64,<span class="subst">{image_file.read()}</span>"</span></span><br><span class="line">                            }</span><br><span class="line">                        }</span><br><span class="line">                    ]</span><br><span class="line">                }</span><br><span class="line">            ],</span><br><span class="line">            max_tokens=<span class="number">300</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> response.choices[<span class="number">0</span>].message.content</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">response = gpt4v_chat(<span class="string">"chart.png"</span>, <span class="string">"Analyze the main trends in this chart"</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<h3 id="open-source-multimodal-llms">Open-Source Multimodal LLMs</h3>
<p><strong>LLaVA (Large Language and Vision Assistant)</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llava.model.builder <span class="keyword">import</span> load_pretrained_model</span><br><span class="line"><span class="keyword">from</span> llava.utils <span class="keyword">import</span> disable_torch_init</span><br><span class="line"><span class="keyword">from</span> llava.conversation <span class="keyword">import</span> conv_templates, SeparatorStyle</span><br><span class="line"><span class="keyword">from</span> llava.mm_utils <span class="keyword">import</span> tokenizer_image_token, get_model_name_from_path</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_llava_model</span>(<span class="params">model_path=<span class="string">"liuhaotian/llava-v1.5-7b"</span></span>):</span><br><span class="line">    disable_torch_init()</span><br><span class="line">    model_name = get_model_name_from_path(model_path)</span><br><span class="line">    tokenizer, model, image_processor, context_len = load_pretrained_model(</span><br><span class="line">        model_path, <span class="literal">None</span>, model_name</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> tokenizer, model, image_processor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">llava_chat</span>(<span class="params">image_path, question, tokenizer, model, image_processor</span>):</span><br><span class="line">    <span class="string">"""Use LLaVA for visual question answering"""</span></span><br><span class="line">    conv_mode = <span class="string">"llava_v1"</span></span><br><span class="line">    conv = conv_templates[conv_mode].copy()</span><br><span class="line">    </span><br><span class="line">    image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">    image_tensor = image_processor.preprocess(image, return_tensors=<span class="string">"pt"</span>)[<span class="string">"pixel_values"</span>][<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build conversation</span></span><br><span class="line">    conv.append_message(conv.roles[<span class="number">0</span>], <span class="string">f"&lt;image&gt;\n<span class="subst">{question}</span>"</span>)</span><br><span class="line">    conv.append_message(conv.roles[<span class="number">1</span>], <span class="literal">None</span>)</span><br><span class="line">    prompt = conv.get_prompt()</span><br><span class="line">    </span><br><span class="line">    input_ids = tokenizer_image_token(</span><br><span class="line">        prompt, tokenizer, IMAGE_TOKEN_INDEX=IMAGE_TOKEN_INDEX, return_tensors=<span class="string">"pt"</span></span><br><span class="line">    ).unsqueeze(<span class="number">0</span>).cuda()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        output_ids = model.generate(</span><br><span class="line">            input_ids,</span><br><span class="line">            images=image_tensor.unsqueeze(<span class="number">0</span>).half().cuda(),</span><br><span class="line">            do_sample=<span class="literal">True</span>,</span><br><span class="line">            temperature=<span class="number">0.2</span>,</span><br><span class="line">            top_p=<span class="number">0.7</span>,</span><br><span class="line">            num_beams=<span class="number">1</span>,</span><br><span class="line">            max_new_tokens=<span class="number">512</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p><strong>MiniGPT-4</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> minigpt4.models <span class="keyword">import</span> load_pre-trained_minigpt4</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_minigpt4</span>():</span><br><span class="line">    model = load_pre-trained_minigpt4(</span><br><span class="line">        llama_model=<span class="string">"path/to/vicuna"</span>,</span><br><span class="line">        pretrained_ckpt=<span class="string">"path/to/pretrained_minigpt4.pth"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minigpt4_chat</span>(<span class="params">image_path, question, model</span>):</span><br><span class="line">    <span class="string">"""Use MiniGPT-4 for dialogue"""</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">    response = model.generate(image, question, max_new_tokens=<span class="number">300</span>)</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>
<h2 id="audio-text-models">Audio-Text Models</h2>
<h3 id="whisper-large-scale-speech-recognition">Whisper: Large-Scale
Speech Recognition</h3>
<p>Whisper is a multilingual speech recognition model developed by
OpenAI that supports speech-to-text conversion in multiple
languages.</p>
<p><strong>Features</strong>:</p>
<ul>
<li><strong>Multilingual Support</strong>: Supports 99 languages</li>
<li><strong>Robustness</strong>: Good adaptability to background noise,
accents, and dialects</li>
<li><strong>Zero-shot Capability</strong>: Can handle new languages
without fine-tuning</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> whisper</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WhisperASR</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_size=<span class="string">"base"</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        model_size: tiny, base, small, medium, large</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = whisper.load_model(model_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transcribe</span>(<span class="params">self, audio_path, language=<span class="literal">None</span>, task=<span class="string">"transcribe"</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        task: "transcribe" or "translate" (translate to English)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.model.transcribe(</span><br><span class="line">            audio_path,</span><br><span class="line">            language=language,</span><br><span class="line">            task=task,</span><br><span class="line">            verbose=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transcribe_with_timestamps</span>(<span class="params">self, audio_path</span>):</span><br><span class="line">        <span class="string">"""Transcription with timestamps"""</span></span><br><span class="line">        result = self.model.transcribe(</span><br><span class="line">            audio_path,</span><br><span class="line">            word_timestamps=<span class="literal">True</span>,</span><br><span class="line">            verbose=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">batch_transcribe</span>(<span class="params">self, audio_paths</span>):</span><br><span class="line">        <span class="string">"""Batch transcription"""</span></span><br><span class="line">        results = []</span><br><span class="line">        <span class="keyword">for</span> audio_path <span class="keyword">in</span> audio_paths:</span><br><span class="line">            result = self.transcribe(audio_path)</span><br><span class="line">            results.append(result)</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">asr = WhisperASR(model_size=<span class="string">"base"</span>)</span><br><span class="line">result = asr.transcribe(<span class="string">"audio.mp3"</span>, language=<span class="string">"zh"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Transcription: <span class="subst">{result[<span class="string">'text'</span>]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># With timestamps</span></span><br><span class="line">result_with_ts = asr.transcribe_with_timestamps(<span class="string">"audio.mp3"</span>)</span><br><span class="line"><span class="keyword">for</span> segment <span class="keyword">in</span> result_with_ts[<span class="string">"segments"</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"[<span class="subst">{segment[<span class="string">'start'</span>]:<span class="number">.2</span>f}</span>s - <span class="subst">{segment[<span class="string">'end'</span>]:<span class="number">.2</span>f}</span>s] <span class="subst">{segment[<span class="string">'text'</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Advanced Features</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transcribe_with_vad</span>(<span class="params">audio_path, model</span>):</span><br><span class="line">    <span class="string">"""Transcribe with Voice Activity Detection (VAD) for segmentation"""</span></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Load audio</span></span><br><span class="line">    sample_rate, audio = wavfile.read(audio_path)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># VAD detection (simplified example)</span></span><br><span class="line">    <span class="comment"># In practice, can use libraries like webrtcvad</span></span><br><span class="line">    segments = detect_speech_segments(audio, sample_rate)</span><br><span class="line">    </span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> segments:</span><br><span class="line">        segment_audio = audio[<span class="built_in">int</span>(start*sample_rate):<span class="built_in">int</span>(end*sample_rate)]</span><br><span class="line">        <span class="comment"># Save temporary file</span></span><br><span class="line">        temp_path = <span class="string">"temp_segment.wav"</span></span><br><span class="line">        wavfile.write(temp_path, sample_rate, segment_audio)</span><br><span class="line">        </span><br><span class="line">        result = model.transcribe(temp_path)</span><br><span class="line">        results.append({</span><br><span class="line">            <span class="string">"start"</span>: start,</span><br><span class="line">            <span class="string">"end"</span>: end,</span><br><span class="line">            <span class="string">"text"</span>: result[<span class="string">"text"</span>]</span><br><span class="line">        })</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h3 id="audio-text-alignment">Audio-Text Alignment</h3>
<p><strong>Wav2Vec2 + BERT</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Wav2Vec2Processor, Wav2Vec2Model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchaudio</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AudioTextAlignment</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.audio_processor = Wav2Vec2Processor.from_pretrained(<span class="string">"facebook/wav2vec2-base"</span>)</span><br><span class="line">        self.audio_model = Wav2Vec2Model.from_pretrained(<span class="string">"facebook/wav2vec2-base"</span>)</span><br><span class="line">        self.text_tokenizer = AutoTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line">        self.text_model = AutoModel.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">align</span>(<span class="params">self, audio_path, text</span>):</span><br><span class="line">        <span class="string">"""Align audio and text"""</span></span><br><span class="line">        <span class="comment"># Process audio</span></span><br><span class="line">        waveform, sample_rate = torchaudio.load(audio_path)</span><br><span class="line">        inputs = self.audio_processor(</span><br><span class="line">            waveform.squeeze().numpy(),</span><br><span class="line">            sampling_rate=sample_rate,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span></span><br><span class="line">        )</span><br><span class="line">        audio_features = self.audio_model(**inputs).last_hidden_state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Process text</span></span><br><span class="line">        text_inputs = self.text_tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>)</span><br><span class="line">        text_features = self.text_model(**text_inputs).last_hidden_state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute alignment (simplified example)</span></span><br><span class="line">        alignment = self.compute_alignment(audio_features, text_features)</span><br><span class="line">        <span class="keyword">return</span> alignment</span><br></pre></td></tr></table></figure>
<h2 id="video-understanding">Video Understanding</h2>
<h3 id="video-encoding">Video Encoding</h3>
<p><strong>Video-ChatGPT</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> VideoChatGPTProcessor, VideoChatGPTModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VideoUnderstanding</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"MBZUAI/Video-ChatGPT"</span></span>):</span><br><span class="line">        self.processor = VideoChatGPTProcessor.from_pretrained(model_name)</span><br><span class="line">        self.model = VideoChatGPTModel.from_pretrained(model_name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">understand_video</span>(<span class="params">self, video_path, question</span>):</span><br><span class="line">        <span class="string">"""Understand video content and answer questions"""</span></span><br><span class="line">        <span class="comment"># Load video (needs preprocessing into frame sequence)</span></span><br><span class="line">        video_frames = self.load_video_frames(video_path)</span><br><span class="line">        </span><br><span class="line">        inputs = self.processor(</span><br><span class="line">            text=question,</span><br><span class="line">            videos=video_frames,</span><br><span class="line">            return_tensors=<span class="string">"pt"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs = self.model.generate(**inputs)</span><br><span class="line">        answer = self.processor.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> answer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_video_frames</span>(<span class="params">self, video_path, num_frames=<span class="number">8</span></span>):</span><br><span class="line">        <span class="string">"""Extract frames from video"""</span></span><br><span class="line">        <span class="keyword">import</span> cv2</span><br><span class="line">        cap = cv2.VideoCapture(video_path)</span><br><span class="line">        frames = []</span><br><span class="line">        </span><br><span class="line">        total_frames = <span class="built_in">int</span>(cap.get(cv2.CAP_PROP_FRAME_COUNT))</span><br><span class="line">        frame_indices = [<span class="built_in">int</span>(i * total_frames / num_frames) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_frames)]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> frame_indices:</span><br><span class="line">            cap.<span class="built_in">set</span>(cv2.CAP_PROP_POS_FRAMES, idx)</span><br><span class="line">            ret, frame = cap.read()</span><br><span class="line">            <span class="keyword">if</span> ret:</span><br><span class="line">                frames.append(frame)</span><br><span class="line">        </span><br><span class="line">        cap.release()</span><br><span class="line">        <span class="keyword">return</span> frames</span><br></pre></td></tr></table></figure>
<h3 id="temporal-modeling">Temporal Modeling</h3>
<p><strong>Video-LLaMA</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VideoLLaMA</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Load video encoder and LLM</span></span><br><span class="line">        self.video_encoder = self.load_video_encoder()</span><br><span class="line">        self.llm = self.load_llm()</span><br><span class="line">        self.projector = self.load_projector()  <span class="comment"># Connect video features to LLM</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_video</span>(<span class="params">self, video_path, question</span>):</span><br><span class="line">        <span class="string">"""Process video and generate answer"""</span></span><br><span class="line">        <span class="comment"># Extract video features</span></span><br><span class="line">        video_features = self.video_encoder(video_path)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Project to LLM space</span></span><br><span class="line">        projected_features = self.projector(video_features)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate answer</span></span><br><span class="line">        prompt = self.build_prompt(question, projected_features)</span><br><span class="line">        answer = self.llm.generate(prompt)</span><br><span class="line">        <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure>
<h2 id="practical-building-multimodal-applications">Practical: Building
Multimodal Applications</h2>
<h3 id="multimodal-retrieval-system">Multimodal Retrieval System</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPProcessor, CLIPModel</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultimodalRetrievalSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">"openai/clip-vit-base-patch32"</span></span>):</span><br><span class="line">        self.model = CLIPModel.from_pretrained(model_name)</span><br><span class="line">        self.processor = CLIPProcessor.from_pretrained(model_name)</span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        </span><br><span class="line">        self.image_embeddings = []</span><br><span class="line">        self.text_embeddings = []</span><br><span class="line">        self.image_paths = []</span><br><span class="line">        self.texts = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_image</span>(<span class="params">self, image_path</span>):</span><br><span class="line">        <span class="string">"""Add image to index"""</span></span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        inputs = self.processor(images=image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            image_embed = self.model.get_image_features(**inputs)</span><br><span class="line">            image_embed = image_embed / image_embed.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.image_embeddings.append(image_embed.cpu().numpy())</span><br><span class="line">        self.image_paths.append(image_path)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_text</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">"""Add text to index"""</span></span><br><span class="line">        inputs = self.processor(text=[text], return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            text_embed = self.model.get_text_features(**inputs)</span><br><span class="line">            text_embed = text_embed / text_embed.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.text_embeddings.append(text_embed.cpu().numpy())</span><br><span class="line">        self.texts.append(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search_by_text</span>(<span class="params">self, query_text, top_k=<span class="number">5</span></span>):</span><br><span class="line">        <span class="string">"""Search images by text query"""</span></span><br><span class="line">        inputs = self.processor(text=[query_text], return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            query_embed = self.model.get_text_features(**inputs)</span><br><span class="line">            query_embed = query_embed / query_embed.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.image_embeddings) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        image_embeds = np.vstack(self.image_embeddings)</span><br><span class="line">        similarities = cosine_similarity(</span><br><span class="line">            query_embed.cpu().numpy(),</span><br><span class="line">            image_embeds</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        top_indices = np.argsort(similarities)[::-<span class="number">1</span>][:top_k]</span><br><span class="line">        results = [</span><br><span class="line">            {</span><br><span class="line">                <span class="string">"image_path"</span>: self.image_paths[i],</span><br><span class="line">                <span class="string">"similarity"</span>: <span class="built_in">float</span>(similarities[i])</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> top_indices</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search_by_image</span>(<span class="params">self, query_image_path, top_k=<span class="number">5</span></span>):</span><br><span class="line">        <span class="string">"""Search texts by image query"""</span></span><br><span class="line">        image = Image.<span class="built_in">open</span>(query_image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">        inputs = self.processor(images=image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            query_embed = self.model.get_image_features(**inputs)</span><br><span class="line">            query_embed = query_embed / query_embed.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.text_embeddings) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        text_embeds = np.vstack(self.text_embeddings)</span><br><span class="line">        similarities = cosine_similarity(</span><br><span class="line">            query_embed.cpu().numpy(),</span><br><span class="line">            text_embeds</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        top_indices = np.argsort(similarities)[::-<span class="number">1</span>][:top_k]</span><br><span class="line">        results = [</span><br><span class="line">            {</span><br><span class="line">                <span class="string">"text"</span>: self.texts[i],</span><br><span class="line">                <span class="string">"similarity"</span>: <span class="built_in">float</span>(similarities[i])</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> top_indices</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">retrieval_system = MultimodalRetrievalSystem()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add images and texts</span></span><br><span class="line">retrieval_system.add_image(<span class="string">"image1.jpg"</span>)</span><br><span class="line">retrieval_system.add_image(<span class="string">"image2.jpg"</span>)</span><br><span class="line">retrieval_system.add_text(<span class="string">"a cat sitting on a mat"</span>)</span><br><span class="line">retrieval_system.add_text(<span class="string">"a dog playing in the park"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Text query images</span></span><br><span class="line">results = retrieval_system.search_by_text(<span class="string">"a cute animal"</span>, top_k=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Image: <span class="subst">{r[<span class="string">'image_path'</span>]}</span>, Similarity: <span class="subst">{r[<span class="string">'similarity'</span>]:<span class="number">.3</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Image query texts</span></span><br><span class="line">results = retrieval_system.search_by_image(<span class="string">"query.jpg"</span>, top_k=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Text: <span class="subst">{r[<span class="string">'text'</span>]}</span>, Similarity: <span class="subst">{r[<span class="string">'similarity'</span>]:<span class="number">.3</span>f}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="multimodal-chatbot">Multimodal Chatbot</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForConditionalGeneration</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForQuestionAnswering</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultimodalChatbot</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Image captioning model</span></span><br><span class="line">        self.caption_processor = BlipProcessor.from_pretrained(</span><br><span class="line">            <span class="string">"Salesforce/blip-image-captioning-base"</span></span><br><span class="line">        )</span><br><span class="line">        self.caption_model = BlipForConditionalGeneration.from_pretrained(</span><br><span class="line">            <span class="string">"Salesforce/blip-image-captioning-base"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># VQA model</span></span><br><span class="line">        self.vqa_processor = BlipProcessor.from_pretrained(</span><br><span class="line">            <span class="string">"Salesforce/blip-vqa-base"</span></span><br><span class="line">        )</span><br><span class="line">        self.vqa_model = BlipForQuestionAnswering.from_pretrained(</span><br><span class="line">            <span class="string">"Salesforce/blip-vqa-base"</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conversation_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_message</span>(<span class="params">self, message, image_path=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""Process user message (may include image)"""</span></span><br><span class="line">        <span class="keyword">if</span> image_path:</span><br><span class="line">            image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">"RGB"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Determine if it's a description request or Q&amp;A request</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"?"</span> <span class="keyword">in</span> message <span class="keyword">or</span> <span class="string">"what"</span> <span class="keyword">in</span> message.lower() <span class="keyword">or</span> <span class="string">"how"</span> <span class="keyword">in</span> message.lower():</span><br><span class="line">                <span class="comment"># VQA mode</span></span><br><span class="line">                inputs = self.vqa_processor(image, message, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    out = self.vqa_model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">                response = self.vqa_processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Image captioning mode</span></span><br><span class="line">                inputs = self.caption_processor(image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    out = self.caption_model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">                response = self.caption_processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Pure text dialogue (can use LLM)</span></span><br><span class="line">            response = self.text_chat(message)</span><br><span class="line">        </span><br><span class="line">        self.conversation_history.append({</span><br><span class="line">            <span class="string">"user"</span>: message,</span><br><span class="line">            <span class="string">"image"</span>: image_path,</span><br><span class="line">            <span class="string">"assistant"</span>: response</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">text_chat</span>(<span class="params">self, message</span>):</span><br><span class="line">        <span class="string">"""Pure text chat (simplified example)"""</span></span><br><span class="line">        <span class="comment"># In practice, can use GPT, LLaMA, etc.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"I can help you with image-related questions. Please provide an image."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Usage example</span></span><br><span class="line">chatbot = MultimodalChatbot()</span><br><span class="line">response = chatbot.process_message(<span class="string">"What is in this image?"</span>, <span class="string">"image.jpg"</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<h3 id="deploying-multimodal-services">Deploying Multimodal
Services</h3>
<p><strong>FastAPI Deployment</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, File, UploadFile, Form</span><br><span class="line"><span class="keyword">from</span> fastapi.responses <span class="keyword">import</span> JSONResponse</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Globally load models</span></span><br><span class="line">caption_model = <span class="literal">None</span></span><br><span class="line">vqa_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.on_event(<span class="params"><span class="string">"startup"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">load_models</span>():</span><br><span class="line">    <span class="keyword">global</span> caption_model, vqa_model</span><br><span class="line">    <span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForConditionalGeneration</span><br><span class="line">    <span class="keyword">from</span> transformers <span class="keyword">import</span> BlipForQuestionAnswering</span><br><span class="line">    </span><br><span class="line">    caption_processor = BlipProcessor.from_pretrained(</span><br><span class="line">        <span class="string">"Salesforce/blip-image-captioning-base"</span></span><br><span class="line">    )</span><br><span class="line">    caption_model = BlipForConditionalGeneration.from_pretrained(</span><br><span class="line">        <span class="string">"Salesforce/blip-image-captioning-base"</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    vqa_processor = BlipProcessor.from_pretrained(<span class="string">"Salesforce/blip-vqa-base"</span>)</span><br><span class="line">    vqa_model = BlipForQuestionAnswering.from_pretrained(</span><br><span class="line">        <span class="string">"Salesforce/blip-vqa-base"</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/caption"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate_caption</span>(<span class="params">file: UploadFile = File(<span class="params">...</span>)</span>):</span><br><span class="line">    <span class="string">"""Generate image caption"""</span></span><br><span class="line">    image_bytes = <span class="keyword">await</span> file.read()</span><br><span class="line">    image = Image.<span class="built_in">open</span>(io.BytesIO(image_bytes)).convert(<span class="string">"RGB"</span>)</span><br><span class="line">    </span><br><span class="line">    inputs = caption_processor(image, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        out = caption_model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">    </span><br><span class="line">    caption = caption_processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> JSONResponse({<span class="string">"caption"</span>: caption})</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/vqa"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params"></span></span><br><span class="line"><span class="params">    file: UploadFile = File(<span class="params">...</span>),</span></span><br><span class="line"><span class="params">    question: <span class="built_in">str</span> = Form(<span class="params">...</span>)</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">"""Visual question answering"""</span></span><br><span class="line">    image_bytes = <span class="keyword">await</span> file.read()</span><br><span class="line">    image = Image.<span class="built_in">open</span>(io.BytesIO(image_bytes)).convert(<span class="string">"RGB"</span>)</span><br><span class="line">    </span><br><span class="line">    inputs = vqa_processor(image, question, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        out = vqa_model.generate(**inputs, max_length=<span class="number">50</span>)</span><br><span class="line">    </span><br><span class="line">    answer = vqa_processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> JSONResponse({<span class="string">"answer"</span>: answer})</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app, host=<span class="string">"0.0.0.0"</span>, port=<span class="number">8000</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Docker Deployment</strong>:</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.9</span>-slim</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install dependencies</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install transformers pillow fastapi uvicorn python-multipart</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Copy code</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> app.py /app/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre-download models (optional)</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> python -c <span class="string">"from transformers import BlipProcessor; BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">"uvicorn"</span>, <span class="string">"app:app"</span>, <span class="string">"--host"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"--port"</span>, <span class="string">"8000"</span>]</span></span><br></pre></td></tr></table></figure>
<h2 id="qa-common-questions-about-multimodal-large-language-models">❓
Q&amp;A: Common Questions About Multimodal Large Language Models</h2>
<p><strong>Q1: What are the main differences between CLIP and
BLIP?</strong></p>
<p>A: CLIP focuses on image-text alignment, achieving zero-shot
capabilities through contrastive learning, but cannot generate text.
BLIP is a unified model that can both understand (VQA, image-text
matching) and generate (image captioning), achieved through multi-task
learning.</p>
<p><strong>Q2: Why does BLIP-2 only need to train very few
parameters?</strong></p>
<p>A: BLIP-2 freezes the image encoder and language model, only training
Q-Former (~188M parameters). Q-Former acts as a bridge, learning to
extract visual features from the frozen image encoder and converting
them into a format that the language model can understand.</p>
<p><strong>Q3: How does Whisper handle speech in different
languages?</strong></p>
<p>A: Whisper was trained on multilingual data and can automatically
detect languages. You can specify the language via the
<code>language</code> parameter or let the model auto-detect. The model
supports 99 languages, including Chinese, English, Japanese, etc.</p>
<p><strong>Q4: How do multimodal models handle long videos?</strong></p>
<p>A: Typically two strategies: 1) Uniformly sample key frames (e.g., 1
frame per second); 2) Use sliding windows to segment videos and then
fuse results. For very long videos, video summarization techniques can
be used first to compress information.</p>
<p><strong>Q5: What's the difference between GPT-4V and open-source
multimodal LLMs like LLaVA?</strong></p>
<p>A: GPT-4V is a closed-source model with powerful performance but
requires API calls and higher costs. Open-source models like LLaVA can
be deployed locally but may have slightly inferior performance. The
choice depends on specific needs: choose GPT-4V for performance, choose
open-source models for customization or cost control.</p>
<p><strong>Q6: How to evaluate multimodal model
performance?</strong></p>
<p>A: Different tasks have different metrics: image captioning uses
BLEU, METEOR, CIDEr; VQA uses accuracy; image-text retrieval uses
Recall@K. Human evaluation can also be conducted to check the accuracy
and fluency of generated content.</p>
<p><strong>Q7: How much data is needed for multimodal
pretraining?</strong></p>
<p>A: Large-scale pretraining typically requires tens of millions to
billions of image-text pairs. CLIP used 400 million pairs, BLIP used 129
million pairs. For specific domains, domain data can be used for
fine-tuning, requiring less data (tens of thousands to hundreds of
thousands).</p>
<p><strong>Q8: How to solve memory issues with multimodal
models?</strong></p>
<p>A: Can adopt the following strategies: 1) Use quantization
(4-bit/8-bit); 2) Use gradient checkpointing; 3) Use parameter-efficient
fine-tuning (LoRA); 4) Use model parallelism; 5) Use smaller model
variants.</p>
<p><strong>Q9: What types of inputs can multimodal models
process?</strong></p>
<p>A: Commonly supported: images (JPG, PNG), text. Partially supported:
audio (Whisper), video (Video-ChatGPT). Future trends include supporting
more modalities, such as 3D models, point clouds, etc.</p>
<p><strong>Q10: How to build a production-grade multimodal
application?</strong></p>
<p>A: Key steps: 1) Choose appropriate models (balance performance and
cost); 2) Implement efficient inference services (batching, caching); 3)
Add monitoring and logging; 4) Implement error handling and fallback
strategies; 5) Use containerized deployment (Docker); 6) Implement load
balancing and auto-scaling.</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>Post title：NLP (11): Multimodal Large Language Models</li>
        <li>Post author：Chen Kai</li>
        <li>Create time：2025-03-30 00:00:00</li>
        <li>
            Post link：https://www.chenk.top/en/nlp-multimodal-nlp/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/en/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/Multimodal/">#Multimodal</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/en/nlp-rag-knowledge-enhancement/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">NLP (10): RAG and Knowledge Enhancement Systems</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/en/llm-workflows-architecture/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">LLM Workflows and Application Architecture: Enterprise Implementation Guide</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#vision-language-model-fundamentals"><span class="nav-number">1.</span> <span class="nav-text">Vision-Language Model
Fundamentals</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#clip-contrastive-learning-for-vision-language-alignment"><span class="nav-number">1.1.</span> <span class="nav-text">CLIP:
Contrastive Learning for Vision-Language Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#blip-unified-vision-language-understanding-and-generation"><span class="nav-number">1.2.</span> <span class="nav-text">BLIP:
Unified Vision-Language Understanding and Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#blip-2-parameter-efficient-multimodal-pretraining"><span class="nav-number">1.3.</span> <span class="nav-text">BLIP-2:
Parameter-Efficient Multimodal Pretraining</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-pretraining-strategies"><span class="nav-number">2.</span> <span class="nav-text">Multimodal Pretraining
Strategies</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-construction"><span class="nav-number">2.1.</span> <span class="nav-text">Data Construction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pretraining-objectives"><span class="nav-number">2.2.</span> <span class="nav-text">Pretraining Objectives</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#image-captioning-and-visual-question-answering"><span class="nav-number">3.</span> <span class="nav-text">Image Captioning
and Visual Question Answering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#image-captioning"><span class="nav-number">3.1.</span> <span class="nav-text">Image Captioning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#visual-question-answering-vqa"><span class="nav-number">3.2.</span> <span class="nav-text">Visual Question Answering
(VQA)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpt-4v-and-multimodal-llms"><span class="nav-number">4.</span> <span class="nav-text">GPT-4V and Multimodal LLMs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gpt-4v-architecture"><span class="nav-number">4.1.</span> <span class="nav-text">GPT-4V Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#open-source-multimodal-llms"><span class="nav-number">4.2.</span> <span class="nav-text">Open-Source Multimodal LLMs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#audio-text-models"><span class="nav-number">5.</span> <span class="nav-text">Audio-Text Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#whisper-large-scale-speech-recognition"><span class="nav-number">5.1.</span> <span class="nav-text">Whisper: Large-Scale
Speech Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#audio-text-alignment"><span class="nav-number">5.2.</span> <span class="nav-text">Audio-Text Alignment</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#video-understanding"><span class="nav-number">6.</span> <span class="nav-text">Video Understanding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#video-encoding"><span class="nav-number">6.1.</span> <span class="nav-text">Video Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-modeling"><span class="nav-number">6.2.</span> <span class="nav-text">Temporal Modeling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#practical-building-multimodal-applications"><span class="nav-number">7.</span> <span class="nav-text">Practical: Building
Multimodal Applications</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multimodal-retrieval-system"><span class="nav-number">7.1.</span> <span class="nav-text">Multimodal Retrieval System</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multimodal-chatbot"><span class="nav-number">7.2.</span> <span class="nav-text">Multimodal Chatbot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deploying-multimodal-services"><span class="nav-number">7.3.</span> <span class="nav-text">Deploying Multimodal
Services</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qa-common-questions-about-multimodal-large-language-models"><span class="nav-number">8.</span> <span class="nav-text">❓
Q&amp;A: Common Questions About Multimodal Large Language Models</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
