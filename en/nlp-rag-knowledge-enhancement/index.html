<!DOCTYPE html>



<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            NLP (10): RAG and Knowledge Enhancement Systems |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"en","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/en/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/en/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/en/tags">TAGS</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">NLP (10): RAG and Knowledge Enhancement Systems</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-02-28 00:00:00</span>
        <span class="mobile">2025-02-28 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/categories/Natural-Language-Processing/">Natural Language Processing</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/en/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/en/tags/RAG/">RAG</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>3.4k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>21 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>As large language models become widely adopted, a core problem has emerged: how to enable models to access and utilize external knowledge? Traditional pre-trained models, despite having massive parameters and strong language understanding capabilities, have their knowledge “frozen” in training data, unable to access the latest information or private knowledge bases. Retrieval-Augmented Generation (RAG) technology addresses this by combining information retrieval with generative models, enabling LLMs to dynamically access external knowledge and generate more accurate, relevant answers.</p>
<p>The core components of RAG systems include vector database selection, Embedding model optimization, retrieval strategy design, reranking techniques, and query rewriting and expansion. An excellent RAG system requires not only efficient retrieval mechanisms but also carefully designed query optimization and result fusion strategies. This article delves deep into various components of RAG systems, from basic architecture to advanced optimization techniques, and demonstrates how to build enterprise-grade RAG systems through practical examples.</p>
<span id="more"></span>
<h2 id="RAG-Fundamentals-and-Architecture"><a class="header-anchor" href="#RAG-Fundamentals-and-Architecture">¶</a>RAG Fundamentals and Architecture</h2>
<h3 id="Core-Concept-of-RAG"><a class="header-anchor" href="#Core-Concept-of-RAG">¶</a>Core Concept of RAG</h3>
<p>The basic idea of RAG is: before generating an answer, first retrieve relevant information from an external knowledge base, then input the retrieved information together with the user query into the generative model.</p>
<p><strong>Mathematical Representation</strong>:</p>
<p>For query <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container>, the output of a RAG system is:</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.779ex;" xmlns="http://www.w3.org/2000/svg" width="29.01ex" height="4.928ex" role="img" focusable="false" viewbox="0 -950 12822.3 2178.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1630,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1908,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(2368,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3034.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="munder" transform="translate(4090.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(66.2,-1100) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(465,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/></g><g data-mml-node="mi" transform="translate(1132,0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"/></g></g></g><g data-mml-node="mi" transform="translate(5701.2,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(6452.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(6841.2,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(7306.2,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(7584.2,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(8044.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(8655.4,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="mi" transform="translate(9155.7,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(9906.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(10295.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(10785.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(11063.7,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(11528.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(11973.3,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(12433.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></p>
<p>where:</p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.636ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 723 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"/></g></g></g></svg></mjx-container> is the set of retrieved documents</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.181ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2732 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(1605,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(2343,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> is the retrieval probability (relevance of document <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g></g></g></svg></mjx-container> to query <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container>)</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.296ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 3666.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"/></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1630,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1908,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(2373,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2817.7,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3277.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> is the generation probability (generating answer <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewbox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container> based on retrieved document <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g></g></g></svg></mjx-container> and query <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container>)</li>
</ul>
<h3 id="RAG-Architecture-Flow"><a class="header-anchor" href="#RAG-Architecture-Flow">¶</a>RAG Architecture Flow</h3>
<p>A typical RAG system includes the following steps:</p>
<ol>
<li><strong>Document Processing</strong>: Split, vectorize, and store original documents in a vector database</li>
<li><strong>Query Processing</strong>: Convert user queries to vector representations</li>
<li><strong>Retrieval</strong>: Retrieve relevant documents from the vector database</li>
<li><strong>Reranking</strong>: Fine-rank retrieval results</li>
<li><strong>Generation</strong>: Input retrieved documents and query into the generative model</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load documents</span></span><br><span class="line">loader = TextLoader(<span class="string">"documents.txt"</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Split documents</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span></span><br><span class="line">)</span><br><span class="line">chunks = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create vector database</span></span><br><span class="line">embeddings = HuggingFaceEmbeddings(</span><br><span class="line">    model_name=<span class="string">"sentence-transformers/all-MiniLM-L6-v2"</span></span><br><span class="line">)</span><br><span class="line">vectorstore = FAISS.from_documents(chunks, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create retriever</span></span><br><span class="line">retriever = vectorstore.as_retriever(search_kwargs={<span class="string">"k"</span>: <span class="number">5</span>})</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Create RAG chain</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">"stuff"</span>,</span><br><span class="line">    retriever=retriever,</span><br><span class="line">    return_source_documents=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Query</span></span><br><span class="line">query = <span class="string">"What is machine learning?"</span></span><br><span class="line">result = qa_chain({<span class="string">"query"</span>: query})</span><br><span class="line"><span class="built_in">print</span>(result[<span class="string">"result"</span>])</span><br><span class="line"><span class="built_in">print</span>(result[<span class="string">"source_documents"</span>])</span><br></pre></td></tr></table></figure>
<h3 id="RAG-vs-Fine-tuning"><a class="header-anchor" href="#RAG-vs-Fine-tuning">¶</a>RAG vs Fine-tuning</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>RAG</td>
<td>Dynamic knowledge updates, low cost, high interpretability</td>
<td>Depends on retrieval quality, may have hallucinations</td>
<td>Frequently updated knowledge bases, multi-domain knowledge</td>
</tr>
<tr>
<td>Fine-tuning</td>
<td>Model fully adapts to task, potentially better performance</td>
<td>High cost, difficult to update, may forget</td>
<td>Specific tasks, relatively stable knowledge</td>
</tr>
</tbody>
</table>
<h2 id="Vector-Database-Selection"><a class="header-anchor" href="#Vector-Database-Selection">¶</a>Vector Database Selection</h2>
<p>Vector databases are core components of RAG systems, responsible for storing and retrieving document vectors. Different vector databases have different characteristics and use cases.</p>
<h3 id="FAISS-Facebook-AI-Similarity-Search"><a class="header-anchor" href="#FAISS-Facebook-AI-Similarity-Search">¶</a>FAISS (Facebook AI Similarity Search)</h3>
<p>FAISS is Facebook’s open-source vector similarity search library, supporting both CPU and GPU acceleration.</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>High performance: Supports multiple index algorithms (IVF, HNSW, LSH)</li>
<li>Memory efficient: Supports memory mapping and quantization</li>
<li>Easy integration: Simple Python API</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Small to medium-scale datasets (millions of vectors)</li>
<li>Need for rapid prototyping</li>
<li>Local deployment</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create index</span></span><br><span class="line">dimension = <span class="number">768</span>  <span class="comment"># Vector dimension</span></span><br><span class="line">index = faiss.IndexFlatL2(dimension)  <span class="comment"># L2 distance</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add vectors</span></span><br><span class="line">vectors = np.random.random((<span class="number">10000</span>, dimension)).astype(<span class="string">'float32'</span>)</span><br><span class="line">index.add(vectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Search</span></span><br><span class="line">query_vector = np.random.random((<span class="number">1</span>, dimension)).astype(<span class="string">'float32'</span>)</span><br><span class="line">k = <span class="number">5</span>  <span class="comment"># Return top-k</span></span><br><span class="line">distances, indices = index.search(query_vector, k)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Top <span class="subst">{k}</span> similar vectors:"</span>)</span><br><span class="line"><span class="keyword">for</span> i, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(indices[<span class="number">0</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Rank <span class="subst">{i+<span class="number">1</span>}</span>: Index <span class="subst">{idx}</span>, Distance <span class="subst">{distances[<span class="number">0</span>][i]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use IVF index (faster, but requires training)</span></span><br><span class="line">nlist = <span class="number">100</span>  <span class="comment"># Number of cluster centers</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(dimension)</span><br><span class="line">index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)</span><br><span class="line">index_ivf.train(vectors)  <span class="comment"># Train index</span></span><br><span class="line">index_ivf.add(vectors)</span><br><span class="line">index_ivf.nprobe = <span class="number">10</span>  <span class="comment"># Number of clusters to check during search</span></span><br></pre></td></tr></table></figure>
<h3 id="Milvus-v2"><a class="header-anchor" href="#Milvus-v2">¶</a>Milvus</h3>
<p>Milvus is a cloud-native vector database supporting distributed deployment and horizontal scaling.</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Distributed architecture: Supports cluster deployment</li>
<li>High availability: Supports data replication and fault recovery</li>
<li>Rich features: Supports scalar filtering, time series, etc.</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Large-scale datasets (tens of millions+)</li>
<li>Production deployment</li>
<li>Need for high availability and scalability</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymilvus <span class="keyword">import</span> connections, Collection, FieldSchema, CollectionSchema, DataType</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connect to Milvus</span></span><br><span class="line">connections.connect(</span><br><span class="line">    alias=<span class="string">"default"</span>,</span><br><span class="line">    host=<span class="string">"localhost"</span>,</span><br><span class="line">    port=<span class="string">"19530"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define schema</span></span><br><span class="line">fields = [</span><br><span class="line">    FieldSchema(name=<span class="string">"id"</span>, dtype=DataType.INT64, is_primary=<span class="literal">True</span>, auto_id=<span class="literal">True</span>),</span><br><span class="line">    FieldSchema(name=<span class="string">"text"</span>, dtype=DataType.VARCHAR, max_length=<span class="number">1000</span>),</span><br><span class="line">    FieldSchema(name=<span class="string">"embedding"</span>, dtype=DataType.FLOAT_VECTOR, dim=<span class="number">768</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = CollectionSchema(fields, <span class="string">"RAG collection"</span>)</span><br><span class="line">collection = Collection(<span class="string">"rag_collection"</span>, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create index</span></span><br><span class="line">index_params = {</span><br><span class="line">    <span class="string">"metric_type"</span>: <span class="string">"L2"</span>,</span><br><span class="line">    <span class="string">"index_type"</span>: <span class="string">"IVF_FLAT"</span>,</span><br><span class="line">    <span class="string">"params"</span>: {<span class="string">"nlist"</span>: <span class="number">1024</span>}</span><br><span class="line">}</span><br><span class="line">collection.create_index(<span class="string">"embedding"</span>, index_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Insert data</span></span><br><span class="line">data = [</span><br><span class="line">    [<span class="string">"Document 1"</span>, <span class="string">"Document 2"</span>, <span class="string">"Document 3"</span>],</span><br><span class="line">    [[<span class="number">0.1</span>] * <span class="number">768</span>, [<span class="number">0.2</span>] * <span class="number">768</span>, [<span class="number">0.3</span>] * <span class="number">768</span>]  <span class="comment"># Example vectors</span></span><br><span class="line">]</span><br><span class="line">collection.insert(data)</span><br><span class="line">collection.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Search</span></span><br><span class="line">search_params = {<span class="string">"metric_type"</span>: <span class="string">"L2"</span>, <span class="string">"params"</span>: {<span class="string">"nprobe"</span>: <span class="number">10</span>}}</span><br><span class="line">results = collection.search(</span><br><span class="line">    data=[[<span class="number">0.15</span>] * <span class="number">768</span>],  <span class="comment"># Query vector</span></span><br><span class="line">    anns_field=<span class="string">"embedding"</span>,</span><br><span class="line">    param=search_params,</span><br><span class="line">    limit=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Pinecone-v2"><a class="header-anchor" href="#Pinecone-v2">¶</a>Pinecone</h3>
<p>Pinecone is a fully managed vector database service requiring no infrastructure management.</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Fully managed: No need to manage servers</li>
<li>Auto-scaling: Automatically adjusts based on load</li>
<li>Simple to use: RESTful API</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Rapid deployment</li>
<li>Small to medium-scale applications</li>
<li>Don’t want to manage infrastructure</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pinecone</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize</span></span><br><span class="line">pinecone.init(api_key=<span class="string">"your-api-key"</span>, environment=<span class="string">"us-west1-gcp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create index</span></span><br><span class="line">index_name = <span class="string">"rag-index"</span></span><br><span class="line"><span class="keyword">if</span> index_name <span class="keyword">not</span> <span class="keyword">in</span> pinecone.list_indexes():</span><br><span class="line">    pinecone.create_index(</span><br><span class="line">        index_name,</span><br><span class="line">        dimension=<span class="number">768</span>,</span><br><span class="line">        metric=<span class="string">"cosine"</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connect to index</span></span><br><span class="line">index = pinecone.Index(index_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Insert vectors</span></span><br><span class="line">vectors = [</span><br><span class="line">    (<span class="string">"vec1"</span>, [<span class="number">0.1</span>] * <span class="number">768</span>, {<span class="string">"text"</span>: <span class="string">"Document 1"</span>}),</span><br><span class="line">    (<span class="string">"vec2"</span>, [<span class="number">0.2</span>] * <span class="number">768</span>, {<span class="string">"text"</span>: <span class="string">"Document 2"</span>})</span><br><span class="line">]</span><br><span class="line">index.upsert(vectors=vectors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Search</span></span><br><span class="line">query_vector = [<span class="number">0.15</span>] * <span class="number">768</span></span><br><span class="line">results = index.query(</span><br><span class="line">    vector=query_vector,</span><br><span class="line">    top_k=<span class="number">5</span>,</span><br><span class="line">    include_metadata=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Chroma-v2"><a class="header-anchor" href="#Chroma-v2">¶</a>Chroma</h3>
<p>Chroma is a lightweight vector database focused on ease of use and developer experience.</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Lightweight: Low resource usage</li>
<li>Ease of use: Clean API design</li>
<li>Flexibility: Supports multiple deployment methods</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Development and testing</li>
<li>Small-scale applications</li>
<li>Rapid prototyping</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="keyword">from</span> chromadb.config <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create client</span></span><br><span class="line">client = chromadb.Client(Settings(</span><br><span class="line">    chroma_db_impl=<span class="string">"duckdb+parquet"</span>,</span><br><span class="line">    persist_directory=<span class="string">"./chroma_db"</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create collection</span></span><br><span class="line">collection = client.create_collection(</span><br><span class="line">    name=<span class="string">"rag_collection"</span>,</span><br><span class="line">    metadata={<span class="string">"hnsw:space"</span>: <span class="string">"cosine"</span>}</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add documents</span></span><br><span class="line">collection.add(</span><br><span class="line">    documents=[<span class="string">"Document 1"</span>, <span class="string">"Document 2"</span>, <span class="string">"Document 3"</span>],</span><br><span class="line">    embeddings=[[<span class="number">0.1</span>] * <span class="number">768</span>, [<span class="number">0.2</span>] * <span class="number">768</span>, [<span class="number">0.3</span>] * <span class="number">768</span>],</span><br><span class="line">    ids=[<span class="string">"id1"</span>, <span class="string">"id2"</span>, <span class="string">"id3"</span>],</span><br><span class="line">    metadatas=[{<span class="string">"source"</span>: <span class="string">"doc1"</span>}, {<span class="string">"source"</span>: <span class="string">"doc2"</span>}, {<span class="string">"source"</span>: <span class="string">"doc3"</span>}]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query</span></span><br><span class="line">results = collection.query(</span><br><span class="line">    query_embeddings=[[<span class="number">0.15</span>] * <span class="number">768</span>],</span><br><span class="line">    n_results=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Vector-Database-Comparison"><a class="header-anchor" href="#Vector-Database-Comparison">¶</a>Vector Database Comparison</h3>
<table>
<thead>
<tr>
<th>Database</th>
<th>Scale</th>
<th>Deployment</th>
<th>Characteristics</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>FAISS</td>
<td>Millions</td>
<td>Local</td>
<td>High performance, easy to use</td>
<td>Development, small-medium scale</td>
</tr>
<tr>
<td>Milvus</td>
<td>Tens of millions+</td>
<td>Distributed</td>
<td>Scalable, high availability</td>
<td>Production, large scale</td>
</tr>
<tr>
<td>Pinecone</td>
<td>Millions</td>
<td>Managed</td>
<td>Simple, no ops needed</td>
<td>Rapid deployment</td>
</tr>
<tr>
<td>Chroma</td>
<td>Hundreds of thousands</td>
<td>Local/Cloud</td>
<td>Lightweight, easy to use</td>
<td>Development, small scale</td>
</tr>
</tbody>
</table>
<h2 id="Embedding-Model-Comparison"><a class="header-anchor" href="#Embedding-Model-Comparison">¶</a>Embedding Model Comparison</h2>
<p>The quality of Embedding models directly affects retrieval performance. Different models have different characteristics and use cases.</p>
<h3 id="General-Embedding-Models"><a class="header-anchor" href="#General-Embedding-Models">¶</a>General Embedding Models</h3>
<p><strong>OpenAI text-embedding-ada-002</strong>:</p>
<ul>
<li>Dimension: 1536</li>
<li>Advantages: Excellent performance, multilingual support</li>
<li>Disadvantages: Requires API calls, has cost</li>
</ul>
<p><strong>sentence-transformers</strong>:</p>
<ul>
<li>Open-source model collection</li>
<li>Advantages: Free, can deploy locally, good performance</li>
<li>Common models:
<ul>
<li><code>all-MiniLM-L6-v2</code>: Fast, lightweight</li>
<li><code>all-mpnet-base-v2</code>: Better performance</li>
<li><code>multi-qa-mpnet-base</code>: Optimized for Q&amp;A</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model</span></span><br><span class="line">model = SentenceTransformer(<span class="string">'all-mpnet-base-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode text</span></span><br><span class="line">texts = [</span><br><span class="line">    <span class="string">"Machine learning is a branch of artificial intelligence"</span>,</span><br><span class="line">    <span class="string">"Deep learning uses neural networks"</span>,</span><br><span class="line">    <span class="string">"Natural language processing handles text data"</span></span><br><span class="line">]</span><br><span class="line">embeddings = model.encode(texts)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Embedding shape: <span class="subst">{embeddings.shape}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Embedding dimension: <span class="subst">{embeddings.shape[<span class="number">1</span>]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute similarity</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">similarity_matrix = cosine_similarity(embeddings)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Similarity matrix:"</span>)</span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></table></figure>
<h3 id="Domain-Specific-Embeddings"><a class="header-anchor" href="#Domain-Specific-Embeddings">¶</a>Domain-Specific Embeddings</h3>
<p>For specific domains, Embedding models can be fine-tuned using domain data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, InputExample, losses</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load base model</span></span><br><span class="line">model = SentenceTransformer(<span class="string">'all-mpnet-base-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare training data</span></span><br><span class="line">train_examples = [</span><br><span class="line">    InputExample(texts=[<span class="string">"Machine Learning"</span>, <span class="string">"ML"</span>]),</span><br><span class="line">    InputExample(texts=[<span class="string">"Deep Learning"</span>, <span class="string">"DL"</span>]),</span><br><span class="line">    InputExample(texts=[<span class="string">"Natural Language Processing"</span>, <span class="string">"NLP"</span>])</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss function (contrastive learning)</span></span><br><span class="line">train_dataloader = DataLoader(train_examples, shuffle=<span class="literal">True</span>, batch_size=<span class="number">16</span>)</span><br><span class="line">train_loss = losses.CosineSimilarityLoss(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fine-tune</span></span><br><span class="line">model.fit(</span><br><span class="line">    train_objectives=[(train_dataloader, train_loss)],</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    warmup_steps=<span class="number">100</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save model</span></span><br><span class="line">model.save(<span class="string">'./domain-specific-embedding'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Embedding-Model-Selection-Guide"><a class="header-anchor" href="#Embedding-Model-Selection-Guide">¶</a>Embedding Model Selection Guide</h3>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Dimension</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>text-embedding-ada-002</td>
<td>1536</td>
<td>Medium</td>
<td>High</td>
<td>Production, multilingual</td>
</tr>
<tr>
<td>all-MiniLM-L6-v2</td>
<td>384</td>
<td>Fast</td>
<td>Medium</td>
<td>Rapid prototyping, resource-constrained</td>
</tr>
<tr>
<td>all-mpnet-base-v2</td>
<td>768</td>
<td>Medium</td>
<td>High</td>
<td>Balance performance and speed</td>
</tr>
<tr>
<td>multi-qa-mpnet-base</td>
<td>768</td>
<td>Medium</td>
<td>High</td>
<td>Q&amp;A tasks</td>
</tr>
</tbody>
</table>
<h2 id="Retrieval-Strategy-Optimization"><a class="header-anchor" href="#Retrieval-Strategy-Optimization">¶</a>Retrieval Strategy Optimization</h2>
<p>Retrieval strategies determine how to retrieve relevant documents from vector databases. Different strategies suit different scenarios.</p>
<h3 id="Dense-Retrieval"><a class="header-anchor" href="#Dense-Retrieval">¶</a>Dense Retrieval</h3>
<p>Dense Retrieval uses vector similarity for retrieval, the most common method.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Strong semantic understanding</li>
<li>Can handle synonyms and semantic similarity</li>
<li>Simple implementation</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Less sensitive to keyword matching</li>
<li>May miss exact matching documents</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dense_retrieval</span>(<span class="params">query_embedding, vectorstore, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""Dense retrieval"""</span></span><br><span class="line">    <span class="comment"># Compute similarity</span></span><br><span class="line">    similarities = vectorstore.similarity_search_with_score(</span><br><span class="line">        query_embedding,</span><br><span class="line">        k=top_k</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> similarities</span><br></pre></td></tr></table></figure>
<h3 id="Sparse-Retrieval"><a class="header-anchor" href="#Sparse-Retrieval">¶</a>Sparse Retrieval</h3>
<p>Sparse Retrieval uses keyword matching (e.g., BM25) for retrieval.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Precise keyword matching</li>
<li>Sensitive to exact terms</li>
<li>No Embedding model needed</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot understand semantics</li>
<li>Not sensitive to synonyms</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> rank_bm25 <span class="keyword">import</span> BM25Okapi</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sparse_retrieval</span>(<span class="params">query, documents, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""Sparse retrieval (BM25)"""</span></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    tokenized_docs = [jieba.lcut(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> documents]</span><br><span class="line">    tokenized_query = jieba.lcut(query)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create BM25 index</span></span><br><span class="line">    bm25 = BM25Okapi(tokenized_docs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve</span></span><br><span class="line">    scores = bm25.get_scores(tokenized_query)</span><br><span class="line">    top_indices = np.argsort(scores)[::-<span class="number">1</span>][:top_k]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [(documents[i], scores[i]) <span class="keyword">for</span> i <span class="keyword">in</span> top_indices]</span><br></pre></td></tr></table></figure>
<h3 id="Hybrid-Retrieval"><a class="header-anchor" href="#Hybrid-Retrieval">¶</a>Hybrid Retrieval</h3>
<p>Hybrid Retrieval combines Dense and Sparse retrieval, leveraging strengths of both.</p>
<p><strong>Strategies</strong>:</p>
<ol>
<li><strong>RRF (Reciprocal Rank Fusion)</strong>: Merge rankings from both retrieval methods</li>
<li><strong>Weighted Fusion</strong>: Weighted sum of both retrieval results</li>
<li><strong>Reranking Fusion</strong>: Retrieve with both methods first, then fuse with reranking model</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hybrid_retrieval</span>(<span class="params">query, query_embedding, vectorstore, documents, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""Hybrid retrieval"""</span></span><br><span class="line">    <span class="comment"># Dense retrieval</span></span><br><span class="line">    dense_results = dense_retrieval(query_embedding, vectorstore, top_k=top_k*<span class="number">2</span>)</span><br><span class="line">    dense_scores = {doc: score <span class="keyword">for</span> doc, score <span class="keyword">in</span> dense_results}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sparse retrieval</span></span><br><span class="line">    sparse_results = sparse_retrieval(query, documents, top_k=top_k*<span class="number">2</span>)</span><br><span class="line">    sparse_scores = {doc: score <span class="keyword">for</span> doc, score <span class="keyword">in</span> sparse_results}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># RRF fusion</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rrf_score</span>(<span class="params">doc, rank, k=<span class="number">60</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (k + rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute RRF scores</span></span><br><span class="line">    combined_scores = {}</span><br><span class="line">    <span class="keyword">for</span> rank, (doc, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dense_results, <span class="number">1</span>):</span><br><span class="line">        combined_scores[doc] = combined_scores.get(doc, <span class="number">0</span>) + rrf_score(doc, rank)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> rank, (doc, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(sparse_results, <span class="number">1</span>):</span><br><span class="line">        combined_scores[doc] = combined_scores.get(doc, <span class="number">0</span>) + rrf_score(doc, rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sort and return top-k</span></span><br><span class="line">    sorted_docs = <span class="built_in">sorted</span>(combined_scores.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_docs[:top_k]</span><br></pre></td></tr></table></figure>
<h2 id="Reranking-Techniques"><a class="header-anchor" href="#Reranking-Techniques">¶</a>Reranking Techniques</h2>
<p>Reranking fine-ranks initial retrieval results to improve final result quality.</p>
<h3 id="Cross-Encoder-Reranking-v2"><a class="header-anchor" href="#Cross-Encoder-Reranking-v2">¶</a>Cross-Encoder Reranking</h3>
<p>Cross-Encoder inputs query and document together into the model to compute relevance scores.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>High accuracy</li>
<li>Can understand query-document interactions</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>High computational cost (cannot pre-compute)</li>
<li>Slow</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> CrossEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load Cross-Encoder model</span></span><br><span class="line">reranker = CrossEncoder(<span class="string">'cross-encoder/ms-marco-MiniLM-L-6-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rerank</span>(<span class="params">query, documents, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""Reranking"""</span></span><br><span class="line">    <span class="comment"># Build query-document pairs</span></span><br><span class="line">    pairs = [[query, doc] <span class="keyword">for</span> doc <span class="keyword">in</span> documents]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute relevance scores</span></span><br><span class="line">    scores = reranker.predict(pairs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sort</span></span><br><span class="line">    ranked_indices = np.argsort(scores)[::-<span class="number">1</span>]</span><br><span class="line">    ranked_docs = [documents[i] <span class="keyword">for</span> i <span class="keyword">in</span> ranked_indices[:top_k]]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ranked_docs</span><br></pre></td></tr></table></figure>
<h3 id="Multi-Stage-Retrieval"><a class="header-anchor" href="#Multi-Stage-Retrieval">¶</a>Multi-Stage Retrieval</h3>
<p>Multi-stage retrieval combines fast retrieval with precise reranking:</p>
<ol>
<li><strong>Stage 1</strong>: Use fast methods (Dense/Sparse) to retrieve many candidates (e.g., 100)</li>
<li><strong>Stage 2</strong>: Use reranking model to fine-rank candidates (e.g., top-5)</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multi_stage_retrieval</span>(<span class="params">query, query_embedding, vectorstore, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">"""Multi-stage retrieval"""</span></span><br><span class="line">    <span class="comment"># Stage 1: Fast retrieval of many candidates</span></span><br><span class="line">    candidates = dense_retrieval(query_embedding, vectorstore, top_k=<span class="number">100</span>)</span><br><span class="line">    candidate_docs = [doc <span class="keyword">for</span> doc, _ <span class="keyword">in</span> candidates]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 2: Reranking</span></span><br><span class="line">    reranked_docs = rerank(query, candidate_docs, top_k=top_k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> reranked_docs</span><br></pre></td></tr></table></figure>
<h2 id="Query-Rewriting-and-Expansion"><a class="header-anchor" href="#Query-Rewriting-and-Expansion">¶</a>Query Rewriting and Expansion</h2>
<p>Query optimization can improve retrieval performance, including query rewriting, query expansion, and query decomposition.</p>
<h3 id="Query-Rewriting"><a class="header-anchor" href="#Query-Rewriting">¶</a>Query Rewriting</h3>
<p>Query rewriting converts user queries into forms more suitable for retrieval.</p>
<p><strong>Methods</strong>:</p>
<ol>
<li><strong>Synonym expansion</strong>: Add synonyms</li>
<li><strong>Query completion</strong>: Complete incomplete queries</li>
<li><strong>Query simplification</strong>: Remove redundant words</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rewrite_query</span>(<span class="params">original_query, llm</span>):</span><br><span class="line">    <span class="string">"""Query rewriting"""</span></span><br><span class="line">    prompt = PromptTemplate(</span><br><span class="line">        input_variables=[<span class="string">"query"</span>],</span><br><span class="line">        template=<span class="string">"""</span></span><br><span class="line"><span class="string">        Rewrite the following query into a form more suitable for information retrieval, maintaining the original meaning but using more precise terms:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Original query: {query}</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Rewritten query:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    rewritten = llm(prompt.<span class="built_in">format</span>(query=original_query))</span><br><span class="line">    <span class="keyword">return</span> rewritten.strip()</span><br></pre></td></tr></table></figure>
<h3 id="Query-Expansion"><a class="header-anchor" href="#Query-Expansion">¶</a>Query Expansion</h3>
<p>Query expansion adds related terms and concepts.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">expand_query</span>(<span class="params">query, llm</span>):</span><br><span class="line">    <span class="string">"""Query expansion"""</span></span><br><span class="line">    prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">    Generate relevant keywords and synonyms for the following query for information retrieval:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Query: <span class="subst">{query}</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Relevant keywords (comma-separated):</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    expanded_terms = llm(prompt).strip().split(<span class="string">','</span>)</span><br><span class="line">    expanded_query = query + <span class="string">" "</span> + <span class="string">" "</span>.join(expanded_terms)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> expanded_query</span><br></pre></td></tr></table></figure>
<h3 id="Query-Decomposition"><a class="header-anchor" href="#Query-Decomposition">¶</a>Query Decomposition</h3>
<p>For complex queries, decompose into multiple sub-queries.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decompose_query</span>(<span class="params">query, llm</span>):</span><br><span class="line">    <span class="string">"""Query decomposition"""</span></span><br><span class="line">    prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">    Decompose the following complex query into multiple simple sub-queries:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Complex query: <span class="subst">{query}</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Sub-queries (one per line):</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    subqueries = llm(prompt).strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> [q.strip() <span class="keyword">for</span> q <span class="keyword">in</span> subqueries <span class="keyword">if</span> q.strip()]</span><br></pre></td></tr></table></figure>
<h2 id="Practical-Building-Enterprise-Grade-RAG-Systems"><a class="header-anchor" href="#Practical-Building-Enterprise-Grade-RAG-Systems">¶</a>Practical: Building Enterprise-Grade RAG Systems</h2>
<h3 id="Building-RAG-with-LangChain"><a class="header-anchor" href="#Building-RAG-with-LangChain">¶</a>Building RAG with LangChain</h3>
<p>LangChain provides a complete RAG toolchain.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> DirectoryLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load documents</span></span><br><span class="line">loader = DirectoryLoader(<span class="string">"./documents"</span>, glob=<span class="string">"*.txt"</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Split documents</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span></span><br><span class="line">)</span><br><span class="line">chunks = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create Embedding</span></span><br><span class="line">embeddings = HuggingFaceEmbeddings(</span><br><span class="line">    model_name=<span class="string">"sentence-transformers/all-mpnet-base-v2"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create vector database</span></span><br><span class="line">vectorstore = Chroma.from_documents(</span><br><span class="line">    documents=chunks,</span><br><span class="line">    embedding=embeddings,</span><br><span class="line">    persist_directory=<span class="string">"./chroma_db"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Custom Prompt</span></span><br><span class="line">prompt_template = <span class="string">"""Use the following context to answer the question. If you don't know the answer, say you don't know, don't make up an answer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Context: {context}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: {question}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Answer:"""</span></span><br><span class="line"></span><br><span class="line">PROMPT = PromptTemplate(</span><br><span class="line">    template=prompt_template,</span><br><span class="line">    input_variables=[<span class="string">"context"</span>, <span class="string">"question"</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Create RAG chain</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">"stuff"</span>,</span><br><span class="line">    retriever=vectorstore.as_retriever(search_kwargs={<span class="string">"k"</span>: <span class="number">5</span>}),</span><br><span class="line">    chain_type_kwargs={<span class="string">"prompt"</span>: PROMPT},</span><br><span class="line">    return_source_documents=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. Query</span></span><br><span class="line">query = <span class="string">"What is RAG?"</span></span><br><span class="line">result = qa_chain({<span class="string">"query"</span>: query})</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Answer: <span class="subst">{result[<span class="string">'result'</span>]}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Source documents: <span class="subst">{result[<span class="string">'source_documents'</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Building-RAG-with-LlamaIndex"><a class="header-anchor" href="#Building-RAG-with-LlamaIndex">¶</a>Building RAG with LlamaIndex</h3>
<p>LlamaIndex focuses on the data layer for LLM applications.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader, ServiceContext</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings <span class="keyword">import</span> HuggingFaceEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load documents</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">"./documents"</span>).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create service context</span></span><br><span class="line">embed_model = HuggingFaceEmbedding(</span><br><span class="line">    model_name=<span class="string">"sentence-transformers/all-mpnet-base-v2"</span></span><br><span class="line">)</span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">service_context = ServiceContext.from_defaults(</span><br><span class="line">    llm=llm,</span><br><span class="line">    embed_model=embed_model,</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create index</span></span><br><span class="line">index = VectorStoreIndex.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create query engine</span></span><br><span class="line">query_engine = index.as_query_engine(</span><br><span class="line">    similarity_top_k=<span class="number">5</span>,</span><br><span class="line">    response_mode=<span class="string">"compact"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Query</span></span><br><span class="line">response = query_engine.query(<span class="string">"What is RAG?"</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="built_in">print</span>(response.source_nodes)</span><br></pre></td></tr></table></figure>
<h3 id="Advanced-RAG-Patterns"><a class="header-anchor" href="#Advanced-RAG-Patterns">¶</a>Advanced RAG Patterns</h3>
<p><strong>Parent-Child Retrieval</strong>:</p>
<ul>
<li>Storage: Split documents into small chunks (child chunks)</li>
<li>Retrieval: Retrieve child chunks, but return parent chunks (containing more context)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create parent-child document splitter</span></span><br><span class="line">parent_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">2000</span>)</span><br><span class="line">child_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split documents</span></span><br><span class="line">parent_docs = parent_splitter.split_documents(documents)</span><br><span class="line">child_docs = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> parent_docs:</span><br><span class="line">    children = child_splitter.split_documents([parent])</span><br><span class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> children:</span><br><span class="line">        child.metadata[<span class="string">"parent_id"</span>] = parent.metadata.get(<span class="string">"id"</span>)</span><br><span class="line">    child_docs.extend(children)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store child chunks (for retrieval)</span></span><br><span class="line">vectorstore = Chroma.from_documents(child_docs, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return parent chunks during retrieval</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retrieve_with_parent</span>(<span class="params">query, vectorstore</span>):</span><br><span class="line">    <span class="comment"># Retrieve child chunks</span></span><br><span class="line">    child_results = vectorstore.similarity_search(query, k=<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get parent chunks</span></span><br><span class="line">    parent_ids = <span class="built_in">set</span>([r.metadata[<span class="string">"parent_id"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> child_results])</span><br><span class="line">    parent_docs = [doc <span class="keyword">for</span> doc <span class="keyword">in</span> parent_docs <span class="keyword">if</span> doc.metadata.get(<span class="string">"id"</span>) <span class="keyword">in</span> parent_ids]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parent_docs</span><br></pre></td></tr></table></figure>
<p><strong>Self-RAG</strong>:</p>
<ul>
<li>Use LLM to judge whether retrieval is needed</li>
<li>Critically evaluate retrieval results</li>
<li>Decide whether to use retrieved information based on evaluation</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_rag</span>(<span class="params">query, llm, vectorstore</span>):</span><br><span class="line">    <span class="string">"""Self-RAG implementation"""</span></span><br><span class="line">    <span class="comment"># 1. Judge whether retrieval is needed</span></span><br><span class="line">    need_retrieval_prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">    Determine whether the following query needs to retrieve from external knowledge base:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Query: <span class="subst">{query}</span></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    If retrieval is needed, answer "yes", otherwise answer "no".</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    need_retrieval = llm(need_retrieval_prompt).strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="string">"yes"</span> <span class="keyword">in</span> need_retrieval.lower():</span><br><span class="line">        <span class="comment"># 2. Retrieve</span></span><br><span class="line">        docs = vectorstore.similarity_search(query, k=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Evaluate retrieval results</span></span><br><span class="line">        evaluation_prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">        Evaluate the relevance of the following retrieval results to the query:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Query: <span class="subst">{query}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Retrieval results:</span></span><br><span class="line"><span class="string">        <span class="subst">{<span class="built_in">chr</span>(<span class="number">10</span>).join([<span class="string">f"<span class="subst">{i+<span class="number">1</span>}</span>. <span class="subst">{doc.page_content[:<span class="number">200</span>]}</span>"</span> <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs)])}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Give a relevance score (1-5) for each result and explain why.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        evaluation = llm(evaluation_prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. Generate answer</span></span><br><span class="line">        answer_prompt = <span class="string">f"""</span></span><br><span class="line"><span class="string">        Answer the question based on the following retrieval results:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Query: <span class="subst">{query}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Retrieval results:</span></span><br><span class="line"><span class="string">        <span class="subst">{<span class="built_in">chr</span>(<span class="number">10</span>).join([doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs])}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Relevance evaluation:</span></span><br><span class="line"><span class="string">        <span class="subst">{evaluation}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Answer:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        answer = llm(answer_prompt)</span><br><span class="line">        <span class="keyword">return</span> answer, docs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Direct generation (no retrieval needed)</span></span><br><span class="line">        answer = llm(<span class="string">f"Answer the following question: <span class="subst">{query}</span>"</span>)</span><br><span class="line">        <span class="keyword">return</span> answer, []</span><br></pre></td></tr></table></figure>
<h2 id="❓-Q-A-Common-Questions-on-RAG"><a class="header-anchor" href="#❓-Q-A-Common-Questions-on-RAG">¶</a>❓ Q&amp;A: Common Questions on RAG</h2>
<h3 id="Q1-What’s-the-difference-between-RAG-and-fine-tuning-When-to-use-RAG"><a class="header-anchor" href="#Q1-What’s-the-difference-between-RAG-and-fine-tuning-When-to-use-RAG">¶</a>Q1: What’s the difference between RAG and fine-tuning? When to use RAG?</h3>
<p><strong>A</strong>:</p>
<ul>
<li><strong>RAG</strong>: Dynamically retrieves external knowledge, suitable for frequently updated knowledge bases, need to access latest information, multi-domain knowledge</li>
<li><strong>Fine-tuning</strong>: Encodes knowledge into model parameters, suitable for specific tasks, relatively stable knowledge, need for ultimate performance</li>
<li><strong>Choice</strong>: If knowledge needs frequent updates or involves private data, choose RAG; if task is fixed and performance requirements are high, consider fine-tuning</li>
</ul>
<h3 id="Q2-How-to-choose-a-vector-database"><a class="header-anchor" href="#Q2-How-to-choose-a-vector-database">¶</a>Q2: How to choose a vector database?</h3>
<p><strong>A</strong>: Choice depends on:</p>
<ul>
<li><strong>Data scale</strong>: Millions use FAISS, tens of millions+ use Milvus</li>
<li><strong>Deployment</strong>: Local use FAISS/Chroma, cloud deployment use Milvus/Pinecone</li>
<li><strong>Ops capability</strong>: Don’t want to manage infrastructure use Pinecone, have ops team use Milvus</li>
<li><strong>Development stage</strong>: Rapid prototyping use FAISS/Chroma, production use Milvus</li>
</ul>
<h3 id="Q3-How-to-choose-Embedding-models"><a class="header-anchor" href="#Q3-How-to-choose-Embedding-models">¶</a>Q3: How to choose Embedding models?</h3>
<p><strong>A</strong>:</p>
<ul>
<li><strong>General scenarios</strong>: <code>all-mpnet-base-v2</code> or <code>text-embedding-ada-002</code></li>
<li><strong>Resource-constrained</strong>: <code>all-MiniLM-L6-v2</code> (fast, low dimension)</li>
<li><strong>Q&amp;A tasks</strong>: <code>multi-qa-mpnet-base</code></li>
<li><strong>Multilingual</strong>: <code>paraphrase-multilingual-mpnet-base-v2</code></li>
<li><strong>Domain-specific</strong>: Fine-tune general models with domain data</li>
</ul>
<h3 id="Q4-How-to-choose-between-Dense-Retrieval-and-Sparse-Retrieval"><a class="header-anchor" href="#Q4-How-to-choose-between-Dense-Retrieval-and-Sparse-Retrieval">¶</a>Q4: How to choose between Dense Retrieval and Sparse Retrieval?</h3>
<p><strong>A</strong>:</p>
<ul>
<li><strong>Dense Retrieval</strong>: Suitable for semantic understanding, synonym matching, concept retrieval</li>
<li><strong>Sparse Retrieval</strong>: Suitable for exact keyword matching, term retrieval</li>
<li><strong>Recommendation</strong>: Use Hybrid Retrieval, combining strengths of both</li>
</ul>
<h3 id="Q5-How-to-improve-retrieval-accuracy"><a class="header-anchor" href="#Q5-How-to-improve-retrieval-accuracy">¶</a>Q5: How to improve retrieval accuracy?</h3>
<p><strong>A</strong>: Multiple approaches:</p>
<ol>
<li><strong>Optimize Embedding</strong>: Use better models or domain fine-tuning</li>
<li><strong>Improve splitting strategy</strong>: Choose appropriate splitting methods based on document characteristics</li>
<li><strong>Use Reranking</strong>: Cross-Encoder reranking</li>
<li><strong>Query optimization</strong>: Query rewriting, expansion, decomposition</li>
<li><strong>Multi-stage retrieval</strong>: Coarse ranking then fine ranking</li>
</ol>
<h3 id="Q6-What-to-do-when-RAG-systems-have-hallucinations"><a class="header-anchor" href="#Q6-What-to-do-when-RAG-systems-have-hallucinations">¶</a>Q6: What to do when RAG systems have hallucinations?</h3>
<p><strong>A</strong>:</p>
<ol>
<li><strong>Improve retrieval quality</strong>: Ensure retrieved documents are relevant to query</li>
<li><strong>Prompt design</strong>: Clearly require model to answer based on retrieved content, say “don’t know” if unknown</li>
<li><strong>Result verification</strong>: Fact-check key information</li>
<li><strong>Use Self-RAG</strong>: Let model evaluate relevance of retrieval results</li>
<li><strong>Confidence scoring</strong>: Give confidence scores to generated results, prompt user when confidence is low</li>
</ol>
<h3 id="Q7-How-to-handle-long-documents"><a class="header-anchor" href="#Q7-How-to-handle-long-documents">¶</a>Q7: How to handle long documents?</h3>
<p><strong>A</strong>:</p>
<ol>
<li><strong>Parent-Child Retrieval</strong>: Retrieve small chunks, return large chunks</li>
<li><strong>Sliding window</strong>: Include adjacent chunks during retrieval</li>
<li><strong>Document summarization</strong>: Generate summaries for long documents, retrieve summaries</li>
<li><strong>Hierarchical retrieval</strong>: Retrieve chapters first, then specific content</li>
</ol>
<h3 id="Q8-How-to-optimize-RAG-system-latency"><a class="header-anchor" href="#Q8-How-to-optimize-RAG-system-latency">¶</a>Q8: How to optimize RAG system latency?</h3>
<p><strong>A</strong>:</p>
<ol>
<li><strong>Async retrieval</strong>: Retrieve multiple queries in parallel</li>
<li><strong>Caching</strong>: Cache results of common queries</li>
<li><strong>Index optimization</strong>: Use faster indexes (e.g., HNSW)</li>
<li><strong>Batch processing</strong>: Process multiple queries in batches</li>
<li><strong>Model optimization</strong>: Use faster Embedding and generative models</li>
</ol>
<h3 id="Q9-How-to-evaluate-RAG-system-performance"><a class="header-anchor" href="#Q9-How-to-evaluate-RAG-system-performance">¶</a>Q9: How to evaluate RAG system performance?</h3>
<p><strong>A</strong>: Evaluation metrics:</p>
<ul>
<li><strong>Retrieval metrics</strong>: Recall@K, MRR (Mean Reciprocal Rank), NDCG</li>
<li><strong>Generation metrics</strong>: BLEU, ROUGE, BERTScore, human evaluation</li>
<li><strong>End-to-end metrics</strong>: Answer accuracy, relevance, completeness</li>
<li><strong>System metrics</strong>: Latency, throughput, cost</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_rag_system</span>(<span class="params">qa_chain, test_set</span>):</span><br><span class="line">    <span class="string">"""Evaluate RAG system"""</span></span><br><span class="line">    results = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> query, ground_truth <span class="keyword">in</span> test_set:</span><br><span class="line">        <span class="comment"># Generate answer</span></span><br><span class="line">        result = qa_chain({<span class="string">"query"</span>: query})</span><br><span class="line">        answer = result[<span class="string">"result"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute metrics</span></span><br><span class="line">        <span class="keyword">from</span> rouge_score <span class="keyword">import</span> rouge_scorer</span><br><span class="line">        scorer = rouge_scorer.RougeScorer([<span class="string">'rouge1'</span>, <span class="string">'rougeL'</span>], use_stemmer=<span class="literal">True</span>)</span><br><span class="line">        scores = scorer.score(ground_truth, answer)</span><br><span class="line">        </span><br><span class="line">        results.append({</span><br><span class="line">            <span class="string">"query"</span>: query,</span><br><span class="line">            <span class="string">"answer"</span>: answer,</span><br><span class="line">            <span class="string">"ground_truth"</span>: ground_truth,</span><br><span class="line">            <span class="string">"rouge1"</span>: scores[<span class="string">"rouge1"</span>].fmeasure,</span><br><span class="line">            <span class="string">"rougeL"</span>: scores[<span class="string">"rougeL"</span>].fmeasure</span><br><span class="line">        })</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute average scores</span></span><br><span class="line">    avg_rouge1 = <span class="built_in">sum</span>([r[<span class="string">"rouge1"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results]) / <span class="built_in">len</span>(results)</span><br><span class="line">    avg_rougeL = <span class="built_in">sum</span>([r[<span class="string">"rougeL"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results]) / <span class="built_in">len</span>(results)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> {</span><br><span class="line">        <span class="string">"avg_rouge1"</span>: avg_rouge1,</span><br><span class="line">        <span class="string">"avg_rougeL"</span>: avg_rougeL,</span><br><span class="line">        <span class="string">"results"</span>: results</span><br><span class="line">    }</span><br></pre></td></tr></table></figure>
<h3 id="Q10-How-to-build-a-multi-turn-conversational-RAG-system"><a class="header-anchor" href="#Q10-How-to-build-a-multi-turn-conversational-RAG-system">¶</a>Q10: How to build a multi-turn conversational RAG system?</h3>
<p><strong>A</strong>:</p>
<ol>
<li><strong>Context management</strong>: Maintain conversation history</li>
<li><strong>Query rewriting</strong>: Combine current query with history</li>
<li><strong>Contextual retrieval</strong>: Consider conversation context during retrieval</li>
<li><strong>Memory mechanism</strong>: Distinguish short-term memory (current conversation) and long-term memory (knowledge base)</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConversationalRAG</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, qa_chain</span>):</span><br><span class="line">        self.qa_chain = qa_chain</span><br><span class="line">        self.conversation_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="comment"># 1. Build contextual query</span></span><br><span class="line">        context_query = self._build_contextual_query(query)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Retrieve and generate</span></span><br><span class="line">        result = self.qa_chain({<span class="string">"query"</span>: context_query})</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Update history</span></span><br><span class="line">        self.conversation_history.append({</span><br><span class="line">            <span class="string">"user"</span>: query,</span><br><span class="line">            <span class="string">"assistant"</span>: result[<span class="string">"result"</span>]</span><br><span class="line">        })</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result[<span class="string">"result"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_contextual_query</span>(<span class="params">self, current_query</span>):</span><br><span class="line">        <span class="string">"""Build contextual query"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.conversation_history:</span><br><span class="line">            <span class="keyword">return</span> current_query</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add recent conversations</span></span><br><span class="line">        recent_history = self.conversation_history[-<span class="number">3</span>:]</span><br><span class="line">        context = <span class="string">"\n"</span>.join([</span><br><span class="line">            <span class="string">f"User: <span class="subst">{h[<span class="string">'user'</span>]}</span>\nAssistant: <span class="subst">{h[<span class="string">'assistant'</span>]}</span>"</span></span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> recent_history</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"""</span></span><br><span class="line"><span class="string">        Conversation history:</span></span><br><span class="line"><span class="string">        <span class="subst">{context}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Current question: <span class="subst">{current_query}</span></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Please answer the current question based on the conversation history.</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></table></figure>
<hr>
<p>RAG technology provides large language models with the ability to access external knowledge, a key technology for building knowledge-enhanced AI systems. An excellent RAG system requires careful design of various components, from vector database selection to retrieval strategy optimization, from query processing to result generation. In practice, it’s necessary to select appropriate components and technologies based on specific needs, continuously optimize and iterate, to build efficient and accurate RAG systems.</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>Post title：NLP (10): RAG and Knowledge Enhancement Systems</li>
        <li>Post author：Chen Kai</li>
        <li>Create time：2025-02-28 00:00:00</li>
        <li>
            Post link：https://www.chenk.top/en/nlp-rag-knowledge-enhancement/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/en/tags/LLM/">#LLM</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/NLP/">#NLP</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/en/tags/RAG/">#RAG</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/en/nlp-multimodal-nlp/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">NLP (11): Multimodal Large Language Models</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/en/moslora/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">MoSLoRA: Mixture-of-Subspaces in Low-Rank Adaptation</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#RAG-Fundamentals-and-Architecture"><span class="nav-number">1.</span> <span class="nav-text">RAG Fundamentals and Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Core-Concept-of-RAG"><span class="nav-number">1.1.</span> <span class="nav-text">Core Concept of RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RAG-Architecture-Flow"><span class="nav-number">1.2.</span> <span class="nav-text">RAG Architecture Flow</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RAG-vs-Fine-tuning"><span class="nav-number">1.3.</span> <span class="nav-text">RAG vs Fine-tuning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-Database-Selection"><span class="nav-number">2.</span> <span class="nav-text">Vector Database Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FAISS-Facebook-AI-Similarity-Search"><span class="nav-number">2.1.</span> <span class="nav-text">FAISS (Facebook AI Similarity Search)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Milvus-v2"><span class="nav-number">2.2.</span> <span class="nav-text">Milvus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pinecone-v2"><span class="nav-number">2.3.</span> <span class="nav-text">Pinecone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Chroma-v2"><span class="nav-number">2.4.</span> <span class="nav-text">Chroma</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vector-Database-Comparison"><span class="nav-number">2.5.</span> <span class="nav-text">Vector Database Comparison</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding-Model-Comparison"><span class="nav-number">3.</span> <span class="nav-text">Embedding Model Comparison</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#General-Embedding-Models"><span class="nav-number">3.1.</span> <span class="nav-text">General Embedding Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Domain-Specific-Embeddings"><span class="nav-number">3.2.</span> <span class="nav-text">Domain-Specific Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding-Model-Selection-Guide"><span class="nav-number">3.3.</span> <span class="nav-text">Embedding Model Selection Guide</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Retrieval-Strategy-Optimization"><span class="nav-number">4.</span> <span class="nav-text">Retrieval Strategy Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dense-Retrieval"><span class="nav-number">4.1.</span> <span class="nav-text">Dense Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sparse-Retrieval"><span class="nav-number">4.2.</span> <span class="nav-text">Sparse Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hybrid-Retrieval"><span class="nav-number">4.3.</span> <span class="nav-text">Hybrid Retrieval</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reranking-Techniques"><span class="nav-number">5.</span> <span class="nav-text">Reranking Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Encoder-Reranking-v2"><span class="nav-number">5.1.</span> <span class="nav-text">Cross-Encoder Reranking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Stage-Retrieval"><span class="nav-number">5.2.</span> <span class="nav-text">Multi-Stage Retrieval</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Query-Rewriting-and-Expansion"><span class="nav-number">6.</span> <span class="nav-text">Query Rewriting and Expansion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Query-Rewriting"><span class="nav-number">6.1.</span> <span class="nav-text">Query Rewriting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Query-Expansion"><span class="nav-number">6.2.</span> <span class="nav-text">Query Expansion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Query-Decomposition"><span class="nav-number">6.3.</span> <span class="nav-text">Query Decomposition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Practical-Building-Enterprise-Grade-RAG-Systems"><span class="nav-number">7.</span> <span class="nav-text">Practical: Building Enterprise-Grade RAG Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-RAG-with-LangChain"><span class="nav-number">7.1.</span> <span class="nav-text">Building RAG with LangChain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-RAG-with-LlamaIndex"><span class="nav-number">7.2.</span> <span class="nav-text">Building RAG with LlamaIndex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advanced-RAG-Patterns"><span class="nav-number">7.3.</span> <span class="nav-text">Advanced RAG Patterns</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%9D%93-Q-A-Common-Questions-on-RAG"><span class="nav-number">8.</span> <span class="nav-text">❓ Q&amp;A: Common Questions on RAG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Q1-What%E2%80%99s-the-difference-between-RAG-and-fine-tuning-When-to-use-RAG"><span class="nav-number">8.1.</span> <span class="nav-text">Q1: What’s the difference between RAG and fine-tuning? When to use RAG?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q2-How-to-choose-a-vector-database"><span class="nav-number">8.2.</span> <span class="nav-text">Q2: How to choose a vector database?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q3-How-to-choose-Embedding-models"><span class="nav-number">8.3.</span> <span class="nav-text">Q3: How to choose Embedding models?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q4-How-to-choose-between-Dense-Retrieval-and-Sparse-Retrieval"><span class="nav-number">8.4.</span> <span class="nav-text">Q4: How to choose between Dense Retrieval and Sparse Retrieval?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q5-How-to-improve-retrieval-accuracy"><span class="nav-number">8.5.</span> <span class="nav-text">Q5: How to improve retrieval accuracy?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q6-What-to-do-when-RAG-systems-have-hallucinations"><span class="nav-number">8.6.</span> <span class="nav-text">Q6: What to do when RAG systems have hallucinations?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q7-How-to-handle-long-documents"><span class="nav-number">8.7.</span> <span class="nav-text">Q7: How to handle long documents?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q8-How-to-optimize-RAG-system-latency"><span class="nav-number">8.8.</span> <span class="nav-text">Q8: How to optimize RAG system latency?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q9-How-to-evaluate-RAG-system-performance"><span class="nav-number">8.9.</span> <span class="nav-text">Q9: How to evaluate RAG system performance?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q10-How-to-build-a-multi-turn-conversational-RAG-system"><span class="nav-number">8.10.</span> <span class="nav-text">Q10: How to build a multi-turn conversational RAG system?</span></a></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
