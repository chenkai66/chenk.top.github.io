---
title: 浅谈位置编码
tags: 位置编码
categories: Algorithm
date: 2024-09-19 19:00:00
mathjax: true

---

Transformer 模型由于依赖自注意力机制，无法直接捕捉输入序列中的位置信息，因此需要引入位置编码。位置编码主要分为两大类：绝对位置编码和相对位置编码。绝对位置编码通过为每个输入位置分配独立的编码来融入位置信息，而相对位置编码则专注于捕捉不同 Token 之间的相对距离。本文详细介绍了各种位置编码的变体，包括训练式、三角函数式、递归式、相乘式的绝对位置编码，以及经典的相对位置编码、XLNet 式和 T5 式相对位置编码，帮助读者理解不同位置编码的设计思路及其在 Transformer 中的应用。

<!-- more -->

与 RNN、CNN 等传统模型不同，Transformer 模型由于完全依赖于自注意力机制，因此需要专门的机制来捕捉输入序列中的位置信息。纯粹的自注意力模块并不能识别输入序列的顺序信息，因为它对不同位置的 Token 处理没有先验，无法区分第一个 Token 和第二个 Token。这使得位置编码在 Transformer 中至关重要。通过引入位置信息，Transformer 才能够正确地理解输入文本的顺序。

位置编码方法可以大致分为两大类：**绝对位置编码**和**相对位置编码**。虽然理论上主要有这两大类，但在实践中每种方式都衍生出了各种变体，研究人员为优化模型的表现提出了许多不同的方案。本文将探讨各种绝对位置编码与相对位置编码的设计，并分析一些不按常规套路出牌的位置编码方案。

# 绝对位置编码

绝对位置编码的核心思想是为每个位置引入独立的位置信息，并将其融入输入表示中，使得 Transformer 可以区分不同位置的 Token。绝对位置编码的设计方式多种多样，从最简单的训练式位置编码到基于三角函数的编码，每种方法都有其独特的优缺点。

## 训练式位置编码

训练式位置编码是最为直接的一种位置编码方式。在这种方法中，位置信息被视为可训练的参数，类似于模型中的其他权重参数，在训练过程中通过反向传播进行优化和更新。具体来说，训练式位置编码会为每一个可能的位置分配一个向量，这个向量会随着训练过程而不断调整，以便模型能够捕捉到输入序列中的顺序信息。

### 编码方式

假设 Transformer 模型的最大序列长度为 $512$，编码的维度为 $768$，我们可以初始化一个 $512 \times 768$ 的矩阵 $P$，其中每一行对应于一个位置的编码向量。例如，矩阵 $P$ 的第 $k$ 行表示第 $k$ 个位置的编码向量 $p_k$。在模型的训练过程中，位置编码矩阵 $P$ 将像其他可训练参数一样，通过梯度下降法进行优化。

对于每个输入序列中的第 $k$ 个 Token，其输入向量 $x_k$ 会与相应的位置向量 $p_k$ 相加得到新的输入表示：

$$
x_k^{\text{new}} = x_k + p_k
$$

其中，$x_k$ 是输入 Token 的原始向量表示，$p_k$ 是该位置的编码向量。通过将位置信息融入到输入表示中，Transformer 模型能够捕捉序列中的顺序关系。

### 优点

#### 高度灵活

训练式位置编码的一个显著优点在于其**高度灵活性**。由于位置编码向量是可训练的，模型可以根据具体的任务需求和数据分布，自主学习到最适合的位置信息表示。这种方式没有预先设定的生成规则，而是完全由模型根据数据自动优化，因此在某些任务中可能能够捕捉到更复杂的位置信息模式。

#### 适应性强

与其他固定的编码方式（如三角函数式位置编码）相比，训练式位置编码可以更加灵活地适应不同的数据分布和任务需求。模型可以学习到更复杂的位置信息表示，从而在某些任务中表现更优，尤其是当任务中的位置信息具有较强的特定模式时，训练式位置编码的可调节性可以带来显著的性能提升。

### 缺点

尽管训练式位置编码具有一定的优势，但它也存在一些明显的局限性，尤其是**外推性**问题。

#### 外推性差

训练式位置编码的主要缺点在于**缺乏外推性**（Extrapolation）。由于位置编码向量是直接与输入序列的每个位置相关联的，因此在训练过程中，模型只能学习到固定长度范围内的位置信息。例如，如果模型的最大序列长度设定为 $512$，那么它只能处理长度为 $512$ 以内的序列。对于更长的序列，模型无法生成有效的位置信息，因为训练过程中没有见过超出这个范围的样本。

当模型在推理阶段遇到超过训练时长度的输入（如长度为 $1024$ 的序列）时，$p_k$ 的向量只能通过随机初始化或其他方式扩展，但由于这些额外的位置信息没有经过训练，通常效果较差。模型在处理这些超长序列时无法像处理短序列那样准确捕捉顺序信息。

#### 参数量大

对于较长的输入序列，训练式位置编码需要为每个位置分配一个独立的编码向量，这意味着它会引入大量额外的参数。例如，若模型的最大序列长度为 $1024$，编码维度为 $768$，那么光是位置编码矩阵就包含 $1024 \times 768 = 786432$ 个可训练参数。随着输入序列长度的增加，位置编码的参数量也会急剧增加，这可能导致模型的训练开销变大，并增加过拟合的风险。

### 外推性问题的应对措施

为了应对训练式位置编码的外推性问题，研究人员提出了一些改进方案，旨在扩展模型处理长序列的能力。

#### 随机初始化超长位置

一种直接的应对方法是对超出训练长度的序列位置向量进行**随机初始化**。当输入序列的长度超出模型训练时的最大长度时，可以对这些新的位置向量 $p_k$ 随机赋值，并通过微调模型进一步训练。然而，这种方法效果有限，尤其是在序列长度远超出训练范围时，模型的性能通常会显著下降。

#### 分段式训练

另一种改进方式是**分段式训练**，即将长序列分解为若干较短的子序列，每个子序列单独进行训练。通过这种方式，模型可以学习到较长文本中的局部顺序信息，并在推理阶段将多个子序列的结果组合起来，以处理长序列任务。这种方法能够在一定程度上缓解外推性问题，但仍然难以完全解决长序列的全局依赖问题。

#### 层次分解

为了更好地应对外推性问题，研究人员还提出了**层次分解位置编码**的方法。例如，层次分解可以将长序列分解为多个不同的层级，每个层级上采用不同的编码方式，从而实现跨层次的顺序信息捕捉。这种方法不仅增强了模型处理长文本的能力，还能够通过共享较高层级的位置信息，减少参数的冗余，提高模型的外推性。

## 三角函数式位置编码

三角函数式位置编码，也称为 Sinusoidal 位置编码，是 Transformer 论文 [《Attention is All You Need》](https://arxiv.org/abs/1706.03762 "Attention is All You Need") 中提出的绝对位置编码方法。它通过为每个输入位置 $k$ 生成周期性的编码，将位置信息融入到输入序列中。其核心思想是使用正弦和余弦函数，使得位置编码呈现周期性变化，以此捕捉输入序列中不同位置的相对顺序。

### 编码公式

三角函数式位置编码的具体公式如下：

$$
\begin{aligned}
\boldsymbol{p}_{k, 2i} &= \sin \left( \frac{k}{10000^{\frac{2i}{d}}} \right) \\
\boldsymbol{p}_{k, 2i+1} &= \cos \left( \frac{k}{10000^{\frac{2i}{d}}} \right)
\end{aligned}
$$

其中，$d$ 是位置向量的维度，$i$ 代表当前维度的索引，$k$ 是输入序列中第 $k$ 个位置的索引。编码的每个偶数维度采用正弦函数生成 ($\sin$)，奇数维度采用余弦函数生成 ($\cos$)，从而形成交替变化的编码序列，这种设计使得编码在不同维度上呈现出不同频率的波动。

### 解释编码公式

在公式中，$\frac{1}{10000^{\frac{2i}{d}}}$ 控制了正弦和余弦函数的频率。具体来说：

- 当 $i$ 较小时，频率较高，即随着 $k$ 的增加，编码的变化会更加剧烈；
- 当 $i$ 较大时，频率较低，编码变化较为平缓。

这种安排使得三角函数式位置编码可以同时捕捉到**局部细节**和**全局顺序**，从而帮助 Transformer 模型理解输入序列中的顺序关系。通过这种频率变化，模型可以区分位置较近的 Token 和位置较远的 Token。

### 周期性与外推性

三角函数式位置编码的一个重要特性是其**周期性**。由于正弦和余弦函数的周期性，位置编码的变化在一定范围内会重复，这意味着模型可以处理超过训练长度的序列。这一特性为三角函数式位置编码提供了良好的**外推性**（extrapolation），即使输入序列长度超过了训练时的最大长度，模型依然能够通过相同的三角函数生成相应的位置编码，继续进行推理。

例如，假设一个 Transformer 模型在训练时最多处理 512 个位置，测试时需要处理 1024 个位置，三角函数式位置编码可以直接扩展到 1024 的位置，而无需对模型进行微调。通过这种方式，模型在处理长文本时，依然能够保留顺序信息。

### 正余弦函数的相对位置信息

三角函数式位置编码不仅能够捕捉绝对位置信息，还能够在一定程度上表达**相对位置信息**。这源于正弦和余弦函数的和差化积公式，例如：

$$
\sin (\alpha + \beta) = \sin \alpha \cos \beta + \cos \alpha \sin \beta
$$

$$
\cos (\alpha + \beta) = \cos \alpha \cos \beta - \sin \alpha \sin \beta
$$

这表明两个位置的和或者差可以通过它们的正余弦函数进行组合表示。因此，三角函数式位置编码可以自然地表达输入序列中不同位置之间的相对关系，特别是对于那些相对位置距离较短的 Token，模型能够较好地捕捉它们之间的顺序信息。

### 优点

1. **实现简单**：三角函数式位置编码不需要学习参数，因此实现上非常简单。由于编码是根据公式直接生成的，因此在实际应用中不需要通过反向传播进行训练。
2. **外推性好**：通过周期性函数的设计，该编码方法具备较强的外推性，可以很好地处理超出训练长度的序列。
3. **捕捉相对位置**：正弦和余弦函数具有内在的周期性和相对关系，这使得模型能够通过这些函数表达输入序列中的相对位置信息。

### 缺点
1. **周期性导致远距离信息混淆**：尽管正弦和余弦函数具备周期性，但这种周期性也带来了一定的局限性。当位置距离较远时，正余弦函数的值可能会重复，从而导致模型无法有效区分远距离的 Token。这使得该方法在处理非常长的序列时，位置信息的表达能力逐渐衰减。
2. **缺乏灵活性**：由于三角函数的生成公式是固定的，这使得其在处理非常复杂的位置信息时可能表现不佳，无法根据任务需求动态调整位置编码。

### 其他改进与应用

三角函数式位置编码由于其简便性和外推性，得到了广泛应用。尽管它的局限性较为明显，但许多后续的研究工作依然在此基础上进行了改进。例如，在处理超长文本或复杂依赖关系时，研究人员尝试结合三角函数式编码和其他形式的编码，设计出更加灵活和有效的位置信息表示方式。

此外，三角函数式位置编码还启发了其他模型中的位置编码设计。例如，后续的 Transformer-XL 和 T5 等模型也基于三角函数的周期性特性，提出了更为复杂和灵活的相对位置编码方法，以更好地捕捉长距离依赖。

## 递归式位置编码

递归式位置编码是一种通过递归关系来生成位置信息的方式，它不同于绝对位置编码中为每个位置分配独立的编码向量，而是通过定义某个初始位置向量，然后基于递归函数生成后续位置的编码。这种方法借鉴了 RNN（Recurrent Neural Network）的思想，利用序列中的前一个位置的信息推导出当前的位置信息。

### 递归式编码公式

递归式位置编码的基本思想是通过递归关系逐步生成每个位置的编码。例如，我们可以定义第一个位置的编码 $p_0$，然后通过递归公式生成后续的位置编码：

$$
p_{k+1} = f(p_k)
$$

其中，$p_k$ 表示位置 $k$ 的编码，$f$ 是递归函数，它基于前一个位置的编码生成当前的位置编码。

一个典型的递归函数形式可能是线性变换：

$$
p_{k+1} = W p_k + b
$$

其中，$W$ 是权重矩阵，$b$ 是偏置项。每个位置的编码都是从前一个位置的编码中推导出来的，通过这种方式，递归式位置编码可以表示序列中的位置信息。

### RNN 与递归式位置编码的关系

递归式位置编码与 RNN 模型有很多相似之处。RNN 通过递归结构处理输入序列中的时间步，序列中的每个时间步的信息都是基于前一个时间步生成的。同样，递归式位置编码也是通过位置之间的递归关系逐步生成每个位置的编码。

RNN 模型具备一个显著的特性，即它在自然结构中包含位置信息的表示能力，因为每个时间步的状态都取决于前一个时间步。因此，RNN 理论上并不需要额外的位置信息表示。然而，当将 RNN 结合到 Transformer 中时，可以通过递归式位置编码的方式模拟 RNN 的这种递归结构，让 Transformer 捕捉序列中的顺序信息。

### 优点

#### 较好的外推性

递归式位置编码的一个显著优点是其**外推性**。由于后续位置的编码是基于递归关系生成的，因此当处理超长序列时，递归式位置编码能够很好地推导出新的位置信息，不受训练时最大序列长度的限制。即使序列长度超出了训练时的最大范围，递归函数 $f$ 依然可以继续生成新的位置编码，从而处理更长的文本序列。

这种外推性来源于递归函数的通用性。只要定义了递归规则，理论上它可以生成无限长度的序列。因此，递归式位置编码在处理长文本任务时表现出色，能够有效应对序列长度不一致的问题。

#### 捕捉位置之间的顺序依赖

递归式位置编码通过位置之间的递归关系捕捉了序列中的顺序信息。与独立生成每个位置编码的绝对位置编码不同，递归式位置编码使得每个位置的编码都依赖于前一个位置，从而能够更好地表示位置之间的顺序依赖关系。

例如，对于自然语言序列，词语之间的顺序非常重要，递归式位置编码能够通过递归函数模拟这种顺序关系，让 Transformer 更加自然地捕捉到词语之间的位置信息。

### 局限性

#### 计算不易并行化

递归式位置编码的一个明显缺点是它的**并行性较差**。由于每个位置的编码依赖于前一个位置，因此这些编码必须按顺序生成。这与 Transformer 模型的并行计算特点相悖，Transformer 之所以能够大幅提高计算效率，部分原因在于它可以对序列中的所有位置并行处理，而递归式位置编码由于其递归性质，限制了这种并行计算能力。

在大规模数据处理场景中，递归式位置编码可能会导致较大的计算开销，尤其是当序列长度较长时，生成每个位置的编码都需要依赖前面的结果，无法并行计算，从而降低模型的训练和推理效率。

#### 模型复杂性增加

递归式位置编码引入了额外的复杂性。每个位置的编码不仅依赖于递归函数的设计，还依赖于递归关系的初始化。因此，递归函数的选择对编码效果有着重要影响。如果递归函数的设计不合理，可能导致位置信息无法有效捕捉序列中的顺序关系。此外，递归式位置编码的参数量和训练难度也可能随着序列长度的增加而显著增加。

## 基于神经微分方程的位置编码

近年来，递归式位置编码的思想被进一步发展，并结合了更复杂的递归模型和微分方程模型。例如，ICML 2020 论文 [《Learning to Encode Position for Transformer with Continuous Dynamical Model》](https://arxiv.org/abs/2003.09229 "Learning to Encode Position for Transformer with Continuous Dynamical Model") 提出了基于微分方程的位置编码方法，称为 FLOATER。

FLOATER 是一种基于神经微分方程 (Neural ODE) 的位置编码方法，其通过动力系统的连续化建模解决了传统位置编码无法捕捉序列中位置信息动态变化的问题。FLOATER 通过神经网络引入动态递归机制，使得 Transformer 可以在长序列任务中具有更强的外推能力。以下我们详细介绍其数学原理并扩展细节。

### 基本原理：从离散到连续

在 FLOATER 模型中，位置编码被建模为一个随时间变化的动态过程。传统的 Transformer 使用离散的位置编码 $\{p_i\}$，这些编码要么通过三角函数生成（如正弦和余弦编码），要么通过固定的嵌入方式定义。它们都忽略了位置之间可能存在的依赖关系。而 FLOATER 提出，将位置编码视为一个连续函数 $\boldsymbol{p}(t)$，其中 $t$ 代表时间。通过这种方式，位置编码不仅可以捕捉到单个位置的信息，还能够对不同位置之间的关系进行建模。

**动力系统模型：**
$$
\boldsymbol{p}(t) = \boldsymbol{p}(s) + \int_s^t \boldsymbol{h}(\tau, \boldsymbol{p}(\tau); \boldsymbol{\theta}_h) d\tau
$$

在这个公式中：
- $\boldsymbol{p}(t)$ 是时间 $t$ 的位置表示。
- $\boldsymbol{h}(\tau, \boldsymbol{p}(\tau); \boldsymbol{\theta}_h)$ 是一个神经网络，负责计算位置信息的变化率（类似于力的概念）。
- $\tau$ 代表时间步，$\boldsymbol{\theta}_h$ 是神经网络的可训练参数。

这种建模方式可以理解为，序列中的每个位置由一个“隐含的力”推动着不断演变，使得位置编码 $\boldsymbol{p}_i$ 与 $\boldsymbol{p}_{i+1}$ 之间有了动态依赖关系。

### 从 ODE 的角度理解

上述的积分方程可以转化为一个常微分方程（ODE）：
$$
\frac{d \boldsymbol{p}(t)}{d t} = \boldsymbol{h}(t, \boldsymbol{p}(t); \boldsymbol{\theta}_h)
$$

这个微分方程表示了位置向量 $\boldsymbol{p}(t)$ 随着时间 $t$ 的变化率 $\boldsymbol{h}$。$\boldsymbol{h}$ 是一个神经网络，用来学习如何更新位置编码。通过这种方式，FLOATER 可以捕捉到序列中位置之间更复杂的动态变化，使其具有比传统编码更强的表达能力和外推性。

在 Transformer 模型中，位置序列 $\{\boldsymbol{p}_i\}$ 是通过在一系列离散的时间点 $\{t_i\}$ 上取 $\boldsymbol{p}(t)$ 的值来获得的，即：
$$
\boldsymbol{p}_i = \boldsymbol{p}(t_i)
$$
为了简化，通常设定 $t_i = i \cdot \Delta t$，其中 $\Delta t$ 是一个超参数，控制每个时间步的间隔。也可以根据需要选择不等间隔的时间步。

### 与三角函数式位置编码的关系

FLOATER 位置编码的一大特点是它与传统的三角函数式位置编码有内在的联系。通过推导，FLOATER 的神经微分方程可以退化为三角函数式编码的特例。考虑三角函数式位置编码：
$$
p_{i+1}[j] - p_i[j] =
\begin{cases}
\sin\left((i+1) \cdot c^{\frac{j}{d}}\right) - \sin\left(i \cdot c^{\frac{j}{d}}\right), & \text{if } j \text{ is even} \\
\cos\left((i+1) \cdot c^{\frac{j-1}{d}}\right) - \cos\left(i \cdot c^{\frac{j-1}{d}}\right), & \text{if } j \text{ is odd}
\end{cases}
$$

通过积分表示，可以得到：
$$
\int_i^{i+1} c^{-\frac{j}{d}} \cos(\tau \cdot c^{\frac{j}{d}}) d\tau
$$

这表明，对于简单的正弦和余弦编码，也存在与之对应的动力系统 $\boldsymbol{h}(t, \boldsymbol{p}(t))$，即 FLOATER 能够涵盖这种经典的编码方式，但具备更高的灵活性。

### 共享参数与多层 Transformer 的应用

在实际的深度 Transformer 模型中，通常会包含多个编码层（例如，BERT 模型中的 12 层编码器）。每一层都可能使用不同的动力系统 $\boldsymbol{h}^{(n)}(t, \boldsymbol{p}(t); \boldsymbol{\theta}_h^{(n)})$ 来捕捉不同的位置信息变化。然而，这样会引入大量的参数，导致训练效率下降。

为了解决这一问题，FLOATER 通过**共享参数**的方法，减小了计算开销。具体来说，所有层共享同一个神经网络 $\boldsymbol{\theta}_h$，而不同层之间通过设置不同的初始条件来区分位置表示。即：
$$
\boldsymbol{p}^{(1)}(0) \neq \boldsymbol{p}^{(2)}(0)
$$
这种设计不仅节省了参数，还使得每层网络能够捕捉到不同的位置信息变化，提升了模型的表达能力。

### FLOATER 的外推性和灵活性

FLOATER 的一个关键优势是其卓越的**外推性**。传统位置编码通常限制在固定长度的序列内，无法有效处理超出训练范围的序列。而 FLOATER 通过动力系统的连续化建模，可以生成任意长度的位置信息。无论是训练时未见过的序列长度，还是动态变化的输入，FLOATER 都能自适应地生成高质量的位置信息。

此外，FLOATER 在处理**不等间隔时间序列**时也具有显著的优势。在许多任务中，输入数据可能并非等间隔观测（如时间序列数据中的不规则观测）。FLOATER 能够灵活适应这种情况，生成对应的位置信息，提升模型对动态输入的适应能力。

### 实际应用与优化

在实际应用中，FLOATER 可以通过端到端的方式训练，使用标准的反向传播方法优化神经网络 $\boldsymbol{\theta}_h$。FLOATER 在许多自然语言处理任务中表现优异，如机器翻译、文本生成等，特别是在处理长文本或动态输入的任务中，它的效果显著优于传统的位置编码方法。

通过结合现代数值积分方法（如 Runge-Kutta 方法），FLOATER 可以高效地计算位置编码的更新过程，从而在训练和推理阶段提供稳定的性能。

## 相乘式位置编码

相乘式位置编码是一种相对较新的方法，它通过将位置向量与输入向量相乘的方式来引入位置信息。这种方法的思路与传统的加法式位置编码不同，后者通常通过将位置信息加到输入向量上来编码位置。相乘式的位置编码则是通过乘法运算，使得输入向量和位置向量的交互变得更加复杂，从而可能捕捉到更丰富的位置信息。

### 数学表示

传统的加法式位置编码是将每个输入位置的向量 $x_k$ 与其对应的位置信息向量 $p_k$ 相加：
$$
x_k^{\text{new}} = x_k + p_k
$$

而在相乘式位置编码中，位置信息的引入方式是通过将输入向量 $x_k$ 与位置信息 $p_k$ 相乘，形式上可以表示为：
$$
x_k^{\text{new}} = x_k \times p_k
$$

这里，$p_k$ 是每个位置的编码向量，$x_k$ 是输入的原始表示。通过这种方式，相乘的过程能够更加直接地影响输入向量的幅度和方向，进而改变输入向量在高维空间中的位置。这种方式可能捕捉到输入与位置信息之间的更复杂的交互关系，从而增强模型对位置依赖关系的表达能力。

### 作用机制

相乘式位置编码的一个关键特性在于**非线性交互**。在传统的加法式位置编码中，位置信息和输入向量的影响是线性累加的，这意味着它们的相互作用比较简单，无法体现出高阶的复杂关系。而相乘式的位置编码通过乘法引入了一种非线性变化，能够在更高维度上捕捉输入与位置之间的复杂关系。

例如，如果输入向量和位置向量分别包含不同的语义信息，乘法可能会更好地融合这两类信息，产生新的特征表示。这在处理一些复杂的自然语言任务时可能具有更大的潜力，特别是当输入序列中的位置信息与语义强相关时。

### 相比加法式的优势

#### 更复杂的交互
相乘式位置编码通过乘法来表达位置信息和输入之间的非线性关系，这种更复杂的交互方式可以使得模型在捕捉位置依赖时具有更大的表达能力。传统的加法式编码仅仅是简单的叠加，可能在表达较复杂的位置信息时能力不足。

#### 参数共享
在某些模型中，相乘式位置编码可以通过引入共享的参数来控制位置向量的变化，使得每个位置的编码不仅仅依赖于该位置的位置信息，还可能与其他位置的位置信息相关联。这可以使模型在处理长序列时更具鲁棒性，并且能够在全局上更好地捕捉输入之间的关系。

### 局限性

虽然相乘式位置编码具有一定的潜力，但它的具体效果还在探索阶段。相比加法式编码，乘法带来的复杂性也可能带来以下问题：

- **数值不稳定**：乘法可能导致输入向量的幅度发生剧烈变化，特别是在某些情况下，位置向量的数值过大或过小，可能导致模型的梯度更新出现数值不稳定的现象，影响训练效果。
- **对特定任务的适用性**：相乘式位置编码的优势在于捕捉更复杂的位置依赖关系，但在某些任务中，这种复杂性并非必要，反而可能增加模型的复杂度和计算开销。
- **未经过充分验证**：相比于加法式位置编码，乘法式位置编码在主流自然语言处理任务中的应用较少，尚未得到广泛的验证。因此，虽然它在某些实验中展现了优越性，但在大规模任务中的普遍适用性仍需进一步研究。

# 相对位置编码

相对位置编码的核心思想是，它并不关注每个输入 Token 的**绝对位置**，而是强调输入 Token 之间的**相对位置关系**。这种编码方式特别适合自然语言处理任务，因为在大多数语言中，词语的相对顺序通常比其绝对位置更为重要。相对位置编码通过在模型的注意力机制中引入相对位置信息，使模型能够更加灵活地处理长文本或变长输入。

相较于传统的绝对位置编码，**相对位置编码**在保持模型性能的同时，具备更好的**外推性**，即它能适应未见过的长序列输入。

经典的**相对位置编码**概念最早由 Google 提出，来源于论文 [《Self-Attention with Relative Position Representations》](https://arxiv.org/abs/1803.02155)。这种方法通过在**注意力机制**中引入**相对位置信息**来增强模型的表现。与传统的**绝对位置编码**不同，**相对位置编码**更关注输入 Token 之间的相对距离，而不是它们的绝对位置。这使得它在自然语言处理任务中表现得尤为灵活，因为语言中的句法和语义依赖往往更强调相对顺序。

### 数学表示

在相对位置编码中，通过在计算**注意力分数**时引入位置偏置 $R_{i,j}$，用来表示输入 Token $i$ 和 Token $j$ 之间的相对位置。这些位置信息被添加到键向量的表示中，从而对注意力的计算产生影响。

公式为：
$$
a_{i,j} = \text{softmax} \left( \boldsymbol{x}_i \boldsymbol{W}_Q (\boldsymbol{x}_j \boldsymbol{W}_K + R_{i,j}^K)^\top \right)
$$

在这个公式中：
- $\boldsymbol{x}_i \boldsymbol{W}_Q$ 是查询向量 $i$ 的表示，
- $\boldsymbol{x}_j \boldsymbol{W}_K$ 是键向量 $j$ 的表示，
- $R_{i,j}^K$ 是表示 $i$ 和 $j$ 之间相对位置的偏置项。

此时，位置偏置 $R_{i,j}$ 被添加到键向量的表示中，以调整 Token 之间的注意力分数。这意味着，模型在分配注意力权重时，可以根据 Token 之间的相对距离动态调整，这对捕捉长距离依赖非常有帮助。

### 推导过程

通常在带有绝对位置编码的 Self-Attention 机制中，查询、键和值向量的计算如下：

$$
\boldsymbol{q}_{i} = \left( \boldsymbol{x}_{i} + \boldsymbol{p}_{i} \right) \boldsymbol{W}_Q, \quad \boldsymbol{k}_{j} = \left( \boldsymbol{x}_{j} + \boldsymbol{p}_{j} \right) \boldsymbol{W}_K, \quad \boldsymbol{v}_{j} = \left( \boldsymbol{x}_{j} + \boldsymbol{p}_{j} \right) \boldsymbol{W}_V
$$

其中 $\boldsymbol{p}_{i}$ 是位置编码。对于每对位置 $i$ 和 $j$ 的注意力计算：
$$
a_{i,j} = \operatorname{softmax} \left( \boldsymbol{q}_{i} \boldsymbol{k}_{j}^\top \right)
$$

为引入相对位置信息，Google 提出了将 $p_j W_K$ 替换为一个相对位置偏置项 $R_{i,j}^K$，并删除第一项，具体修改如下：
$$
a_{i,j} = \operatorname{softmax} \left( \boldsymbol{x}_i \boldsymbol{W}_Q \left( \boldsymbol{x}_j \boldsymbol{W}_K + R_{i,j}^K \right)^\top \right)
$$

此处，通过在键向量 $k_j$ 中引入 $R_{i,j}^K$，模型可以在计算每对 Token 的相对位置信息时更加精准。

对于输出值的计算，类似地，将原本的绝对位置偏置 $p_j W_V$ 替换为相对位置偏置 $R_{i,j}^V$：
$$
\boldsymbol{o}_i = \sum_j a_{i,j} \left( \boldsymbol{x}_j \boldsymbol{W}_V + R_{i,j}^V \right)
$$

这种机制极大增强了 Transformer 模型在处理长文本时的灵活性，尤其在序列长度远超出训练时最大长度的场景下，也能保持较好的泛化能力。

### 位置截断

为了使相对位置编码能够处理任意长度的文本序列，Google 引入了**位置截断**机制。当 Token $i$ 和 Token $j$ 之间的距离超出某个阈值时，位置偏置 $R_{i,j}$ 会被截断到一个预设的范围。这一机制确保了模型在处理超长序列时，远距离的 Token 不会产生过多的注意力干扰。

截断处理后的公式为：
$$
R_{i,j}^K = p_K \left[ \operatorname{clip}(i - j, p_{\min}, p_{\max}) \right]
$$
$$
R_{i,j}^V = p_V \left[ \operatorname{clip}(i - j, p_{\min}, p_{\max}) \right]
$$

通过这种截断机制，模型只需要有限个位置编码就可以表示任意长度的相对位置。这使得相对位置编码具备极强的外推性，不受序列长度的限制。

### 优点

1. **外推性**：通过使用相对位置编码，模型可以处理超出训练长度的输入序列，不需要重新训练或调整模型参数。这使得 Transformer 在长文本处理任务中具有出色的表现。
   
2. **捕捉相对顺序**：相对位置编码允许模型更好地捕捉句子中词语的相对顺序，而不是依赖绝对位置信息。这种机制在处理长依赖关系的任务中尤为重要。

3. **灵活性**：相对位置编码无需对序列的长度进行固定，能够适应任意长度的输入，从而提升模型的鲁棒性。

### 进一步扩展

在相对位置编码的基础上，后续模型如 XLNet 和 T5 对这一机制进行了进一步的优化。

- **XLNet 式位置编码**：通过引入可训练的全局向量 $u$ 和 $v$，使得模型在处理长距离依赖时更加灵活，进一步提升了对相对位置的建模能力。

- **T5 式位置编码**：T5 引入了更加简化的相对位置编码，通过直接在 Attention 矩阵中加入一个可训练的偏置项，减少了计算复杂度，同时仍然保持了对相对位置信息的有效捕捉。

在这些扩展的基础上，Transformer 模型的外推能力和在长文本上的表现得到了进一步提升。

## XLNet 式相对位置编码

XLNet 式相对位置编码来源于 [Transformer-XL](https://arxiv.org/abs/1901.02860 "Transformer-XL") 的改进。尽管 Transformer-XL 最早提出了相对位置编码的概念，但由于 XLNet 模型在一定程度上超越了 BERT，这种相对位置编码机制广泛被称为 XLNet 式编码。它通过引入相对位置向量 $R_{i-j}$ 和可训练的全局向量 $u$ 和 $v$，增强了对长序列的处理能力。

### 数学表示

在经典 Transformer 模型中，查询向量 $\boldsymbol{q}_i$ 和键向量 $\boldsymbol{k}_j$ 的内积计算如下：

$$
\boldsymbol{q}_i \boldsymbol{k}_j^\top = \boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top
$$

其中，$\boldsymbol{x}_i$ 是输入序列中的第 $i$ 个 Token 的表示，$\boldsymbol{W}_Q$ 和 $\boldsymbol{W}_K$ 分别是查询和键的线性变换矩阵。

Google的 $\boldsymbol{q}_i \boldsymbol{k}_j^\top$ 完全展开为：

$$
\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top}=\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}+\boldsymbol{p}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{p}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}
$$

在原始的相对位置编码中，只考虑了 Token 之间的相对距离。然而，XLNet 进一步引入了两个可训练的全局向量 $u$ 和 $v$，以替代部分位置偏置的功能。具体公式如下：
$$
\boldsymbol{q}_i \boldsymbol{k}_j^\top = \boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top + \boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{R}_{i-j}^\top + u \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top + v \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{R}_{i-j}^\top
$$

- **第一项** $\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top$ 是传统查询和键向量的内积，反映了 Token 之间的注意力分布。
- **第二项** $\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{R}_{i-j}^\top$ 引入了相对位置信息 $R_{i-j}$，用来表示 Token $i$ 和 Token $j$ 之间的距离。
- **第三项和第四项** 则通过引入可训练的全局向量 $u$ 和 $v$，进一步增强了模型对相对位置的捕捉能力。

### 权重矩阵调整

为了处理相对位置向量 $R_{i-j}$ 与输入向量 $\boldsymbol{x}_j$ 在编码空间不一致的问题，XLNet 使用了一个独立的矩阵 $\boldsymbol{W}_{K,R}$ 对相对位置偏置进行权重调整，还有 $\boldsymbol{u} \boldsymbol{W}_{Q}$ 、 $\boldsymbol{v} \boldsymbol{W}_{Q} $ 可以直接合并为单个 $\boldsymbol{u} $、$\boldsymbol{v}$ ，最终的公式如下：

$$
\boldsymbol{q}_i \boldsymbol{k}_j^\top = \boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K, R}^{\top} \boldsymbol{R}_{i-j}^{\top}+\boldsymbol{u} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{v} \boldsymbol{W}_{K, R}^{\top} \boldsymbol{R}_{i-j}^{\top}
$$

这种方式确保了相对位置编码能够在不同编码空间下正确地进行计算。

> **为什么可以合并 $\boldsymbol{u} \boldsymbol{W}_{Q}$ 和 $\boldsymbol{v} \boldsymbol{W}_{Q}$：** 在自注意力机制中，$\boldsymbol{u}$ 和 $\boldsymbol{v}$ 是两个可训练的全局向量，它们用来调整**查询向量和相对位置向量的权重**。这两个向量本质上是全局参数，可以看作是对注意力矩阵的全局修正项，因为它们与 $\boldsymbol{W}_Q$ 的作用是相似的。
>
> 由于 $\boldsymbol{u}$ 和 $\boldsymbol{v}$ 都参与相对位置编码的计算，并且都乘以相同的 $\boldsymbol{W}_Q$，这意味着我们可以将 $\boldsymbol{u} \boldsymbol{W}_Q$ 和 $\boldsymbol{v} \boldsymbol{W}_Q$ 合并为单个 $\boldsymbol{u}$ 和 $\boldsymbol{v}$，因为 $\boldsymbol{W}_Q$ 的矩阵变换在这个部分对查询和相对位置向量作用是统一的。这样做既能简化模型的计算，也不会影响模型的表现。

### 输出计算的简化

在 XLNet 的实现中，位置偏置在输出向量 $\boldsymbol{o}_i$ 上被去除，模型直接使用加权后的输入向量进行输出计算：

$$
\boldsymbol{o}_i = \sum_j a_{i,j} \boldsymbol{x}_j \boldsymbol{W}_V
$$

这一简化使得模型在处理注意力分数时，计算更加高效，同时保持了对位置信息的良好捕捉。

### 优点

1. **长距离依赖捕捉**：通过引入全局向量 $u$ 和 $v$，XLNet 式相对位置编码增强了模型对长距离依赖的建模能力。这使得模型在处理长文本任务时表现尤为出色。
   
2. **灵活性**：与传统的相对位置编码相比，XLNet 提供了更多的灵活性。它可以适应不同长度的输入序列，并通过可训练的向量进一步优化对位置信息的表达。

3. **无截断限制**：XLNet 采用 Sinusoidal 生成方案，不对相对位置信息进行截断。这使得模型具备更好的外推性，能够处理比训练时更长的输入序列。

4. **高效性**

## T5 式相对位置编码

T5 模型（出自文章《[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)》）采用了一种更为简化的相对位置编码方法。相较于其他 Transformer 模型，T5 的相对位置编码设计思想基于展开公式
$$
\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top}=\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}+\boldsymbol{p}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{p}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}
$$
通过剔除部分不必要的注意力计算来简化模型。其核心思路可以分为四类注意力项：“输入-输入”、“输入-位置”、“位置-输入”、“位置-位置”。T5 模型假设输入信息与位置信息是解耦的，避免了不必要的交互，因此它去掉了“输入-位置”和“位置-输入”这两项注意力计算。

具体来说，公式可以简化为：

$$
\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top + \boldsymbol{\beta}_{i,j}
$$

其中，$\boldsymbol{\beta}_{i,j}$ 是一个依赖于位置 $i$ 和 $j$ 的可训练的偏置项。通过这种方式，T5 模型直接在 Attention 矩阵上加了一个可训练的偏置项，而不是像其他模型那样复杂地处理相对位置信息。

### 注意力矩阵的简化

与 XLNet 类似，T5 模型将 $v_j$（值向量）上的位置偏置也直接去掉了。这种做法使得模型在处理时更加高效，并减少了不必要的计算。

这一思路不仅被应用在 T5 中，微软在 ICLR 2021 的论文《[Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595)》中提出的 TUPE 位置编码也采用了类似的机制，简化了位置与输入之间的交互。

### 相对位置的"分桶"处理

T5 的另一个创新之处在于它对相对位置进行了"分桶"处理。具体来说，相对位置 $i-j$ 不再是直接通过距离来表示，而是通过一个映射函数 $f(i-j)$，将相对距离映射到一个较为紧凑的编码空间。这一分桶操作旨在减少较远位置的复杂性。其具体映射关系如下表所示：

| $i-j$    | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |
| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| $f(i-j)$ | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 8    | 8    | 8    | 9    | 9    | 9    | 9    |
| $i-j$    | 16   | 17   | 18   | 19   | 20   | 21   | 22   | 23   | 24   | 25   | 26   | 27   | 28   | 29   | 30   | ...  |
| $f(i-j)$ | 10   | 10   | 10   | 10   | 10   | 10   | 10   | 11   | 11   | 11   | 11   | 11   | 11   | 11   | 11   | ...  |

### 分桶处理的设计思路

这种分桶处理的基本逻辑是，**距离较近的相对位置需要更精细的编码**，例如 $i-j$ 在 0 到 7 范围内的位置都被分配了独立的编码，而较远的位置（如 $i-j$ 在 8 以上）可以共用相同的编码。随着距离的增大，映射的范围会逐渐变大，直到达到指定范围后进行截断处理。

这种设计思想直观且有效，尤其适合需要处理长文本的任务。通过减少远距离位置的复杂性，模型能够更加高效地处理长距离依赖问题，同时保持对近距离相对位置的准确表示。

### 优点

1. **简化计算**：T5 通过去掉“输入-位置”和“位置-输入”注意力项，以及简化位置编码的计算，大幅减少了计算量，从而提升了模型的效率。
   
2. **灵活性**：分桶处理使得 T5 可以灵活处理长文本，不需要对每个相对位置进行单独编码，而是通过映射到少量的桶中来减少远距离依赖的复杂性。

3. **可训练的偏置项**：通过加入 $\boldsymbol{\beta}_{i,j}$ 作为可训练的偏置，T5 可以动态调整不同位置的注意力分布，增强了模型对不同位置依赖的表达能力。

## DeBERTa 式位置编码

DeBERTa（Decoding-enhanced BERT with Disentangled Attention）是微软提出的一种改进版 BERT 模型，首次发表于 2020 年 6 月的论文《[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)》。DeBERTa 近期在 ICLR 2021 上取得了正式认可，并且在 SuperGLUE 榜单上稍微超过了 T5，登上了榜首。

DeBERTa 的核心改进主要体现在**位置编码**上。虽然同样基于位置编码展开的基本框架，但其设计与 T5 位置编码大相径庭。具体来说，T5 将注意力计算中的第二项（输入-位置交互）和第三项（位置-输入交互）去掉，仅保留了输入-输入和位置-位置的计算；而 DeBERTa 则反其道而行，**删除了位置-位置交互项**，并保留了输入-位置和位置-输入的计算部分。

### 数学公式

DeBERTa 的位置编码可以通过如下公式表示：

$$
\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top} = \boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top} + \boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{R}_{i,j}^{\top} + \boldsymbol{R}_{j,i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}
$$

- 第一项：$\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top$ 表示查询向量和键向量的标准内积。
- 第二项：$\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{R}_{i,j}^\top$ 引入了位置向量 $R_{i,j}$，它描述了输入序列中的相对位置信息。
- 第三项：$\boldsymbol{R}_{j,i} \boldsymbol{W}_Q \boldsymbol{W}_K^\top \boldsymbol{x}_j^\top$ 同样是相对位置信息的表述，但从另一个角度对键向量做了位置偏移的调整。

### 位置编码的设计

DeBERTa 中的相对位置编码依旧基于相对位置信息的设计，与 Transformer-XL 和 XLNet 相似，都是通过相对距离 $i-j$ 计算相对位置偏置 $R_{i,j}$。这些偏置项对远距离 Token 之间的注意力计算非常重要，并且 DeBERTa 通过截断机制处理了超长序列的位置编码，确保模型在长文本任务中依然有很好的表现。

不过，DeBERTa 的设计巧妙之处在于其对相对和绝对位置编码的综合运用。DeBERTa 提出了一个新视角，认为 NLP 任务中的大多数场景仅需相对位置信息，但在某些特定场景下，**绝对位置信息** 也可能会对模型性能产生帮助。因此，DeBERTa 将整个模型的结构划分为两部分：

1. **Encoder 部分**：前 11 层 Transformer 仅使用相对位置编码，专注于捕捉输入序列中 Token 之间的相对位置信息。
2. **Decoder 部分**：最后两层引入了绝对位置信息，通过一个称为 EMD（Enhanced Mask Decoder）的模块增强模型对绝对位置信息的捕捉。

### EMD 机制与微调策略

EMD（Enhanced Mask Decoder）是 DeBERTa 引入的一个重要模块，尤其适用于一些需要绝对位置信息的下游任务。在预训练模型的 Base 版中，前 11 层的 Transformer 主要用于捕捉 Token 之间的相对位置信息，而最后两层则加入了绝对位置信息。

在下游任务微调时，DeBERTa 只会使用前 11 层的 Encoder 加上一层 Decoder。通过这种灵活的设计，DeBERTa 能够在需要相对位置信息的任务中表现出色，同时在少数场景下，也能够利用绝对位置信息提升表现。

# 其他位置编码

除了经典的绝对位置编码和相对位置编码之外，还有一些较为独特、创新的编码方式。这些方法虽然不按常规套路出牌，但在特定的应用场景中展现了良好的表现，尤其是在对位置信息的表达和建模上引入了新的思路。

## CNN 式位置编码

尽管传统卷积神经网络（CNN）在处理自然语言处理任务时不依赖显式的位置编码，但它们的位置信息实际上通过**Zero Padding** 实现了一种隐式的位置泄露。这一点在论文 [《How Much Position Information Do Convolutional Neural Networks Encode?》](https://arxiv.org/abs/2001.08248 "How Much Position Information Do Convolutional Neural Networks Encode?") 中得到了系统性分析。研究表明，Zero Padding 的引入使得 CNN 模型能够分辨不同位置的 Token。这是因为在卷积操作中，输入的边界通过填充的零向量改变了卷积核的输出，从而包含了一定的位置信息。

### 原理解析
在图像处理中，CNN 的局部性和共享权重使得它不需要显式的位置编码。然而，当卷积核跨越输入的边缘时，由于使用了填充（padding），这些填充操作泄露了位置信息。因此，卷积核的输出不仅依赖于输入的内容，还受到了这些填充零的影响。这种现象赋予了 CNN 模型隐式的位置信息捕捉能力。

### 优点
- **隐式位置信息**：通过 Zero Padding，CNN 模型能够在无需显式位置编码的情况下，捕捉输入中的位置信息。
- **局部特征捕捉**：CNN 本身的结构设计适合捕捉局部上下文信息，这种方式天然地具有位置感知能力。

### 局限性
- **信息损失**：虽然 Zero Padding 可以泄露位置信息，但其位置信息捕捉并不精确，在处理长序列时，效果会受到限制。

## 复数式位置编码

复数式位置编码是目前位置编码中最为独特的一种方案，它结合了复数的数学性质，提出了一种全新的位置编码思路。该方法来源于 ICLR 2020 论文《[Encoding Word Order in Complex Embeddings](https://arxiv.org/abs/1912.12333)》，其核心思想是通过复数的幅角和模来表示位置信息。相比于传统的实数位置编码，复数位置编码在捕捉序列中复杂的相对位置信息时表现出了不同的特点。

### 数学形式

复数式位置编码（Complex Order）的公式如下：

$$
\left[r_{j, 1} e^{\mathrm{i}\left(\omega_{j, 1} k + \theta_{j, 1}\right)}, \ldots, r_{j, 2} e^{\mathrm{i}\left(\omega_{j, 2} k + \theta_{j, 2}\right)}, \ldots, r_{j, d} e^{\mathrm{i}\left(\omega_{j, d} k + \theta_{j, d}\right)}\right]
$$

其中：
- $i$ 是虚数单位，代表 $i^2 = -1$；
- $j$ 代表某个词；
- $k$ 代表该词在句子中的位置；
- $r_{j,1}, \omega_{j,1}, \theta_{j,1}$ 是与词 $j$ 相关的幅值、频率和相位角度。

该公式中，复数的模（$r_{j,n}$）和幅角（$\omega_{j,n} k + \theta_{j,n}$）共同决定了位置信息的表示。这种表示方式允许模型通过频率和相位的变化，捕捉到词语的位置信息。

### 三组词向量

复数位置编码不仅仅使用单一的向量，而是为每个词生成了三组词向量：

$$
\begin{aligned}
\boldsymbol{r}_{j} &= \left[r_{j, 1}, r_{j, 2}, \cdots, r_{j, d}\right] \\
\boldsymbol{\omega}_{j} &= \left[\omega_{j, 1}, \omega_{j, 2}, \cdots, \omega_{j, d}\right] \\
\boldsymbol{\theta}_{j} &= \left[\theta_{j, 1}, \theta_{j, 2}, \cdots, \theta_{j, d}\right]
\end{aligned}
$$

这三组向量分别对应幅值、频率和相位角度。通过这些向量，模型可以为每个词生成与位置信息相关的复数表示。值得注意的是，尽管公式中显示了三组向量，但在实际应用中，这些向量可以通过参数共享的方式进行简化，减少计算量。

复数式位置编码的与众不同之处不仅仅在于使用了复数，还在于它**完全依赖于复数模型**。输入的嵌入层是复数的，且模型中的每一层 Transformer 都是复数计算的，这在传统的实数嵌入和模型中是很少见的。

通过这种方法，研究人员还比较了复数版的 FastText、LSTM、CNN 等模型的表现。相比于实数模型，复数模型在某些自然语言处理任务中展现出了良好的表现，特别是处理复杂的词序和位置关系时。

### 应用与优势

复数模型虽然目前应用较少，但它具有一些独特的优势：
1. **扩展性强**：通过复数的相位角和幅值，模型能够更自然地表示位置信息的变化，特别是捕捉到句子中复杂的相对位置关系。
2. **多样化的向量表示**：复数模型允许每个词与其位置之间的关系以多组参数表示，提供了更多维度的表达方式。
3. **数学上的优雅性**：复数在处理周期性和波动性变化时具备独特优势，因此适合于表示自然语言中一些潜在的规律性变化。

## RoPE：旋转位置编码

**RoPE**（Rotary Position Embedding）是一种将**绝对位置编码**与**相对位置编码**结合起来的混合方法，能够在保持位置信息的基础上，实现对相对位置信息的有效捕捉。RoPE 的独特之处在于它通过在**自注意力计算**过程中引入位置向量的旋转变换来完成位置编码。这种方法最早由苏神提出，并应用在语言模型 RoFormer 中。后来，谷歌的 Palm 和 Meta 的 LLaMA 也采用了这一位置编码方案。RoPE 的详细推导可参考苏神的博文 [《RoPE：旋转位置编码》](https://spaces.ac.cn/archives/8130 "RoPE：旋转位置编码")。

### 数学表示

RoPE 的核心公式为：
$$
\tilde{\boldsymbol{q}}_m = \boldsymbol{q} e^{i m \theta}, \quad \tilde{\boldsymbol{k}}_n = \boldsymbol{k} e^{i n \theta}
$$

- $\boldsymbol{q}$ 和 $\boldsymbol{k}$ 是查询和键的向量，
- $m$ 和 $n$ 分别表示两个 Token 的相对位置，
- $\theta$ 是旋转角度。

通过将查询向量 $\boldsymbol{q}$ 和键向量 $\boldsymbol{k}$ 进行复数旋转，可以在自注意力的内积运算中引入相对位置信息。这种旋转操作使得 RoPE 不仅能够保持对绝对位置的编码，还能有效捕捉到相对位置信息。

### 进一步公式推导与解释

RoPE 的进一步数学推导可以用复数的形式来表示位置编码。我们可以将位置向量 $\boldsymbol{q}_m$ 和 $\boldsymbol{k}_n$ 通过一个旋转矩阵进行变换：

$$
\boldsymbol{f}(\boldsymbol{q}, m) = \left(\begin{array}{cc}\cos m \theta & -\sin m \theta \\ \sin m \theta & \cos m \theta\end{array}\right)\left(\begin{array}{l}q_{0} \\ q_{1}\end{array}\right)
$$



在这个公式中：

- $\theta$ 是一个固定的旋转角度，与位置信息相关，
- 通过将每个位置的向量进行旋转，模型在处理 Token 的相对位置信息时更为自然。

通过这个旋转矩阵，我们可以在**自注意力机制**中，将绝对位置信息转化为相对位置信息，从而提升模型在处理长序列和复杂文本时的能力。

RoPE 还进一步加入了三角函数的远程衰减机制，来增强其对长距离 Token 的感知能力：

$$
\left(\boldsymbol{W}_{m} \boldsymbol{q}\right)^{\top}\left(\boldsymbol{W}_{n} \boldsymbol{k}\right) = \operatorname{Re}\left[\sum_{i=0}^{d / 2 - 1} \boldsymbol{q}_{[2 i: 2 i+1]} \boldsymbol{k}_{[2 i: 2 i+1]}^{*} e^{\mathrm{i}(m-n) \theta_{i}}\right]
$$

这个公式表明，随着 Token 之间的相对距离增加，旋转的幅度会逐渐减小，反映了相对位置的重要性随距离增加而逐渐减弱的特点。

### 更加Mathematical的推导

通过线性attention演算，现在q和k向量中引入绝对位置信息：

$$
\tilde{\boldsymbol{q}}_{m}=\boldsymbol{f}(\boldsymbol{q}, m), \quad \tilde{\boldsymbol{k}}_{n}=\boldsymbol{f}(\boldsymbol{k}, n)
$$

但是需要实现相对位置编码的话，需要显式融入相对。attention运算中q和k会进行内积，所以考虑在进行向量内积时考虑融入相对位置。所以假设成立恒等式：

$$
\langle\boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k}, n)\rangle=g(\boldsymbol{q}, \boldsymbol{k}, m-n)
$$

其中`m-n`包含着token之间的相对位置信息。

给上述恒等式计算设置初始条件，例如$f(q,0)=q$，$f(k,0)=k$。

求解过程使用复数方式求解

将内积使用复数形式表示：

$$
\langle\boldsymbol{q}, \boldsymbol{k}\rangle=\operatorname{Re}\left[\boldsymbol{q} \boldsymbol{k}^{*}\right]
$$

转化上面内积公式可得：

$$
\operatorname{Re}\left[\boldsymbol{f}(\boldsymbol{q}, m) \boldsymbol{f}^{*}(\boldsymbol{k}, n)\right]=g(\boldsymbol{q}, \boldsymbol{k}, m-n)
$$

假设等式两边都存在复数形式，则有下式：

$$
\boldsymbol{f}(\boldsymbol{q}, m) \boldsymbol{f}^{*}(\boldsymbol{k}, n)=\boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)
$$

将两边公式皆用复数指数形式表示：

存在$r e^{\theta \mathrm{j}}=r \cos \theta+r \sin \theta \mathrm{j}$，即任意复数$z$可以表示为$\boldsymbol{z}=r e^{\theta \mathrm{j}}$，其中$r$为复数的模，$\theta$为幅角。

$$
\begin{aligned} \boldsymbol{f}(\boldsymbol{q}, m) & =R_{f}(\boldsymbol{q}, m) e^{\mathrm{i} \Theta_{f}(\boldsymbol{q}, m)} \\ \boldsymbol{f}(\boldsymbol{k}, n) & =R_{f}(\boldsymbol{k}, n) e^{\mathrm{i} \Theta_{f}(\boldsymbol{k}, n)} \\ \boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) & =R_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) e^{\mathrm{i} \Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)}\end{aligned}
$$

由于带入上面方程中$f(k,n)$带\*是共轭复数，所以指数形式应该是$e^{-x}$形式，带入上式公式可得方程组：

$$
\begin{aligned} R_{f}(\boldsymbol{q}, m) R_{f}(\boldsymbol{k}, n) & =R_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) \\ \Theta_{f}(\boldsymbol{q}, m)-\Theta_{f}(\boldsymbol{k}, n) & =\Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)\end{aligned}
$$

第一个方程带入条件$m=n$化简可得：

$$
R_{f}(\boldsymbol{q}, m) R_{f}(\boldsymbol{k}, m)=R_{g}(\boldsymbol{q}, \boldsymbol{k}, 0)=R_{f}(\boldsymbol{q}, 0) R_{f}(\boldsymbol{k}, 0)=\|\boldsymbol{q}\|\|\boldsymbol{k}\|
$$

$$
R_{f}(\boldsymbol{q}, m)=\|\boldsymbol{q}\|, R_{f}(\boldsymbol{k}, m)=\|\boldsymbol{k}\|
$$

从上式可以看出来复数$f(q,m)$和$f(k,m)$与$m$取值关系不大。

第二个方程带入$m=n$化简可得：

$$
\Theta_{f}(\boldsymbol{q}, m)-\Theta_{f}(\boldsymbol{k}, m)=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, 0)=\Theta_{f}(\boldsymbol{q}, 0)-\Theta_{f}(\boldsymbol{k}, 0)=\Theta(\boldsymbol{q})-\Theta(\boldsymbol{k})
$$

上式公式变量两边挪动下得到：

$$
\Theta_{f}(\boldsymbol{q}, m)-\Theta_{f}(\boldsymbol{k}, m)=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, 0)=\Theta_{f}(\boldsymbol{q}, 0)-\Theta_{f}(\boldsymbol{k}, 0)=\Theta(\boldsymbol{q})-\Theta(\boldsymbol{k})
$$

其中上式结果相当于m是自变量，结果是与m相关的值，假设为 $\varphi(m)$，即$\Theta_{f}(\boldsymbol{q}, m)=\Theta(\boldsymbol{q})+\varphi(m)$

`n`假设为`m`的前一个token，则可得`n=m-1`，带入上上个式子可得：
$$
\varphi(m)-\varphi(m-1)=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, 1)+\Theta(\boldsymbol{k})-\Theta(\boldsymbol{q})
$$

即 $\varphi(m)$是等差数列，假设等式右边为 $\theta$ ，则`m`和`m-1`位置的公差就是为$\theta$，可推得 $\varphi(m)=m \theta$。

得到二维情况下用复数表示的RoPE：

$$
\boldsymbol{f}(\boldsymbol{q}, m)=R_{f}(\boldsymbol{q}, m) e^{\mathrm{i} \Theta_{f}(\boldsymbol{q}, m)}=\|q\| e^{\mathrm{i}(\Theta(\boldsymbol{q})+m \theta)}=\boldsymbol{q} e^{\mathrm{i} m \theta}
$$

矩阵形式是：

$$
\boldsymbol{f}(\boldsymbol{q}, m)=\left(\begin{array}{cc}\cos m \theta & -\sin m \theta \\ \sin m \theta & \cos m \theta\end{array}\right)\left(\begin{array}{l}q_{0} \\ q_{1}\end{array}\right)
$$

公式最后还会采用三角式一样的远程衰减，来增加周期性函数外推位置差异性。

$$
\left(\boldsymbol{W}_{m} \boldsymbol{q}\right)^{\top}\left(\boldsymbol{W}_{n} \boldsymbol{k}\right)=\operatorname{Re}\left[\sum_{i=0}^{d / 2-1} \boldsymbol{q}_{[2 i: 2 i+1]} \boldsymbol{k}_{[2 i: 2 i+1]}^{*} e^{\mathrm{i}(m-n) \theta_{i}}\right]
$$

### 优点

- **相对位置感知**：通过旋转操作，RoPE 能够直接在注意力机制中捕捉输入序列的相对位置信息，而不需要额外的相对位置偏置。
- **应用广泛**：RoPE 已经在多个大规模预训练模型中得到了应用，并展示了良好的效果。

### 局限性

- **复杂度增加**：尽管 RoPE 能够在注意力计算中引入相对位置信息，但其旋转操作会增加一定的计算复杂度，尤其是在处理大规模序列时。

# 长度外推问题

在 Transformer 模型的应用场景中，长度外推性问题是指模型在训练时使用较短的输入序列，而在推理时仍然能够处理更长的序列。随着任务的复杂化和文本长度的增加，如何确保模型在长文本上的表现与训练阶段一致，成为了 Transformer 模型设计中的一个重要挑战。

长度外推性问题的核心在于：训练时序列的最大长度往往受到计算资源的限制，但在推理时，输入序列可能超出训练时的长度范围。由于 Transformer 模型的结构依赖于位置编码来区分输入中的不同位置，如何扩展这一编码使其适应更长的输入，成为了解决这一问题的关键。

研究人员提出了多种方法来解决长度外推问题，其中包括**进制表示**、**线性插值**和**进制转换**等方法。

## 进制表示

**进制表示**是一种通过改变位置编码方式，来提升模型外推性的方法。传统的位置编码通常采用十进制表示，即用一个固定的数值范围来表示位置。在进制表示方法中，通过将位置编码转换为高进制表示（如 16 进制、32 进制等），可以有效减少序列的跨度。例如，十进制表示可能会让较长序列的位置信息变得稀疏且难以区分，而使用 16 进制编码可以在相同的表示长度下，涵盖更多的位置范围。

这种方法的主要优势在于：在不增加模型复杂度的前提下，可以处理更长的序列。此外，进制表示方法还允许模型对不同进制的表示进行训练，使得模型具备更广泛的泛化能力。

### 优点
- **减少编码跨度**：进制表示通过高进制的形式减少位置编码的跨度，能够有效处理长序列。
- **灵活性强**：进制表示方法能够适应不同的任务需求，根据任务的复杂性选择合适的进制。

### 局限性
- **精度可能不足**：在处理极长序列时，进制转换可能会导致精度的损失，尤其是当输入序列具有较多复杂结构时。

## 线性插值

**线性插值**方法通过压缩或拉伸长序列的位置编码，使得原本长序列的位置信息能够适应较短序列的模型。这种方法的核心思想是将超长序列的位置信息投影到训练时的较短序列中，从而实现对长序列的处理。

在线性插值中，输入序列的位置被缩放到较短的范围内。假设训练时最大长度为 512，而推理时输入长度为 1024，线性插值会将长度 1024 的序列的位置映射到 512 的范围内，从而允许模型处理较长的输入。

![](https://github.com/wdndev/llm_interview_note/raw/main/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/3.%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/image/image_iDIO40wFKR.png)

从绝对数值来看，新的[7,4,9]实际上对应的是1498，是原本对应的2倍，映射方式不一致；从相对数值来看，原本相邻数字的差距为1，现在是0.5，最后一个维度更加“拥挤”。所以，做了内插修改后，通常都需要微调训练，以便模型重新适应拥挤的映射关系。不过，内插方案也不尽完美，当处理范围进一步增大时，相邻差异则更小，并且这个相邻差异变小集中在个位数，剩下的百位、十位，还是保留了相邻差异为1。换句话说，**内插方法使得不同维度的分布情况不一样，每个维度变得不对等起来，模型进一步学习难度也更大**。

### 优点
- **简单直接**：线性插值通过简单的缩放操作实现外推，易于实现。
- **适应性强**：线性插值可以适应不同长度的输入，不受限于固定的最大长度。

### 局限性
- **需要微调**：由于插值会改变位置信息的分布，模型通常需要在新的输入上重新微调，以保证性能。

## 直接外推与进制转换

**直接外推**是指直接扩展模型在训练时学习到的位置编码，使其可以处理更长的序列。然而，直接外推的效果往往有限，尤其是当训练数据中没有出现过超长序列时，模型的外推能力较弱。

![](https://github.com/wdndev/llm_interview_note/raw/main/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/3.%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/image/image_KqvSV8SjTV.png)

相比之下，**进制转换**是一种更加有效的外推方法。通过将位置编码转换为不同的进制表示，进制转换能够在不增加输入维度的情况下，处理更长的序列。例如，在处理较长文本时，可以将十进制的位置编码转换为二进制或十六进制，这样可以覆盖更广的长度范围。

### 优点
- **不增加输入维度**：进制转换在处理长序列时，不会额外增加输入维度，保持了模型的简单性。
- **外推性强**：进制转换通过不同进制的切换，能够有效处理更长的输入序列，提升了模型的泛化能力。

### 局限性
- **实现复杂**：进制转换涉及到对位置编码的重新设计，可能需要在不同任务中进行适配。

### 总结

长度外推问题是 Transformer 模型在处理长序列任务中的一大挑战。通过进制表示、线性插值和进制转换等方法，研究人员为解决这一问题提供了多种途径。每种方法都有其优缺点，适用于不同的任务场景。未来，随着模型的规模和输入序列长度的不断增加，这些方法可能会进一步优化和发展。



# 参考

1. 苏剑林. (Feb. 03, 2021). 《[让研究人员绞尽脑汁的Transformer位置编码](https://spaces.ac.cn/archives/8130) 》
2. [大模型语言架构-位置编码](https://github.com/wdndev/llm_interview_note/blob/main/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/3.%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/3.%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81.md)