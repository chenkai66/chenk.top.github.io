---
title: Integrating Large Language Models with Graphical Session-Based Recommendation
tags:
  - GNN
  - Recommend System
  - LLM
categories: Paper
date: 2024-08-31 9:00:00
mathjax: true

---

本文提出了一种名为LLMGR的新框架，旨在将大语言模型（LLM）与图神经网络（GNN）结合，用于解决会话推荐任务（SBR）中的数据稀疏问题。传统的SBR方法依赖于用户交互数据，而忽略了文本信息的丰富性，限制了对用户行为的理解。LLMGR通过设计多任务提示（Prompts），并利用混合编码层，将文本信息与图结构数据相结合，增强了推荐系统对用户行为模式的捕捉能力。框架采用两阶段的提示调优策略，首先构建节点与文本信息的联系，然后捕捉会话中的行为模式。实验结果表明，LLMGR在多个真实数据集上显著优于现有的SOTA方法，尤其在冷启动场景下展现出强大的推荐性能。

<!-- more -->

# 背景介绍

会话推荐系统（SBR）主要依赖于用户的交互序列来进行推荐，近年来GNN因为能够捕捉物品之间的隐式关系和复杂转换，逐渐成为了SOTA（State of the Art）方法。然而，传统的基于图的推荐方法主要依赖于用户交互数据，而忽略了用户与物品相关的文本信息，这限制了捕捉用户交互中隐含的语境和上下文。

[论文原文链接](https://arxiv.org/pdf/2402.16539)

## 基于图的推荐方法背景

基于图的推荐系统（Graph-based Recommendation Systems，GRS）使用图神经网络（GNN）等图结构学习算法来建模用户与物品之间的关系。在推荐系统中，用户的交互数据往往以某种形式的网络或图来表示。例如，用户与商品的点击、购买或评分可以被看作是图中的节点与边，节点代表用户和物品，边则表示交互行为。

### **SR-GNN（Session-based Recurrent Graph Neural Network）**

**SR-GNN** 是一种经典的基于图神经网络（GNN）的会话推荐方法，它主要用于捕捉用户在会话（session）中的行为序列，并预测用户可能点击的下一个物品。该模型的主要思想是将用户在会话中的物品点击行为转化为图结构，通过GNN学习物品之间的转移模式。

#### 核心思路

- **会话图构建**：首先，将用户在一个会话中的点击序列转换为一个有向图，节点表示用户点击的物品，边表示物品之间的点击顺序。这样就可以将用户的行为序列转化为图结构。GNN 在这个图上进行信息传播（message passing），通过聚合邻居节点的信息，学习每个物品的表示。这部分的输出是每个物品节点的嵌入向量，表示物品在当前会话中的状态和与其他物品的关系。虽然 SR-GNN 对每个会话单独构建图，但在处理多个用户时，模型可以通过学习所有会话中的物品转移模式，生成**共享的物品嵌入**。这些物品嵌入不仅反映了单个会话的局部结构，还会通过模型的参数共享机制，捕捉跨用户、跨会话的物品间全局关系。换句话说，虽然图是针对单个会话构建的，但通过模型训练，所有会话的图都将贡献给模型的全局学习。

- **会话表示的生成**：在经过 GNN 的信息传播后，SR-GNN 获取了每个物品的嵌入表示（物品的节点向量）。为了总结整个会话的状态，模型需要生成一个会话级别的表示。通常会话表示可以通过汇总当前会话中所有物品节点的嵌入向量来得到。常见的汇总方法包括平均池化（average pooling）或基于最后一个物品节点的嵌入。

- **引入 GRU 建模时间依赖**：为了弥补 GNN 在长距离依赖上的不足，SR-GNN 引入了 GRU。GRU 能够通过递归的方式**沿时间顺序依次更新隐藏状态**，并在每一步保留**时间序列中的上下文信息**。这样，GRU 能够显式地捕捉整个会话中的**全局时间顺序**，而不仅仅是局部的转移模式。通过结合 GNN（图结构的局部依赖）和 GRU（全局时间依赖），SR-GNN 能更全面地建模用户的行为序列，提升预测准确性。在这个步骤中，模型将会话中的物品嵌入（通过 GNN 计算得到的节点表示）作为 GRU 的输入，逐个输入给 GRU，GRU 将物品嵌入作为序列进行处理。具体步骤如下：

  - 对于每个会话中的物品嵌入，按照点击顺序依次传入 GRU。
  - GRU 内部的门控机制（即更新门和重置门）会自动决定是否保留或更新隐藏状态，从而捕捉序列中的时间依赖性。
  - 最终，GRU 的隐藏状态将包含整个物品序列的动态信息，即用户在会话中的行为模式。

- **预测下一物品**：通过 GNN，我们得到了物品之间的图结构关系的表示，通过 GRU，我们捕捉了用户点击行为的时间顺序信息。最终，SR-GNN 将这两个部分的输出结合起来，用于预测用户在当前会话中最有可能点击的下一个物品。

  在实际操作中，模型会使用最后一个物品节点的 GNN 表示，结合 GRU 最终的隐藏状态，作为对会话的综合表示。这一会话表示会通过一个全连接层映射到物品空间中，从而计算出用户最可能点击的下一个物品的概率分布。

#### 优点

- SR-GNN 能够捕捉用户行为中的局部物品关系以及物品的转移模式。
- 使用GNN后，能够有效捕捉会话中复杂的物品交互关系。
- 结合GRU，增强了对序列信息的处理能力。它能够根据输入序列的顺序信息，动态地更新隐藏状态，从而捕捉用户在整个会话中的兴趣变化。例如，用户可能在会话初期对某一类物品感兴趣，但在后期对另一类物品更感兴趣，GRU 能够在这个过程中动态调整对这些物品的关注。与传统的 RNN 不同，GRU 通过门控机制减少了梯度消失的问题，使得它能够更好地捕捉较长序列中的时间依赖性。

#### 局限性

- SR-GNN 主要依赖局部图结构，在较长序列或复杂的会话中可能表现不佳，因为长距离的依赖关系需要经过多层GNN的传播才能捕捉。

### **GCSAN（Global Contextual Self-attention Network）**

**GCSAN** 是另一种基于图神经网络的会话推荐方法，它在SR-GNN的基础上进行了改进，引入了自注意力机制来捕捉全局上下文信息。该模型的目标是同时学习会话中的局部和全局物品关系。

#### 核心思路

- **局部上下文捕捉**：类似于SR-GNN，GCSAN首先使用GNN提取会话中的局部上下文信息，通过邻居节点的聚合来更新每个物品的表示。局部上下文的捕捉使得模型能够有效学习到物品之间的短程依赖。
- **自注意力机制引入**：为了克服GNN难以捕捉长距离依赖关系的问题，GCSAN引入了自注意力机制。通过自注意力机制，模型能够在会话中对所有物品进行加权计算，识别出与当前物品最相关的物品，而不仅仅依赖于邻居节点。
- **全局上下文学习**：自注意力机制允许模型对会话中的每个物品赋予不同的重要性权重，因此能够有效建模用户的全局兴趣和短期偏好。这样模型不仅可以学习到短期的局部关系，还能捕捉到会话中的全局信息。

#### 优点

- 结合GNN和自注意力机制，既能捕捉局部上下文，又能学习全局依赖信息。
- 对于较长的会话，GCSAN在捕捉长距离依赖关系上表现更好。
- 使用自注意力机制可以让模型在处理复杂的行为序列时更加灵活。

#### 局限性

- 由于自注意力机制的引入，计算复杂度较高，尤其是在处理大规模数据集时，训练速度和效率可能受到影响。

### **HCGR（Hyperbolic Contrastive Graph Representation）**

**HCGR** 是一种在非欧几里得几何空间（如双曲空间）中进行图表示学习的推荐系统。传统的推荐模型通常在欧几里得空间中进行数据建模，但这种方法在高维空间中容易产生信息扭曲。而HCGR则尝试通过在双曲空间中进行学习，以更有效地处理用户和物品的复杂关系。

#### 核心思路

- **双曲空间表示**：HCGR使用双曲空间进行用户和物品的表示学习。与欧几里得空间相比，双曲空间能够更好地表示层次化和非线性的数据结构，从而能够更紧凑地捕捉用户和物品之间的关系。双曲空间特别适合用于建模复杂的层次关系，例如用户和物品之间的多层次交互。
- **协作图构建**：HCGR通过构建用户-物品的协作图来学习用户偏好。每个用户和物品节点在双曲空间中进行嵌入，并通过图神经网络传播和聚合信息，学习节点之间的关系。
- **减少信息扭曲**：在欧几里得空间中，节点表示的距离可能无法准确反映它们的真实关系。而在双曲空间中，节点之间的距离可以更加紧凑地表示用户和物品之间的相似性或差异性，从而提高推荐精度。

#### 优点

- 使用双曲空间表示能够减少在高维数据中的信息扭曲问题，尤其适用于复杂的、层次化的数据结构。
- 能够在非欧几里得几何空间中有效建模用户的偏好和物品之间的复杂关系。
- HCGR模型在稀疏数据和高维数据中具有很好的表现，因为双曲空间可以更好地处理这些场景。

#### 局限性

- 双曲空间的引入虽然可以减少信息扭曲，但也增加了模型的复杂性和理解难度。在实际应用中，如何高效训练和优化此类模型仍是一个挑战。
- 模型的解释性较低，因为用户和物品的表示在双曲空间中并不容易直观理解。

## 基于图的推荐算法的现有问题

尽管基于图的推荐算法在处理复杂的用户-物品交互关系上表现出色，但也面临一些挑战和局限性：

### 长距离依赖问题

GNN在传播信息时，通常只能够捕捉到邻近节点的信息（局部上下文），对于长距离的依赖关系（例如在长会话中前后物品之间的关联），GNN的效果有限。尤其在较长的会话推荐任务中，用户可能在前后两个时间点之间有重要的偏好转移，而传统的GNN模型难以有效地捕捉到这些全局性的行为模式。

### 稀疏性问题

传统的基于图的推荐系统主要依赖用户的交互数据，而交互数据往往具有高度稀疏性。在许多实际场景中，用户与物品的交互频率较低，尤其是冷启动场景下，新用户或新物品的数据更为稀疏。由于图神经网络（GNN）主要依赖邻居节点的聚合来学习节点表示，数据稀疏会导致模型难以有效地捕捉到用户偏好和物品特性。

### 对上下文理解有限

尽管基于图的推荐方法能够很好地捕捉用户与物品之间的结构化交互关系，但它们对与用户或物品相关的文本信息（例如物品描述、评论等）理解有限。这些模型主要依赖于交互数据，而忽略了与用户行为或物品相关的丰富文本信息。

大语言模型（LLMs）在自然语言理解和生成方面展现了强大的能力，因此研究人员提出了将LLM与GNN结合的可能性。然而，将基于图的会话推荐任务直接转化为自然语言任务，存在着结构化数据与自然语言不匹配的问题。论文的核心挑战是**如何将基于图的SBR任务表示为自然语言任务**，并且如何将LLM与图数据的图结构相结合。

## LLMGR 框架的提出

为了应对上述挑战，本文提出了一种结合大语言模型（LLM）和图神经网络（GNN）的会话推荐框架——LLMGR。该框架通过设计多任务提示（prompts），将文本信息和图结构数据结合起来，并采用混合编码层来增强推荐效果。具体而言，LLMGR的主要贡献包括：

### 多任务提示设计

LLMGR通过设计一系列的提示模板，使得大语言模型能够理解会话图的结构，并捕捉用户行为中的潜在偏好。这些提示分为两个主要任务：

1. **主要任务**：建模用户的行为模式，通过提示引导LLM理解用户在会话中的偏好，并预测下一个用户可能点击的物品。该任务主要通过基于节点和会话图的提示来实现。例如：

   ```
   提示：根据会话图和对应的节点列表，推荐下一个最合适的物品。
   响应：...
   ```

   ![](https://pic.imgdb.cn/item/66dbe5dfd9c307b7e9935c29.png)

2. **辅助任务**：对图中的节点与其相关的文本信息进行对齐，通过提示帮助LLM理解物品节点与其文本描述之间的关系。例如：

   ```
   提示：请告诉我哪个节点与以下文本信息相关。
   响应：...
   ```

### 混合编码层

为了使LLM能够有效处理图结构数据，LLMGR设计了混合编码层。该层将会话中的节点ID和图ID编码为与文本信息相同维度的向量，从而使得LLM能够同时处理文本和图结构信息。在混合编码层中，首先通过GNN提取节点的嵌入表示（ID embeddings），然后通过线性变换将这些嵌入向量与文本嵌入结合，最终生成可以被LLM处理的输入向量。

### 两阶段提示调优策略

为了提高模型的性能，LLMGR采用了两阶段的提示调优策略：

1. **辅助提示调优阶段**：在该阶段，模型主要通过节点与文本信息的对齐提示，学习节点ID与其相关文本之间的关系，并更新混合编码层和LLM的参数。
2. **主要提示调优阶段**：在该阶段，模型专注于捕捉会话中的用户行为模式，并在此基础上预测用户的偏好。此时模型的图神经网络部分参数被解冻，从而实现对行为模式的精确建模。



# 具体细节

![](https://pic.imgdb.cn/item/66dbe65ad9c307b7e993cb5d.png)

这张图展示了 **LLMGR 框架**（即将大语言模型和图神经网络结合的会话推荐系统）的架构，分为两个部分：左侧为 **辅助调优阶段**，右侧为 **主要调优阶段**。通过这两个阶段，模型能够结合图结构数据和自然语言信息，进行更精准的推荐。

**1. 左侧：辅助调优阶段**

首先，在左侧的辅助调优阶段，模型主要通过大语言模型（LLM）处理文本信息。这些文本信息通过嵌入查找层生成嵌入表示，并进入混合编码层。混合编码层的作用是将 LLM 生成的文本嵌入与图神经网络（GNN）生成的节点嵌入结合在一起。因为节点和文本嵌入的维度不同，混合编码层会通过线性变换统一维度，使它们可以一起处理。

这一阶段的核心任务是 **节点-上下文对齐**。为了让 LLM 理解图中的节点与文本信息的关联，模型通过提示引导 LLM，将文本信息（如物品的描述）与图结构中的节点进行对齐。例如，提示模型“请告诉我哪个节点与物品描述‘Seagull Pro-G Guitar Stand’相关”。这样，模型可以将文本和节点信息相结合，提升对节点含义的理解。

在这个阶段，GNN 部分的参数是被冻结的，LoRA 部分的参数会根据节点和文本对齐任务进行更新。

**2. 右侧：主要调优阶段**

接下来是右侧的主要调优阶段，这个阶段模型的核心任务是预测用户的行为模式。与辅助调优阶段不同，这一阶段 GNN 的参数会被解冻，意味着 GNN 可以学习图结构中的信息。

在这个阶段，模型通过提示引导 LLM 预测用户接下来可能会点击的物品。例如，提示模型“请根据会话图及其对应的节点列表，推荐下一个最合适的物品”。此时，GNN 学习到的节点嵌入会被传入混合编码层，与 LLM 的文本嵌入结合，从而帮助 LLM 更好地捕捉用户行为模式和物品之间的关系。

通过这种设计，模型能够同时利用 GNN 处理图结构的优势和 LLM 理解自然语言的能力，最终生成更为精准的推荐。

## 图神经网络（GNN）中的信息传播与节点更新

#### 会话图构建
会话推荐任务的基础是将用户的点击行为序列转化为图结构。假设给定一个用户点击序列 $S = [v_1, v_2, \dots, v_n]$，我们将每个物品 $v \in V$ 视为图中的节点，物品之间的点击顺序则构成了边 $e \in E$。

- **图的表示**：构建的会话图表示为 $G = (V, E)$，其中 $V$ 表示节点集合，$E$ 表示有向边的集合。例如，对于序列 $[v_1, v_2, v_3, v_2, v_4]$，构建的会话图如下：
  - 节点 $V = \{v_1, v_2, v_3, v_4\}$
  - 边 $E = \{(v_1 \rightarrow v_2), (v_2 \rightarrow v_3), (v_3 \rightarrow v_2), (v_2 \rightarrow v_4)\}$

#### 信息传播与聚合（GNN）
图神经网络的核心步骤是通过信息传播机制更新每个节点的表示。我们通过对节点的邻居节点进行聚合来更新该节点的嵌入。假设节点 $v$ 的邻居节点集合为 $N(v)$，其第 $l$ 层的嵌入表示为 $\mathbf{x}_v^{(l)}$，则信息传播和更新步骤如下：

- **信息聚合（Aggregator）**：聚合节点 $v$ 的邻居节点的信息，生成中间状态 $\mathbf{t}_v^{(l+1)}$：
  $$
  \mathbf{t}_v^{(l+1)} = f_{\text{aggregator}}\left( \{\mathbf{x}_u^{(l)} | u \in N(v) \} \right) 
  $$
  
- **节点状态更新（Updater）**：使用聚合后的邻居信息 $\mathbf{t}_v^{(l+1)}$ 更新节点 $v$ 的状态：
  
  $$
  \mathbf{x}_v^{(l+1)} = f_{\text{updater}} \left( \mathbf{x}_v^{(l)}, \mathbf{t}_v^{(l+1)} \right) 
  $$

经过 $l$ 层的传播，最终节点 $v$ 的嵌入表示汇集了 $l$-跳邻居节点的信息。

#### 图级表示生成（Graph Readout）
为了得到整个图（会话）的表示，我们需要将所有节点的嵌入汇总为图级别的表示。这一步称为图读出（Graph Readout）：

$$
\mathbf{Z} = f_{\text{readout}} \left( \{\mathbf{x}_v^{(l+1)} | v \in V \} \right) 
$$

常见的汇总操作包括平均池化（mean pooling）或最大池化（max pooling），即将所有节点的嵌入按照某种方式进行汇总。

## 大语言模型（LLM）的编码层与输出层

#### 混合编码层
为了使LLM能够处理会话中的图结构数据，LLMGR设计了混合编码层。该层结合图中的节点ID、会话ID以及文本信息，将这些元素编码为可被LLM处理的输入向量。

- **节点ID的嵌入转换**：由于节点ID的嵌入维度与文本嵌入维度不同，我们需要对节点嵌入进行线性变换：
  $$
  \mathbf{x}_v = f_{\text{in}}(\mathbf{x}_v) = \mathbf{W}_\text{in} \cdot \mathbf{x}_v
  $$
  
  其中，$\mathbf{W}_\text{in}$ 是线性变换的权重矩阵，负责将节点嵌入从原始维度 $d_1$ 转换到与文本嵌入相同的维度 $d_2$。
  
- **文本嵌入**：文本信息通过LLM的分词器和词嵌入层转化为嵌入表示。假设文本 $t$ 的嵌入为 $\mathbf{t}$，则该嵌入与节点ID嵌入一起输入LLM。

- **最终输入向量**：混合编码层将文本嵌入和节点嵌入进行连接，生成LLM的输入：
$$
\mathbf{E} = [\mathbf{t}, \mathbf{x}_v]
$$

#### LLM输出层
经过LLM的处理后，输出为LLM层的结果，假设为 $\mathbf{O}$。为了生成推荐结果，我们采用多层感知器（MLP）来计算每个候选物品的点击概率：

$$
\hat{\mathbf{y}} = f_{\text{out}}(\mathbf{O}) 
$$

其中，$f_{\text{out}}$ 是输出层的线性变换，生成每个物品的点击概率分布 $ \hat{\mathbf{y}} $。

## 损失函数
为了训练LLMGR模型，我们使用交叉熵损失函数来衡量预测的物品分布与真实分布之间的差异。损失函数定义为：

$$
L = -\sum_{i=1}^{m} y_i \log(\hat{y}_i) 
$$

其中，$y_i$ 是真实的点击标签，$\hat{y}_i$ 是模型预测的概率。

## 两阶段调优策略

LLMGR采用两阶段的调优策略，以提升模型的性能：

1. **辅助提示调优阶段**：该阶段冻结图神经网络部分的参数，专注于调整混合编码层和LLM的参数，通过对齐节点与文本信息的提示任务，学习节点ID与文本信息之间的关联。

2. **主要提示调优阶段**：该阶段解冻图神经网络的参数，通过行为模式建模提示任务，捕捉会话中的用户偏好，并最终预测下一步点击的物品。

在这两个调优阶段，损失函数保持不变，都是通过交叉熵损失进行优化。

# 实验及结果

## 实验设计的总体思路

实验的核心是通过在不同的数据集上验证 LLMGR 模型的有效性，并且和其他现有的最先进方法（SOTA）进行对比。为了更好地验证 LLMGR 的性能，实验针对五个核心研究问题（RQ）展开：

- **RQ1**: LLMGR 在会话推荐（SBR）场景中的表现如何？它能否超过现有的最先进模型？
- **RQ2**: LLMGR 的有效性和在不同模型间的移植性如何？
- **RQ3**: LLMGR 的各个组件（如提示、图神经网络等）如何分别贡献于整体模型的性能？
- **RQ4**: LLMGR 如何处理数据稀疏性问题，尤其是在冷启动场景中，它的表现如何？
- **RQ5**: LLMGR 能否提供合理的解释来预测用户偏好，从而提高推荐效果？

为了回答这些问题，实验设置了不同的数据集、比较方法以及评价指标，分别从多个角度验证 LLMGR 的性能。

## 数据集的选择

实验中使用了三个真实世界的公开数据集，这些数据集都来源于亚马逊平台，具体如下：

- **Music**（音乐数据集）：该数据集包含了用户与音乐相关商品的交互数据，例如购买、点击等。
- **Beauty**（美妆数据集）：该数据集记录了用户在美妆商品类别中的互动行为。
- **Pantry**（家庭必需品数据集）：此数据集包含了用户与家庭日用品相关的购买行为和浏览记录。

**为什么选择这些数据集？**

- 这些数据集能够涵盖不同种类的用户行为模式和物品类别，便于检验 LLMGR 模型的适应性和泛化能力。
- 数据集中的稀疏性较高，尤其是在冷启动场景中，适合验证 LLMGR 如何应对数据稀疏性问题。

**数据预处理**

- 为确保数据质量，按照惯例，剔除了那些与物品交互少于 5 次的用户和物品，这样可以保证模型在训练时有足够的数据。
- 数据划分采用了“留一法”（leave-one-out）：对每个用户的交互序列，最后一个物品用于测试，倒数第二个用于验证，剩下的物品用于训练。

## 比较方法

为了验证 LLMGR 的有效性，实验中选择了多个最先进的基准方法进行比较，这些方法在会话推荐任务中表现良好，且各自的原理不同，便于全面对比 LLMGR 的性能。

**基准模型列表**：

- **FPMC**（Factorized Personalized Markov Chain）：经典的基于马尔可夫链的推荐方法，通过考虑用户的最近一次交互，预测下一次可能的点击。它结合了矩阵分解技术，用来学习用户的长期偏好和短期兴趣。
- **CASER**（Convolutional Sequence Embedding Recommendation）：一种基于卷积神经网络（CNN）的推荐方法，使用横向和纵向卷积操作来捕捉用户行为序列中的高阶交互关系。
- **GRU4Rec**（Gated Recurrent Unit for Recommender Systems）：基于递归神经网络（RNN）的会话推荐方法，堆叠了多个 GRU 层，通过序列建模学习用户偏好。
- **NARM**（Neural Attentive Session-based Recommendation）：一种混合了注意力机制和 RNN 的模型，能够有效捕捉会话中的短期行为模式和长期兴趣。
- **STAMP**（Short-Term Attention Priority Model）：基于注意力机制的模型，通过捕捉用户的短期兴趣（例如最近点击的商品），从历史点击中提取用户当前的兴趣。
- **SRGNN**（Session-based Recurrent Graph Neural Network）：基于图神经网络（GNN）的模型，将会话中的点击行为转换为图结构，使用 GNN 学习物品之间的转移模式。
- **GCSAN**（Global Contextual Self-Attention Network）：结合 GNN 和自注意力机制，能够提取会话中的局部上下文信息和全局语义信息。
- **NISER**（Normalized Item and Session Graph Representation）：一种基于 GNN 的方法，通过标准化物品和会话图的表示，缓解了热门物品偏差问题。
- **HCGR**（Hyperbolic Collaborative Graph Representation）：在非欧几里得几何空间中的 GNN 方法，利用双曲空间来减少高维空间中的数据扭曲，特别适合处理幂律分布的推荐场景。

## 评价指标

在 LLMGR 框架中，模型的目标是预测用户在会话中的下一个点击行为。最终的预测结果是每个候选物品的点击概率分布。为了评估模型的推荐质量，我们使用以下几种常见的评价指标：

### **HitRate@K**
**HitRate@K** 是一种常用的指标，用来评估推荐系统在前 K 个推荐结果中是否包含了用户真正感兴趣的物品。

- **计算方法**：如果模型在前 K 个推荐物品中包含了用户实际点击的物品，我们称之为“命中”。
- **公式**：
  
  $$
  \text{HitRate@K} = \frac{\sum_{s \in S} I(y^s \cap \hat{y}^s \neq \emptyset)}{|S|}
  $$

  其中：
  - $S$ 是测试集中的会话集合；
  - $y^s$ 是会话 $s$ 中的真实物品；
  - $\hat{y}^s$ 是模型预测的前 K 个推荐物品；
  - $I(\cdot)$ 是指示函数，若条件为真则取值为 1，反之为 0。


HitRate@K 衡量的是在前 K 个推荐结果中，是否至少命中了一个用户实际感兴趣的物品。其值在 0 到 1 之间，值越大表示推荐系统的命中率越高。

### **NDCG@K**
**NDCG（Normalized Discounted Cumulative Gain）@K** 是一种基于排序的指标，旨在衡量推荐列表中正确物品的排序质量。它不仅关注是否推荐了正确的物品，还关注这些物品在推荐列表中的位置。

- **计算方法**：NDCG 计算的是累积增益（Cumulative Gain, CG），并根据物品的排序位置进行折扣调整。

- **公式**：

  $$
  \text{NDCG@K} = \frac{1}{Z_K} \sum_{i=1}^{K} \frac{2^{I(\hat{y}_i^s \in y^s)} - 1}{\log_2(i + 1)}
  $$

  其中：
  - $K$ 是推荐列表的长度；
  - $i$ 是推荐物品在列表中的位置；
  - $I(\hat{y}_i^s \in y^s)$ 是指示函数，表示位置 $i$ 的物品是否为真实的目标物品；
  - $Z_K$ 是归一化因子，用于将 NDCG 的值限制在 0 到 1 之间。

NDCG 更加注重推荐物品的顺序，如果模型推荐的正确物品排在前面，其得分会更高。NDCG@K 越高，表明模型对推荐结果排序的质量越好。

### **MRR@K**
**MRR（Mean Reciprocal Rank）@K** 是一种衡量推荐系统准确性和排序的指标。它关注的是第一个正确推荐的物品在列表中的排名。

- **计算方法**：MRR 是第一个正确推荐的物品的倒数排名的平均值。

- **公式**：
  $$
  \text{MRR@K} = \frac{1}{|S|} \sum_{s=1}^{|S|} \frac{1}{\text{rank}_s}
  $$

  其中：
  - $|S|$ 是所有会话的数量；
  - $\text{rank}_s$ 是会话 $s$ 中第一个正确物品在推荐列表中的位置（如果没有推荐正确物品，则此项为 0）。

MRR 衡量的是第一个正确物品的平均排名位置，值越高表示模型推荐正确物品的位置越靠前。

## 参数配置

为了确保实验的公平性，所有基线模型和 LLMGR 模型使用了相同的超参数设置：

- **mini-batch 大小**：1024；
- **dropout 率**：0.3，防止过拟合；
- **学习率**：从 ${1 \times 10^{-4}, 1 \times 10^{-3}, 1 \times 10^{-2}, 1 \times 10^{-1}}$ 中调优；
- **嵌入维度**：64；
- **最大序列长度**：50。

模型的训练使用了 Adam 优化器，针对 GNN 模型（例如 SRGNN、GCSAN 等），调优了 GNN 的聚合层数量（从 1 层到 5 层）。

**LLMGR 实现细节**：

- LLMGR 基于 LLaMA2-7B2 模型，并使用 HuggingFace 库进行开发。
- 模型加速使用 DeepSpeed 技术，在 2 个 Nvidia Tesla A100 GPU 上进行训练。
- ID 嵌入直接从预训练的 GCSAN 模型中提取，且在实验中没有进行修改。
- 使用 AdamW 优化器优化 LLMGR 模型，学习率在 ${1 \times 10^{-3}, 1 \times 10^{-4}, 1 \times 10^{-5}}$ 中调优，batch size 为 16。
- 使用 cosine scheduler 调整学习率，并设置了权重衰减值为 1e-2。
- 在辅助任务调优阶段，模型训练 1 个 epoch；在主要任务调优阶段，模型在每个数据集上训练 3 个 epoch。

## 实验结果分析

### RQ1: LLMGR 在会话推荐任务中的性能（与 SOTA 方法对比）

实验结果表明，LLMGR 相比于现有的基准模型（如 GRU4Rec、STAMP、SRGNN 等），在各项指标（HitRate@K、NDCG@K、MRR@K）上均表现更优，特别是在高 K 值时，LLMGR 的排名能力更强。相对于最具竞争力的基准模型，LLMGR 在以下指标上有明显的提升：

- **HR@20**：提高了约 **8.68%**；
- **NDCG@20**：提高了 **10.71%**；
- **MRR@20**：提高了 **11.75%**。

这表明 LLMGR 模型不仅能够准确预测用户下一个可能点击的物品，还能较好地对推荐列表进行排序。

### RQ2: LLMGR 的有效性和移植性

为了验证 LLMGR 的移植性，实验将其应用于其他基准模型上，观察其性能的提升情况。实验表明：

- LLMGR 在所有测试模型（如 GCSAN、GRU4Rec、STAMP）上的表现均有所提升。
- 对于简单的基准模型（如 GRU4Rec、STAMP），LLMGR 提供了显著的性能提升，表明即使是较简单的模型，结合 LLMGR 也可以超越许多 SOTA 的会话推荐方法。

在不同的模型上，LLMGR 平均提升了约 **8.58%**（Music 数据集）和 **17.09%**（Beauty 数据集）。这表明 LLMGR 具有良好的移植性，并且可以应用于多种模型，增强其性能。

### RQ3: LLMGR 组件的贡献分析（消融实验）

为了分析 LLMGR 中各个组件的贡献，消融实验去除了辅助任务（如节点-文本对齐任务），只保留主要任务进行训练。结果显示：

- 移除辅助任务后，模型性能在多个指标上显著下降，特别是 **NDCG** 和 **MRR** 指标，这表明节点-文本对齐任务在提升模型排序能力方面起到了关键作用。
- 在 Music 数据集中，移除辅助任务后 HitRate@20 下降了 **2.04%**，而在 Beauty 数据集中，下降幅度更大，NDCG@20 降低了 **4.16%**。

### RQ4: 冷启动分析

为了验证 LLMGR 在冷启动场景中的表现，实验将数据集划分为“暖启动”和“冷启动”场景：

- **暖启动**：用户与物品的交互数据丰富，系统可以从中学习到充足的偏好信息；
- **冷启动**：用户与物品的交互数据非常少，传统推荐系统难以从中学习到用户偏好。

实验表明：

- 在冷启动场景中，LLMGR 的表现显著优于传统的基准模型，能够有效处理数据稀疏性问题。
- 与暖启动场景相比，冷启动场景下 LLMGR 的性能提升更为显著，这主要得益于 LLM 模型在处理少量数据时所展现的语言理解和知识迁移能力。

# 代码示例







