---
title: 矩阵低秩近似 —— 伪逆
tags: 
  - Matrix
  - ML Basics
categories: Algorithm
date: 2023-09-23 18:30:00
mathjax: true

---

伪逆（Pseudo-Inverse），也称为广义逆（Generalized Inverse），是线性代数中的一个重要工具，尤其是在处理不可逆或非方阵时。与标准的逆矩阵不同，伪逆可以扩展矩阵求逆的概念，适用于更广泛的矩阵类型。这在机器学习、优化以及控制理论等多个领域中都有广泛应用。本文将详细探讨伪逆的定义、推导过程及其应用，并提供数学证明。

<!-- more -->

# 什么是伪逆？

对于给定的矩阵 $A$，**伪逆**（记作 $A^+$）提供了一种解决线性方程组的方式，即使这些方程组没有唯一解或精确解。特别地，当 $A$ 不可逆或不是方阵时，我们可以通过定义 $A^+$ 来最小化误差的平方和。

伪逆通过以下优化问题定义：

$$
\min_B \| AB - M \|_F^2
$$

其中：
- $A \in \mathbb{R}^{m \times n}$ 是我们希望计算伪逆的矩阵。
- $M \in \mathbb{R}^{m \times n}$ 是目标矩阵。
- $\| \cdot \|_F$ 表示 **Frobenius 范数**，定义为：

$$
\| X \|_F = \sqrt{\sum_{i,j} X_{ij}^2}
$$

简单来说，伪逆通过最小化矩阵乘积 $AB$ 与目标矩阵 $M$ 的误差来近似逆矩阵。

# Frobenius 范数

在进一步讨论伪逆之前，我们首先要了解一下Frobenius范数，这是矩阵优化问题中广泛使用的工具。

对于矩阵 $X \in \mathbb{R}^{m \times n}$，**Frobenius 范数**定义为：

$$
\| X \|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} X_{ij}^2}
$$

它也可以通过矩阵的迹来表示：

$$
\| X \|_F = \sqrt{\text{Tr}(X^T X)}
$$

其中 $\text{Tr}(\cdot)$ 表示迹运算，即矩阵对角线元素之和。

> 为什么将矩阵 $X$ 的每个元素平方然后求和等价于求 $X^T X$ 的迹？
>
> 事实上，当我们计算 $X^T X$ 的对角线元素时，这些元素是 $X$ 中每一行向量与自己相乘得到的平方和（即内积），所以最终结果相当于对每个元素的平方求和。通过这个公式我们可以看到 Frobenius 范数实际上是矩阵乘法结果中对角线元素之和的平方根，这与直接将矩阵元素平方求和再开平方的结果是一样的。

Frobenius 范数可以看作是欧几里得范数在矩阵上的推广，将矩阵的元素视为向量的元素来计算。

# 优化视角

从优化的角度来看，伪逆可以通过以下问题来定义：

$$
\min_B \| AB - M \|_F^2
$$

这个问题的本质是寻找一个 $B$，使得 $AB$ 与 $M$ 之间的 Frobenius 范数最小化。当 $M$ 是单位矩阵时，我们称 $B$ 为 **右伪逆**：

$$
A^+ = \min_B \| AB - I \|_F^2
$$

同样地，**左伪逆**也可以通过类似的方式定义。

## 求解伪逆

为了解决上述优化问题，我们对目标函数求导：

$$
\frac{\partial}{\partial B} \| AB - M \|_F^2 = 2A^T (AB - M)
$$

令其等于零，得到：

$$
A^T (AB - M) = 0
$$

解得：

$$
B = (A^T A)^{-1} A^T M
$$

当 $M$ 为单位矩阵时，伪逆的解为：

$$
A^+ = (A^T A)^{-1} A^T
$$

## 一般形式

对于不满秩的矩阵 $(A^T A)$ 不可逆的情况，可以通过正则化来稳定计算伪逆：

$$
A^+ = \lim_{\lambda \to 0} (A^T A + \lambda I)^{-1} A^T
$$

这被称为**Tikhonov 正则化**或**岭回归**。

为了证明 $A^T A + \lambda I$ 是可逆的，我们可以从以下几个步骤来展开：

### $A^T A$ 的不可逆性

矩阵 $A^T A$ 是 $A$ 的**协方差矩阵**，如果 $A$ 的列向量线性相关，$A^T A$ 就会是**奇异矩阵**，不可逆。这是因为在线性相关的情况下，$A^T A$ 的某些特征值为 0，导致行列式为 0，进而不可逆。

### 引入正则化项 $\lambda I$

为了保证矩阵 $A^T A$ 可逆，我们引入正则化项 $\lambda I$，其中 $\lambda > 0$，$I$ 是单位矩阵。加入正则化项的目的是增加对角线上的元素，使得整个矩阵的特征值都大于零，从而使其可逆。

我们需要证明 $A^T A + \lambda I$ 是可逆的。

### 可逆性证明（三种方法）

#### 正定性

首先，我们知道矩阵 $A^T A + \lambda I$ 是对称矩阵，因为 $A^T A$ 和单位矩阵 $I$ 都是对称的。对称矩阵的可逆性可以通过其是否正定来判断。

考虑任意向量 $x \in \mathbb{R}^n$，我们计算 $x^T (A^T A + \lambda I) x$：

$$
x^T (A^T A + \lambda I) x = x^T A^T A x + \lambda x^T I x = \|Ax\|_2^2 + \lambda \|x\|_2^2
$$

其中 $\|Ax\|_2^2 \geq 0$ 且 $\|x\|_2^2 \geq 0$，由于 $\lambda > 0$，我们可以得到：

$$
x^T (A^T A + \lambda I) x = \|Ax\|_2^2 + \lambda \|x\|_2^2 > 0 \quad \forall x \neq 0
$$

这表明 $A^T A + \lambda I$ 是正定矩阵，因此它是可逆的。

#### 特征值的变化

假设 $A^T A$ 的特征值为 $\sigma_1^2, \sigma_2^2, \dots, \sigma_r^2$，其中一些 $\sigma_i^2 = 0$（即 $A$ 不满秩）。对于 $A^T A + \lambda I$，其特征值变为：

$$
\text{特征值}(A^T A + \lambda I) = \sigma_1^2 + \lambda, \sigma_2^2 + \lambda, \dots, \sigma_r^2 + \lambda
$$

由于 $\lambda > 0$，即使某些 $\sigma_i^2 = 0$，加入 $\lambda$ 后特征值也变成了 $\lambda$，因此所有特征值都严格大于零。特征值严格大于零意味着矩阵是正定的，因此可逆。

#### 行列式不为零

可逆矩阵的充要条件是行列式不为零。行列式可以通过特征值的乘积来计算：

$$
\text{det}(A^T A + \lambda I) = \prod_{i=1}^r (\sigma_i^2 + \lambda)
$$

由于 $\lambda > 0$，每个 $\sigma_i^2 + \lambda > 0$，因此行列式不为零，这也证明了 $A^T A + \lambda I$ 是可逆的。

## 伪逆的性质

伪逆具有以下重要性质：
1. **对称性**：$(A^+)^+ = A$，即伪逆的伪逆等于原矩阵。
2. **一致性**：$A A^+ A = A$，这保证了伪逆与标准逆矩阵具有类似的行为。
3. **最小误差**：伪逆最小化 $AB$ 与 $M$ 之间的 Frobenius 范数。

# 奇异值分解（SVD）与伪逆

伪逆的一个强大计算方法是通过**奇异值分解（SVD）**。对于任意矩阵 $A \in \mathbb{R}^{m \times n}$，其 SVD 分解为：

$$
A = U \Sigma V^T
$$

其中：
- $U \in \mathbb{R}^{m \times m}$ 和 $V \in \mathbb{R}^{n \times n}$ 是正交矩阵。
- $\Sigma \in \mathbb{R}^{m \times n}$ 是包含 $A$ 的奇异值的对角矩阵。

伪逆可以通过 SVD 计算为：

$$
A^+ = V \Sigma^+ U^T
$$

其中 $\Sigma^+$ 是 $\Sigma$ 的伪逆，通过将非零奇异值取倒数，零保持不变。

## 示例

考虑矩阵：

$$
A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix}
$$

通过 SVD 分解 $A$ 为：

$$
A = U \Sigma V^T
$$

然后通过取 $\Sigma$ 的伪逆，计算出 $A^+$。

```python
import numpy as np

# 定义矩阵 A
A = np.array([[1, 2],
              [3, 4],
              [5, 6]])

# 进行 SVD 分解
U, Sigma, VT = np.linalg.svd(A)

# 计算 Σ 的伪逆
Sigma_inv = np.zeros((A.shape[1], A.shape[0]))  # 创建一个与 A 的维度相匹配的矩阵
Sigma_inv[:len(Sigma), :len(Sigma)] = np.diag(1 / Sigma)  # 对非零的奇异值取倒数

# 计算伪逆 A^+
A_pseudo_inverse = VT.T @ Sigma_inv @ U.T

# 输出伪逆
A_pseudo_inverse

# array([[-1.33333333, -0.33333333,  0.66666667],
#       [ 1.08333333,  0.33333333, -0.41666667]])
```

# 伪逆的应用

## 线性系统求解

伪逆在求解无唯一解的线性方程组时尤为有效。对于线性方程组 $Ax = b$，如果 $A$ 不可逆，伪逆提供了最小二乘意义下的最佳近似解：

$$
x = A^+ b
$$

## 低秩近似

在机器学习中，伪逆常用于**低秩矩阵近似**，例如在**主成分分析（PCA）**中，通过伪逆找到最优投影，将高维数据压缩到低维。

## 控制理论

在控制系统中，伪逆用于设计控制器，即使系统可能不可完全控制或观测。伪逆通过最小二乘方法计算控制规律，从而减少误差。

# 数值稳定性与计算考虑

## 数值不稳定性的来源

在计算伪逆时，最常用的方法是通过**奇异值分解 (SVD)**。SVD 将矩阵分解为三个部分：正交矩阵 $U$，奇异值矩阵 $\Sigma$，以及正交矩阵 $V^T$。伪逆的计算依赖于对 $\Sigma$ 取倒数，这本质上是对矩阵的奇异值进行反转操作。

问题在于，当某些奇异值 $\sigma_i$ 非常接近 0 时，取倒数的结果 $\frac{1}{\sigma_i}$ 会变得非常大。这会导致数值计算中的不稳定性，因为计算机无法准确表示非常小或非常大的数。这种不稳定性在数值上表现为：
- 计算误差被放大。
- 最终结果不准确甚至可能溢出。

举个例子，假设有一个矩阵 $A$，它的奇异值为 $\Sigma = \text{diag}(1, 0.00001, 0)$。当我们尝试取伪逆时，$\Sigma^+$ 的结果是：

$$
\Sigma^+ = \text{diag}(1, 100000, \infty)
$$

这意味着与小奇异值对应的反转操作会导致非常大的数值。这些数值将导致在后续的矩阵乘法中产生巨大的计算误差。

## Tikhonov 正则化：数值稳定性的方法

为了解决奇异值非常小时的数值不稳定问题，我们可以引入**Tikhonov 正则化**（也称为岭回归）。其核心思想是在计算过程中人为地增加一个小的正则化项 $\lambda$，以避免奇异值过小的问题。

正则化方法通过修改原始问题来改善数值稳定性。通常的做法是修改伪逆计算公式，将伪逆表示为：

$$
A^+ = \lim_{\lambda \to 0} (A^T A + \lambda I)^{-1} A^T
$$

其中 $\lambda > 0$ 是一个非常小的正则化参数，$I$ 是单位矩阵。引入 $\lambda I$ 的主要目的是避免直接使用非常小的奇异值，从而减小数值不稳定的影响。

### 数值稳定性的来源

通过添加 $\lambda I$，我们实际是增加了矩阵的对角线元素。这会使得矩阵的最小特征值变得不再为 0 或非常小，而是增加了一个 $\lambda$。即使原矩阵 $A^T A$ 是奇异矩阵，加上 $\lambda I$ 之后它也会变得可逆，从而可以安全地进行伪逆的计算。

### 正则化的影响

在计算伪逆时，正则化项 $\lambda$ 的大小对结果有直接影响：
- 如果 $\lambda$ 过大，正则化效果会过强，导致伪逆的解偏离实际的解。
- 如果 $\lambda$ 过小，则无法有效改善数值不稳定性。

因此，选择适当的 $\lambda$ 是至关重要的。通常，通过交叉验证或经验选择合适的正则化参数。

### 示例：使用正则化计算伪逆

假设我们有矩阵 $A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix}$，其奇异值中存在较小的值。我们可以通过添加正则化项来计算伪逆，以避免数值不稳定。

```python
import numpy as np

# 定义矩阵 A
A = np.array([[1, 2],
              [3, 4],
              [5, 6]])

# 正则化参数 lambda
lambda_reg = 1e-5

# 计算 A^T A 和 正则化项
ATA_reg = A.T @ A + lambda_reg * np.eye(A.shape[1])

# 计算伪逆
A_pseudo_inverse_reg = np.linalg.inv(ATA_reg) @ A.T

# 输出伪逆
print(A_pseudo_inverse_reg)

# [[-1.33328236 -0.33331944  0.66664347]
# [ 1.08329309  0.33332236 -0.41664837]]
```

# 结论

伪逆是逆矩阵的推广，适用于更广泛的矩阵类型，包括非方阵或不可逆矩阵。它在优化、机器学习和控制理论中具有重要应用。通过奇异值分解，我们可以高效计算伪逆，并将其应用于实际问题。

# 参考文献

1. [Moore–Penrose 伪逆](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)
2. [奇异值分解](https://en.wikipedia.org/wiki/Singular_value_decomposition)
3. [Tikhonov 正则化](https://en.wikipedia.org/wiki/Tikhonov_regularization)

