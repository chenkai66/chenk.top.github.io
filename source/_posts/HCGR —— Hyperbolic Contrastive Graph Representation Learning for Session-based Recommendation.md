---
title: HCGR —— Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation
tags:
  - GNN
  - Recommend System
categories: Paper
date: 2024-08-08 17:00:00
mathjax: true
---

本文提出了一种新的超曲面对比图表示学习方法（HCGR），用于解决基于会话的推荐系统中的挑战。会话推荐系统需要从用户的短期行为中推断用户的兴趣，而传统的方法通常依赖于欧氏空间进行图神经网络的表示学习。这种方法难以捕捉用户行为中的层次结构，尤其是在用户的兴趣是多层次或呈现树状结构时。HCGR通过引入洛伦兹超曲面几何空间，将推荐问题嵌入到非欧空间中，以更有效地捕捉用户行为中的层次信息。此外，论文提出了一种自适应超曲面注意力机制，以及对比学习方法来优化模型，提升推荐性能。实验结果表明，该方法在多个公开数据集上的表现优于当前的先进基准方法，显著提高了命中率、NDCG和MRR等指标。

<!-- more -->

# 背景介绍

随着电子商务、音乐流媒体和新闻应用等平台的兴起，推荐系统在这些平台上起到了至关重要的作用。会话推荐系统（Session-based Recommendation, SBR）尤其适用于无法获取用户长期历史数据的情况，在短期的会话内通过分析用户的行为来预测用户下一步最有可能感兴趣的物品。为了提高推荐的准确性，研究者们提出了多种基于顺序和图的模型来捕捉用户行为的时序依赖性和物品之间的关联。

## 基于序列的模型

早期的推荐系统大多基于序列模型，如马尔可夫链（Markov Chains, MC）模型。这类模型仅考虑用户最近的行为来进行下一步的预测，典型的模型包括FPMC和FOSSIL。然而，这类模型无法有效处理长程依赖，且高阶马尔可夫模型尽管能够部分解决这一问题，但计算成本极高，难以扩展到大规模数据。

随后，循环神经网络（RNN）模型被引入到推荐系统中。GRU4REC是最早的一种基于RNN的推荐模型，它利用门控循环单元（GRU）来捕捉用户会话中的长程依赖。然而，RNN模型假设会话中的物品是有固定顺序依赖的，在实际中，这种假设可能引入错误的依赖关系。

### 马尔可夫链模型（Markov Chains, MC）

早期的推荐系统大多采用马尔可夫链（Markov Chains, MC）模型来建模用户行为。马尔可夫链模型假设用户的下一步行为仅依赖于其最近的历史行为，而不考虑更长的历史数据。这种方式较为简单且计算效率较高，适用于用户行为短期依赖性较强的场景。

#### **典型模型：FPMC**

Factorizing Personalized Markov Chains（FPMC）是马尔可夫链和矩阵分解结合的一种方法，旨在结合用户的长期兴趣（通过矩阵分解捕捉）和短期行为（通过马尔可夫链捕捉）来进行下一步的推荐。FPMC通过以下方式解决了推荐系统中的两个主要问题：

- **个性化**：通过矩阵分解方法学习每个用户的个性化兴趣模型。
- **短期行为捕捉**：通过马尔可夫链模型将用户最近的行为作为关键因素进行预测。

其目标函数为：

$$
\hat{y}(u, i) = \mathbf{u}_u^\top \mathbf{v}_i + \mathbf{h}_s^\top \mathbf{v}_i
$$

其中：
- $\mathbf{u}_u$ 是用户 $u$ 的隐向量。
- $\mathbf{v}_i$ 是物品 $i$ 的隐向量。
- $\mathbf{h}_s$ 是当前会话 $s$ 的历史隐向量。

该公式结合了用户的长期偏好（通过$\mathbf{u}_u^\top \mathbf{v}_i$）和马尔可夫链捕捉的短期行为（通过$\mathbf{h}_s^\top \mathbf{v}_i$）。然而，FPMC和其他类似的马尔可夫链模型都有一个核心的局限性，即它们只能捕捉用户最近的行为，而无法有效处理长程依赖。这导致了用户的长期兴趣可能无法被充分利用，从而限制了模型的表现。

#### **典型模型：FOSSIL**

FOSSIL（Factorized Sequential Prediction with Item Similarity Models）是另一种基于马尔可夫链的推荐模型，它在捕捉序列依赖性的同时，结合了基于相似度的推荐技术。FOSSIL通过学习物品之间的相似性矩阵，改善了马尔可夫链模型中仅关注最近行为的缺点，尝试在短期行为和物品相似性之间找到平衡。目标函数为：

$$
\hat{y}(u, i) = \sum_{t=1}^{T} \alpha_t \mathbf{v}_{i_{t}}^\top \mathbf{v}_i + \mathbf{u}_u^\top \mathbf{v}_i
$$

其中：
- $\mathbf{v}_{i_t}$ 是用户 $u$ 在时间步 $t$ 的物品 $i_t$ 的向量。
- $\mathbf{v}_i$ 是待推荐物品 $i$ 的向量。
- $\alpha_t$ 是衰减系数，用于控制历史行为的重要性。

该公式通过历史物品的相似性（$\mathbf{v}_{i_{t}}^\top \mathbf{v}_i$）来进行预测，并结合用户的长期偏好（$\mathbf{u}_u^\top \mathbf{v}_i$）。

然而，尽管FOSSIL引入了物品相似性，但它依然无法处理长程依赖。并且，FOSSIL和FPMC等模型基于低阶马尔可夫链的假设，即用户的下一步行为仅依赖于最近的行为，这种假设在捕捉复杂用户行为时存在局限性。为了解决这一问题，高阶马尔可夫链模型被提出，高阶模型能够考虑多个前序状态的影响，但这会带来计算成本的急剧上升，难以扩展到大规模数据集。

### 循环神经网络（Recurrent Neural Networks, RNN）

为了克服马尔可夫链模型无法捕捉长程依赖的问题，循环神经网络（RNN）被引入到推荐系统中。RNN通过递归结构使得网络能够记住长时间的序列信息，因此非常适合于建模具有长时间依赖关系的用户行为。

#### **典型模型：GRU4REC**

GRU4REC是RNN在推荐系统中的经典应用之一，专门用于会话推荐任务。它利用了门控循环单元（GRU，Gated Recurrent Unit）来解决传统RNN中长时间依赖难以训练的问题。具体来说，GRU通过引入**重置门**（reset gate）和**更新门**（update gate）机制，能够更有效地捕捉并维护长时间的用户行为序列。其核心公式包括以下部分：

1. **重置门**：

$$
r_t = \sigma(W_r \cdot [h_{t-1}, x_t])
$$

2. **更新门**：

$$
z_t = \sigma(W_z \cdot [h_{t-1}, x_t])
$$

3. **候选隐藏状态**：

$$
\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t])
$$

4. **隐藏状态更新**：

$$
h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t
$$

其中：
- $x_t$ 是在时间步 $t$ 的输入（即用户在时间步 $t$ 的物品）。
- $h_t$ 是在时间步 $t$ 的隐藏状态。
- $W_r, W_z, W$ 是需要学习的权重矩阵。

GRU4REC通过以下方式解决了会话推荐中的两个问题：

- **长程依赖捕捉**：与马尔可夫链模型不同，GRU4REC能够处理整个会话的长时间依赖，使得用户的早期行为信息能够在推荐时发挥作用。
- **序列推荐优化**：GRU4REC通过优化排名损失函数（pair-wise ranking loss），提高了推荐结果的排序性能，这在推荐系统中非常关键。

然而，RNN模型（包括GRU4REC）的一个核心假设是会话中的物品是按照固定顺序依赖的，即后续的物品行为总是依赖于前面的物品。然而，在实际应用中，这种假设可能引入错误的依赖关系。例如，用户在一场音乐会应用中可能随机选择不同的歌曲播放，这种行为并不存在严格的顺序依赖关系。如果模型过度依赖顺序，可能会导致对用户兴趣的错误建模。

## 改进的序列模型：引入注意力机制

为了弥补RNN在处理用户随机行为上的不足，研究者引入了注意力机制来增强模型的灵活性。注意力机制允许模型在做出预测时，能够根据上下文自适应地关注序列中最相关的部分，而不是严格依赖整个序列的顺序。

#### **典型模型：NARM 和 SASRec**

1. **NARM（Neural Attentive Session-based Recommendation）**：NARM是一个结合了注意力机制的会话推荐模型，它在捕捉用户长期兴趣和短期兴趣的同时，使用了注意力机制来突出当前会话中最重要的物品。其核心公式如下：

   * **全局表示**（通过RNN计算得到）：

   $$
   \mathbf{h}_g = GRU(x_1, x_2, \dots, x_t)
   $$

   * **局部表示**（通过注意力机制计算）：

   $$
   \alpha_i = \frac{\exp(e_i)}{\sum_{j=1}^{t} \exp(e_j)}, \quad e_i = \mathbf{q}^\top \cdot \tanh(W_1 \mathbf{h}_i + W_2 \mathbf{h}_g)
   $$

   其中，$\mathbf{h}_i$ 是在时间步 $i$ 的隐藏状态。$\mathbf{q}$ 是用于计算注意力权重的查询向量。

   * **最终的会话表示**：

   $$
   \mathbf{c} = \sum_{i=1}^{t} \alpha_i \mathbf{h}_i
   $$

   NARM的贡献在于：

   - **短期与长期兴趣结合**：通过引入两个不同的编码器，NARM同时捕捉了用户的长期兴趣（通过RNN）和短期兴趣（通过注意力机制）。
   - **灵活性**：通过注意力机制，NARM能够根据不同的会话上下文自适应地选择最相关的行为，而不是盲目依赖序列顺序。

2. **SASRec（Self-Attentive Sequential Recommendation）**：SASRec进一步改进了基于注意力的序列推荐方法，它采用了自注意力机制（self-attention），该机制最早在Transformer模型中提出。SASRec允许模型在序列中捕捉任意两个物品之间的依赖关系，而不仅仅是相邻物品。其核心公式为：

   * **自注意力得分计算**：

   $$
   \text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
   $$

   * **前馈网络**：

   $$
   FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2
   $$

   * **位置编码**：

   $$
   PE_{(pos, 2i)} = \sin \left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
   $$

   $$
   PE_{(pos, 2i+1)} = \cos \left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
   $$

   它的优势包括：

   - **长距离依赖的捕捉**：通过自注意力机制，SASRec能够更加灵活地捕捉到序列中的长距离依赖关系，克服了RNN固定顺序的局限性。
   - **并行计算**：与RNN不同，自注意力机制允许模型在训练时进行并行计算，这极大地提升了训练效率。

**局限性：注意力机制的计算成本**

虽然注意力机制增强了模型的灵活性，使其能够处理非顺序行为，但自注意力机制的一个主要问题在于其计算复杂度。随着序列长度的增加，自注意力机制的计算成本呈二次增长，这在处理长序列时可能会带来较高的计算负担。此外，注意力机制在捕捉全局信息时，可能会忽视一些局部结构信息（如层次结构），这在复杂的推荐场景中也可能影响模型的性能。

## 基于图神经网络的模型

为了更好地捕捉用户行为中的复杂关系，图神经网络（GNN）成为了近年来的研究热点。SR-GNN等模型通过将用户的点击序列建模为图结构，利用图神经网络来捕捉会话中物品之间的关联。然而，现有的图神经网络模型通常在欧氏空间中进行嵌入和信息传递，这使得它们难以处理具有层次结构的数据。例如，用户点击行为通常遵循幂律分布，即用户倾向于点击少量热门物品，而少量物品会吸引大量用户。欧氏空间难以有效捕捉这种层次关系，导致嵌入时信息丢失。

## 超曲面几何的引入

### 欧氏几何的局限性

在传统的表示学习中，欧氏空间（Euclidean space）通常用于嵌入数据和计算距离。例如，最常见的嵌入方法如Word2Vec、Node2Vec和GraphSAGE等，均使用欧氏空间来捕捉数据中的特征和相似性。欧氏几何的基本特点是它的平坦性（零曲率），这意味着它擅长表示具有线性或近似线性关系的数据。然而，在很多现实场景中，数据并不是简单的线性关系，而是具有高度复杂的层次结构或树状结构，尤其是当数据呈现幂律分布时（如社交网络中的连接关系或推荐系统中的用户点击行为）。在这些场景下，欧氏空间难以有效地捕捉这种层次结构，主要原因包括：

1. **维度灾难**：要在欧氏空间中准确地表示具有层次结构的数据，往往需要极高的维度，这不仅增加了计算成本，还导致了信息冗余和模型复杂度的增加。
2. **空间表达能力有限**：欧氏空间中距离的增加是线性的，这意味着随着距离的增加，捕捉细微差异的能力减弱，特别是当数据具有指数增长的结构时（如树状结构）。

### 非欧几何的优越性

与欧氏空间相比，非欧几何空间能够更好地表示复杂和非线性的结构，特别是负曲率空间（如超曲面几何）在表示具有层次结构的数据时表现尤为突出。非欧几何的基本原理来自黎曼几何学，它允许对空间进行弯曲，从而使得该空间能够更紧凑地表达数据中的层次结构。

### 超曲面几何的基本概念

超曲面几何（Hyperbolic Geometry）是一种具有负曲率的非欧几何空间，其特点是在负曲率下空间的体积增长速度远快于欧氏空间。具体来说，超曲面空间中的距离随着从中心点辐射而指数增加，这使得其能够在较小的维度内表达出更复杂的层次结构。

超曲面空间的主要特点如下：

1. **负曲率**：负曲率意味着空间是"凹"的。与欧氏几何中距离是线性增长的不同，超曲面几何中的距离是指数增长的。这样的几何特性使得它能够更有效地捕捉数据中的层次关系。
2. **树状结构的自然表示**：在超曲面几何中，物体的数量随着距离从中心点的增加呈指数增长。因此，超曲面空间非常适合表示像树状数据这样具有层次性增长特征的数据。例如，社交网络、分子生物学数据、自然语言中的句法结构以及推荐系统中的用户点击行为，都可以通过树状结构来表达，且超曲面空间可以更自然地捕捉这种层次性。
3. **较低维度下的高表现力**：由于超曲面几何的指数增长特性，负曲率空间能够在相对较低的维度内保留大量的结构信息。这意味着在高维欧氏空间中才能有效捕捉到的关系，在较低维度的超曲面空间中也可以清晰地表示。这对于降低模型复杂度、减少计算开销和提高效率至关重要。

### 已有研究中的应用

在超曲面几何的背景下，近年来的研究表明，它在表示复杂数据（如社交网络、自然语言处理、生物网络等）中具有显著的优势。

1. **社交网络中的应用**：社交网络的数据具有显著的层次性和幂律分布（如较少的中心节点连接了大量的外围节点）。传统的欧氏几何在捕捉这种长尾分布时往往表现不佳。研究表明，超曲面几何能够有效地嵌入社交网络中的节点，并且能够通过较低的维度捕捉用户之间的层次关系。特别是像Poincaré嵌入这样的方法，通过将网络节点嵌入到超曲面空间中，显著提高了节点分类、链接预测等任务的性能。
2. **自然语言处理中的应用**：自然语言中的句法结构常常以树状结构出现。超曲面几何能够以更紧凑的方式表示这种层次性。Poincaré嵌入被用于词汇的层次结构学习，能够有效地表示语言中的概念层次关系。与Word2Vec等欧氏嵌入相比，超曲面嵌入能够在更低的维度中捕捉到词汇间的语义相似度。
3. **生物网络中的应用**：生物学中，许多系统（如蛋白质网络、基因调控网络）表现出明显的层次结构。超曲面几何已经被用于蛋白质结构的嵌入，通过负曲率空间的表示，能够更好地捕捉蛋白质分子之间的关系及其功能。

### 在推荐系统中的应用

尽管超曲面几何在上述领域中的应用取得了显著成果，但在会话推荐系统中的应用仍然较少。会话推荐系统中的数据也具有类似的幂律分布和层次结构，尤其是用户的点击行为和兴趣变化往往符合树状或层次性增长的特点。因此，超曲面几何有潜力在此类任务中表现优异。

现有的基于图神经网络的推荐系统通常在欧氏空间中进行嵌入和信息聚合，但由于无法有效捕捉层次结构，模型的表现受到限制。例如，在用户行为数据中，少数热门物品会吸引大量用户点击，而大多数长尾物品则只有少量点击。超曲面几何能够更有效地表示这种数据分布，避免欧氏空间中的高维嵌入和信息丢失。

## 论文的贡献

这篇论文通过引入超曲面几何来解决现有基于欧氏空间方法的局限性，主要的创新点包括：

1. **超曲面空间的嵌入**：论文提出了将用户行为序列中的物品嵌入到洛伦兹超曲面空间，通过利用其负曲率特性，能够更加有效地捕捉用户行为中的层次结构，避免了欧氏空间中的信息损失问题。
2. **自适应超曲面注意力机制**：论文设计了一种新的超曲面注意力机制，能够在超曲面空间中灵活地加权邻居节点的影响，从而更好地捕捉不同物品对用户偏好的影响。
3. **对比学习优化**：通过引入对比学习方法，模型能够在超曲面空间中区分正负样本之间的距离，从而增强推荐结果的精确性和多样性。

论文的实验表明，HCGR在多个公开数据集上大幅提升了推荐的准确性，特别是在命中率、NDCG和MRR指标上相比于现有模型具有显著优势。这表明，超曲面几何可以为捕捉用户行为的层次结构提供有效的解决方案，同时为未来的推荐系统研究指明了新的方向。

# 具体细节

## 超曲面几何空间的介绍

首先，我们从超曲面几何空间（Hyperbolic Space）开始。与我们日常所熟悉的欧氏几何不同。欧氏几何是平坦的，而超曲面几何是"凹"的，负曲率使得其可以更有效地表示具有层次结构的数据，例如社交网络、用户行为序列等。

### 超曲面几何的定义
在论文中，超曲面几何空间被定义为一个具有负常曲率 $c$ 的黎曼流形（Riemannian Manifold），具体定义如下：

$$
\mathcal{H}^d = \left\{ \mathbf{x} \in \mathbb{R}^{d+1} \mid \langle \mathbf{x}, \mathbf{x} \rangle_{\mathcal{L}} = -k, \mathbf{x}_0 > 0 \right\}
$$

其中：
- $\mathcal{H}^d$ 是 $d$ 维的超曲面几何空间。
- $\mathbf{x} \in \mathbb{R}^{d+1}$ 是这个空间中的一个点，表示为 $d+1$ 维坐标。这里使用 $d+1$ 维是为了满足超曲面几何中的负曲率属性。
- $\langle \mathbf{x}, \mathbf{x} \rangle_{\mathcal{L}}$ 是洛伦兹内积（Lorentz inner product），用于在超曲面几何空间中计算两个点之间的相似度或距离。
- $k$ 是负曲率的常数，通常定义为 $k = -1/c$，其中 $c$ 是曲率。

### 洛伦兹内积（Lorentz Inner Product）
洛伦兹内积用于计算超曲面几何中的点的内积，其公式为：

$$
\langle \mathbf{x}, \mathbf{y} \rangle_{\mathcal{L}} = -\mathbf{x}_0 \mathbf{y}_0 + \sum_{i=1}^{d} \mathbf{x}_i \mathbf{y}_i
$$

其中：
- $\mathbf{x}, \mathbf{y}$ 是在 $d+1$ 维空间中的两个点。
- $\mathbf{x}_0, \mathbf{y}_0$ 是这些点的第一个坐标，负号表明该空间的负曲率。
- $\sum_{i=1}^{d} \mathbf{x}_i \mathbf{y}_i$ 表示其余 $d$ 个坐标上的欧式点乘运算。这部分和欧氏几何中的内积类似，表示余下维度上的相似性度量。

洛伦兹内积帮助我们在超曲面几何中定义点与点之间的"内积"或相似度，它在负曲率的空间中使得高维数据可以被更有效地组织在一起。与欧氏几何不同，超曲面几何可以更加紧凑地嵌入层次结构的关系，使得它特别适合用来建模复杂的用户行为。

### 超曲面几何的距离函数
超曲面几何中的点之间的距离由以下公式计算：

$$
d_{\mathcal{L}}(\mathbf{x}, \mathbf{y}) = \sqrt{k} \operatorname{arcosh}\left( -\frac{\langle \mathbf{x}, \mathbf{y} \rangle_{\mathcal{L}}}{k} \right)
$$

其中：
- $d_{\mathcal{L}}(\mathbf{x}, \mathbf{y})$ 表示两个点 $\mathbf{x}$ 和 $\mathbf{y}$ 之间的距离。
- $\operatorname{arcosh}$ 是反双曲余弦函数。
- $\langle \mathbf{x}, \mathbf{y} \rangle_{\mathcal{L}}$ 是洛伦兹内积。
- $k = -\frac{1}{c}$，$c$ 为曲率。

这个公式与欧氏几何中的距离公式有显著不同。欧氏几何中的距离是线性变化的，而在超曲面几何中，距离的变化是指数型的。因此，超曲面几何能够在较小的空间中嵌入更多的信息，并更好地表示层次结构和幂律分布的数据。

### 切空间（Tangent Space）
切空间用于在超曲面几何中进行线性近似。切空间的定义为：

$$
\mathcal{T}_{\mathbf{x}} \mathcal{H}^d = \left\{ \mathbf{v} \in \mathbb{R}^{d+1} \mid \langle \mathbf{v}, \mathbf{x} \rangle_{\mathcal{L}} = 0 \right\}
$$

即在点 $\mathbf{x}$ 处的切空间中的切向量 $\mathbf{v}$ 满足 $\langle \mathbf{v}, \mathbf{x} \rangle_{\mathcal{L}} = 0$，表示切向量与 $\mathbf{x}$ 在该点的正交关系。切空间使得在超曲面几何空间中的计算变得更加简单，它允许我们在高维曲面上进行线性操作，这对于后续的嵌入与信息传递至关重要。

---

## 超曲面几何的映射：指数映射与对数映射

超曲面几何与欧氏空间之间的变换通过指数映射（Exponential Map）和对数映射（Logarithmic Map）实现。

### 指数映射
指数映射是从切空间 $\mathcal{T}_{\mathbf{x}} \mathcal{H}^d$ 到超曲面空间 $\mathcal{H}^d$ 的映射，公式如下：

$$
\exp_{\mathbf{x}}^k(\mathbf{v}) = \cosh\left( \frac{\|\mathbf{v}\|_{\mathcal{L}}}{\sqrt{k}} \right) \mathbf{x} + \sqrt{k} \sinh\left( \frac{\|\mathbf{v}\|_{\mathcal{L}}}{\sqrt{k}} \right) \frac{\mathbf{v}}{\|\mathbf{v}\|_{\mathcal{L}}}
$$

其中：
- $\mathbf{v}$ 是切空间中的向量。
- $\cosh$ 和 $\sinh$ 分别是双曲余弦和双曲正弦函数。
- $\|\mathbf{v}\|_{\mathcal{L}}$ 是向量的洛伦兹范数，表示向量的长度。

指数映射将线性化的切空间向量 $\mathbf{v}$ 映射回超曲面几何中的真实空间，这使得我们可以在超曲面空间中进行非线性操作，如进行坐标的转换或数据嵌入。

### 对数映射
对数映射是指数映射的逆操作，将超曲面空间的点映射到切空间，公式如下：

$$
\log_{\mathbf{x}}^k(\mathbf{y}) = d_{\mathcal{L}}(\mathbf{x}, \mathbf{y}) \frac{\mathbf{y} + \frac{1}{k} \langle \mathbf{x}, \mathbf{y} \rangle_{\mathcal{L}} \mathbf{x}}{\left\|\mathbf{y} + \frac{1}{k} \langle \mathbf{x}, \mathbf{y} \rangle_{\mathcal{L}} \mathbf{x} \right\|_{\mathcal{L}}}
$$

其中：
- $\log_{\mathbf{x}}^k(\mathbf{y})$ 表示将 $\mathbf{y}$ 从超曲面空间映射回切空间。
- $\frac{1}{k}$ 是曲率的倒数，确保与超曲面几何的曲率一致。

对数映射提供了从超曲面几何空间到切空间的线性近似，使得我们可以在局部进行欧氏几何的线性操作，例如更新向量或进行简单的向量加法，这种操作在复杂的几何结构中尤为重要。

## 图神经网络（GNN）中的信息聚合机制

在这篇论文中，图神经网络用于捕捉用户行为序列中的层次结构。其信息聚合公式为：

$$
\mathbf{t}_v^{(l+1)} = f_{\text{aggregator}} \left( \mathbf{x}_u^{(l)}, u \in N(v) \right)
$$

$f_{\text{aggregator}}$ 是聚合函数，它将邻居节点的信息汇总在一起，通常是求和、平均或加权求和。$\mathbf{x}_u^{(l)}$ 是邻居节点 $u$ 在第 $l$ 层的表示。
$$
\mathbf{x}_v^{(l+1)} = f_{\text{updater}} \left( \mathbf{x}_v^{(l)}, \mathbf{t}_v^{(l+1)} \right)
$$

$f_{\text{updater}}$ 是更新函数，用于根据当前的节点表示 $\mathbf{x}_v^{(l)}$ 和聚合的邻居信息 $\mathbf{t}_v^{(l+1)}$ 来更新节点表示。通常，$f_{\text{updater}}$ 可以是一个简单的非线性函数（如ReLU），或是更加复杂的函数如GRU或LSTM。

其中：

- $\mathbf{t}_v^{(l+1)}$ 是节点 $v$ 的临时向量，表示从邻居节点 $u$ 聚合来的信息。
- $f_{\text{aggregator}}$ 是信息聚合器，用于整合邻居节点的信息。
- $f_{\text{updater}}$ 是更新器，用于更新节点状态。

## 超曲面几何中的注意力机制

## 超曲面几何中的信息聚合机制

在超曲面几何中，为了有效捕捉用户点击的物品之间的层次结构，信息的聚合方式必须能够表示不同层次之间的联系。本文提出了基于超曲面几何的注意力机制，用于对用户的偏好进行建模。

为了在超曲面中执行加法和乘法等向量操作，传统方法面临着技术难题。通过参考之前的工作，论文提出了在超曲面中进行这些操作的解决方案。以下是核心的向量操作定义：

### 向量操作
对于超曲面中的向量乘法和加法，公式分别如下：

$$
W \otimes^k \mathbf{x}^H := \exp_{\mathbf{o}}^k\left(W \log_{\mathbf{o}}^k\left(\mathbf{x}^H\right)\right)
$$

$$
\mathbf{x}^H \oplus^k \mathbf{b} := \exp_{\mathbf{x}^H}^k\left(P_{o \rightarrow \mathbf{x}^H}^k(\mathbf{b})\right)
$$

在这里：
- $W \otimes^k \mathbf{x}^H$ 代表了在超曲面中进行向量的加权操作，其中 $W$ 是权重矩阵。
- $\log_{\mathbf{o}}^k$ 是将超曲面空间中的点映射到其切空间的对数映射，使得向量加法和乘法可以在欧几里得空间内近似计算。
- $\exp_{\mathbf{o}}^k$ 是指数映射，将点从切空间映射回超曲面空间。

### 平行运输（Parallel Transport）
由于超曲面是非欧空间，平行运输用于在不同的切空间之间传递向量而不改变其本质特征。对于任意两点 $\mathbf{x}$ 和 $\mathbf{y}$，切向量 $\mathbf{v}$ 的平行运输定义如下：

$$
P_{\mathbf{x} \rightarrow \mathbf{y}}(\mathbf{v}) = \mathbf{v} - \frac{\langle \log_{\mathbf{x}}(\mathbf{y}), \mathbf{v} \rangle_L}{d_{\mathcal{L}}(\mathbf{x}, \mathbf{y})^2} (\log_{\mathbf{x}}(\mathbf{y}) + \log_{\mathbf{y}}(\mathbf{x}))
$$

其中，$\langle \cdot, \cdot \rangle_L$ 表示洛伦兹内积，$d_{\mathcal{L}}(\mathbf{x}, \mathbf{y})$ 是 $\mathbf{x}$ 和 $\mathbf{y}$ 之间的距离。

## 超曲面几何中的非线性激活

在深度学习中，非线性激活是模型表达能力的关键。为了适应超曲面几何中的负曲率，论文提出了新的非线性激活机制。公式如下：

$$
\sigma^{\otimes^{k_{l+1}, k_l}}\left(\mathbf{x}^H\right) = \exp_{\mathbf{o}}^{k_{l+1}}\left(\sigma\left(\log_{\mathbf{o}}^{k_l}\left(\mathbf{x}^H\right)\right)\right)
$$

在此公式中：
- $\sigma$ 表示传统的非线性激活函数，例如 ReLU 或 sigmoid。
- $k_l$ 和 $k_{l+1}$ 分别表示第 $l$ 层和第 $l+1$ 层的超曲面的曲率。
- $\log_{\mathbf{o}}^{k_l}$ 和 $\exp_{\mathbf{o}}^{k_{l+1}}$ 分别是从超曲面空间映射到切空间的对数映射和从切空间映射回超曲面空间的指数映射。

通过这种方式，超曲面的几何结构在网络的各层之间保持一致，同时支持非线性操作。

## 超曲面几何中的注意力机制

为了捕捉用户行为中的层次关系，论文提出了基于超曲面几何的图神经网络（GNN）中的注意力机制。传统的注意力机制在欧几里得空间中进行计算，而本文扩展了这种机制，使其在超曲面空间中生效。

### 注意力权重的计算

$$
w_{ij} = \operatorname{Softmax}_{j \in \mathcal{N}(i)} \left( W_a \left( \log_{\mathbf{o}}^k \left( \mathbf{x}_i^{H, l} \right) \| \log_{\mathbf{o}}^k \left( \mathbf{x}_j^{H, l} \right) \right) + b_a \right)
$$

其中：
- $w_{ij}$ 是节点 $i$ 和 $j$ 之间的注意力权重。
- $\log_{\mathbf{o}}^k$ 是对数映射，用于将超曲面空间中的点 $\mathbf{x}_i$ 和 $\mathbf{x}_j$ 映射到切空间。
- $W_a$ 是可学习的权重矩阵，$b_a$ 是偏置项。
- $\|$ 表示向量的拼接操作，用于将两个切空间中的向量组合。

## 对比学习的损失函数

论文使用对比学习损失来优化模型的推荐性能。其损失函数由交叉熵损失和对比损失组成。

### 交叉熵损失

$$
L_e = -\sum_{i=1}^{n} \left( \mathbf{y}_i \log \left( \widehat{\mathbf{y}}_i \right) + (1 - \mathbf{y}_i) \log \left( 1 - \widehat{\mathbf{y}}_i \right) \right)
$$

交叉熵损失用于评估模型的推荐准确性，其中 $\mathbf{y}_i$ 是真实标签，$\widehat{\mathbf{y}}_i$ 是模型的预测概率。

### 对比学习损失

$$
L_c = \sum_{i=1}^{n} \max \left( d_L \left( \widehat{\mathbf{y}}_i, \mathbf{y}_P \right) - d_L \left( \widehat{\mathbf{y}}_i, \mathbf{y}_N \right) + \xi, 0 \right)
$$

对比学习损失用于将正样本和负样本之间的距离分开，$d_L(\cdot, \cdot)$ 表示超曲面空间中的洛伦兹距离，$\mathbf{y}_P$ 和 $\mathbf{y}_N$ 分别是正样本和负样本。

最终的总损失函数结合了交叉熵损失和对比学习损失：

$$
L_{\text{total}} = \gamma * L_e + \beta * L_c
$$

其中，$\gamma$ 和 $\beta$ 分别控制两个损失函数的权重。

# 局限性

超曲面几何在推荐系统中的应用虽然具有一些理论优势，但其局限性也比较明显，导致目前并没有被广泛采用：

1. **计算复杂性高**：超曲面几何涉及复杂的数学运算，如对数映射和指数映射，这些操作在高维空间中计算成本较高，尤其是在大规模推荐场景下，实时处理困难。
2. **难以优化**：超曲面几何空间中的梯度计算和优化过程相比欧氏空间更加复杂，这使得训练神经网络时，模型容易出现收敛慢或训练不稳定的现象。
3. **模型解释性差**：尽管超曲面几何可以更好地表示层次结构，但对非专业用户而言，它的几何解释性不如欧氏空间直观，使得推荐结果难以解释和信任。
4. **工具和框架支持较少**：当前主流的深度学习框架（如TensorFlow和PyTorch）主要针对欧氏空间的操作进行了优化，而对超曲面几何的支持仍然有限，工具链不成熟。

因此，虽然超曲面几何在理论上有较好的潜力，但由于上述技术障碍，它还未成为推荐系统领域的主流选择。
