<!DOCTYPE html>



<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Chen Kai">
    
    <title>
        
            推荐系统（五）—— Embedding表示学习 |
        
        Chen Kai Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.svg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"chenk.top","root":"/","language":"zh-CN","default_language":"zh-CN","languages":["zh-CN","en"],"path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.svg","favicon":"/images/logo.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chen Kai Blog" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    
    
    
    
    

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Chen Kai Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            
                            
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                    <li class="menu-item lang-switch lang-switch-trigger" title="Language">
                        <i class="fas fa-globe"></i>
                    </li>
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item lang-switch-trigger"><i class="fas fa-globe"></i></div>
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    
                    
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    
    
    
    

    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">推荐系统（五）—— Embedding表示学习</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Chen Kai</span>
                        
                            <span class="author-label">BOSS</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    
    
    
    
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2025-06-26 00:00:00</span>
        <span class="mobile">2025-06-26 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Recommendation-Systems/">Recommendation Systems</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Deep-Learning/">Deep Learning</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Embedding/">Embedding</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>25.7k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>103 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>在推荐系统中，如何将用户和物品表示为计算机能够理解和计算的向量，是一个核心问题。Embedding（嵌入）技术正是解决这一问题的关键。从早期的协同过滤到如今的深度学习推荐系统，Embedding
技术经历了从简单到复杂、从浅层到深层的演进过程。</p>
<p>本文将深入探讨推荐系统中 Embedding
表示学习的理论基础与实践方法。我们会从最基础的 Embedding
概念出发，逐步深入到 Item2Vec、Node2Vec、Two-Tower 模型、DSSM
等经典算法，并详细讲解负采样策略、ANN 近邻检索技术，以及如何评估
Embedding
的质量。每个算法都会配有完整的代码实现，帮助读者真正理解和掌握这些技术。</p>
<span id="more"></span>
<h2 id="embedding-基础理论">Embedding 基础理论</h2>
<h3 id="什么是-embedding">什么是 Embedding</h3>
<p>Embedding 是将高维稀疏的离散对象（如用户 ID、物品
ID、类别等）映射到低维稠密的连续向量空间中的技术。在推荐系统中，这些向量能够捕获对象之间的语义相似性：相似的物品在向量空间中距离更近，不相似的物品距离更远。</p>
<p>形式上，给定一个离散对象集合 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.481ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 8610.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g><g data-mml-node="mo" transform="translate(1046.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mo" transform="translate(2102.6,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"/></g><g data-mml-node="msub" transform="translate(2602.6,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mn" transform="translate(518,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(3524.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(3968.8,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mn" transform="translate(518,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(4890.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(5335,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"/></g><g data-mml-node="mo" transform="translate(6673.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(7118.3,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(8110.6,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"/></g></g></g></svg></mjx-container></span>，Embedding 函数 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="11.043ex" height="2.395ex" role="img" focusable="false" viewbox="0 -853.7 4880.8 1058.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(827.8,0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"/></g><g data-mml-node="mi" transform="translate(1383.6,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g><g data-mml-node="mo" transform="translate(2430.3,0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"/></g><g data-mml-node="msup" transform="translate(3708.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"/></g></g><g data-mml-node="mi" transform="translate(755,363) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></g></svg></mjx-container></span> 将每个对象
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.837ex" height="1.359ex" role="img" focusable="false" viewbox="0 -443 812 600.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span> 映射到一个 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 维向量 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="7.351ex" height="2.288ex" role="img" focusable="false" viewbox="0 -853.7 3249.2 1011.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1131.7,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/></g><g data-mml-node="msup" transform="translate(2076.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"/></g></g><g data-mml-node="mi" transform="translate(755,363) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></g></svg></mjx-container></span>，其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 通常远小于 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex;" xmlns="http://www.w3.org/2000/svg" width="2.998ex" height="2.26ex" role="img" focusable="false" viewbox="0 -749.5 1325 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g><g data-mml-node="mo" transform="translate(1047,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g></g></g></svg></mjx-container></span>。</p>
<h3 id="embedding-的优势">Embedding 的优势</h3>
<p><strong>维度压缩</strong>：原始的特征空间可能是百万甚至千万维的（每个
ID 对应一维），而 Embedding
将其压缩到几十到几百维，大大降低了存储和计算成本。</p>
<p><strong>语义捕获</strong>：通过训练，Embedding
能够学习到对象之间的潜在关系。例如，经常被同一用户点击的物品，其
Embedding 向量会更接近。</p>
<p><strong>泛化能力</strong>：对于训练集中未出现的新物品，可以通过其属性或其他信息初始化
Embedding，然后通过少量数据快速学习。</p>
<p><strong>可计算性</strong>：向量空间中的相似度计算（如余弦相似度、内积）比原始特征空间中的计算更高效。</p>
<h3 id="embedding-学习的基本原理">Embedding 学习的基本原理</h3>
<p>Embedding 学习的基本思路：<strong>相似的对象应该有相似的 Embedding
向量</strong>。这个"相似性"的定义取决于具体的任务和算法：</p>
<ul>
<li><strong>协同过滤</strong>：被同一用户交互过的物品相似</li>
<li><strong>序列推荐</strong>：在序列中相邻的物品相似</li>
<li><strong>图结构</strong>：在图中有边连接的节点相似</li>
<li><strong>语义匹配</strong>：在语义空间中匹配的查询和文档相似</li>
</ul>
<h3 id="损失函数与优化目标">损失函数与优化目标</h3>
<p>大多数 Embedding 学习算法都可以归结为优化以下形式的损失函数：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.222ex;" xmlns="http://www.w3.org/2000/svg" width="33.463ex" height="5.371ex" role="img" focusable="false" viewbox="0 -950 14790.5 2374.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g><g data-mml-node="mo" transform="translate(958.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="munder" transform="translate(2014.6,0)"><g data-mml-node="mo" transform="translate(414,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(0,-1147.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(1012,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g><g data-mml-node="mo" transform="translate(1424,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(1813,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2480,0)"><g data-mml-node="mi"><path data-c="50" d="M37 475Q19 475 19 487Q19 536 103 604T327 682Q329 682 344 682T380 682T421 683H463Q625 683 695 615Q718 591 726 564Q733 547 733 525Q733 412 607 312T321 205H312Q293 205 293 217Q293 224 302 236T333 260T385 274Q558 287 614 407Q633 445 633 477Q633 515 612 543T556 585T481 607T399 614H370L368 603Q352 463 312 312T242 82T202 -13Q190 -33 164 -45T121 -57Q108 -57 108 -45Q108 -40 120 -10T151 73T192 190T233 349T266 539Q267 546 269 565T272 598T274 613H270Q209 613 163 588Q131 572 113 518Q102 502 80 490T37 475Z"/></g></g></g></g><g data-mml-node="mi" transform="translate(4453.2,0)"><path data-c="2113" d="M345 104T349 104T361 95T369 80T352 59Q268 -20 206 -20Q170 -20 146 3T113 53T99 104L94 129Q94 130 79 116T48 86T28 70Q22 70 15 79T7 94Q7 98 12 103T58 147L91 179V185Q91 186 91 191T92 200Q92 282 128 400T223 612T336 705Q397 705 397 636V627Q397 453 194 233Q185 223 180 218T174 211T171 208T165 201L163 186Q159 142 159 123Q159 17 208 17Q228 17 253 30T293 56T335 94Q345 104 349 104ZM360 634Q360 655 354 661T336 668Q328 668 322 666T302 645T272 592Q252 547 229 467T192 330L179 273Q179 272 186 280T204 300T221 322Q327 453 355 590Q360 612 360 634Z"/></g><g data-mml-node="mo" transform="translate(4870.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(5259.2,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(5809.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(6198.2,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(7052.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(7496.8,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g><g data-mml-node="mo" transform="translate(8398.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(8787.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(9231.8,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mo" transform="translate(10340,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(10951.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(11951.5,0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"/></g><g data-mml-node="mi" transform="translate(12534.5,0)"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"/></g><g data-mml-node="mo" transform="translate(13256.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(13645.5,0)"><g data-mml-node="mi"><path data-c="1D404" d="M723 286Q721 284 700 145T677 3V0H39V62H147V618H39V680H660V676Q662 670 675 552T691 428V424H629V428Q629 429 627 448T618 494T601 541Q574 593 527 605T382 618H374H304V384H336Q338 384 347 384T361 384T376 386T392 390T407 397T421 407T432 423Q442 444 443 482V501H505V205H443V224Q442 258 435 278T411 307T380 318T336 322H304V62H375H394Q429 62 449 62T497 66T541 76T577 95T609 126T632 170T651 232Q661 287 661 289H723V286Z"/></g></g><g data-mml-node="mo" transform="translate(14401.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p>
<p>其中：</p>
<ul>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.129ex;" xmlns="http://www.w3.org/2000/svg" width="1.658ex" height="1.674ex" role="img" focusable="false" viewbox="0 -683 733 740"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="50" d="M37 475Q19 475 19 487Q19 536 103 604T327 682Q329 682 344 682T380 682T421 683H463Q625 683 695 615Q718 591 726 564Q733 547 733 525Q733 412 607 312T321 205H312Q293 205 293 217Q293 224 302 236T333 260T385 274Q558 287 614 407Q633 445 633 477Q633 515 612 543T556 585T481 607T399 614H370L368 603Q352 463 312 312T242 82T202 -13Q190 -33 164 -45T121 -57Q108 -57 108 -45Q108 -40 120 -10T151 73T192 190T233 349T266 539Q267 546 269 565T272 598T274 613H270Q209 613 163 588Q131 572 113 518Q102 502 80 490T37 475Z"/></g></g></g></g></svg></mjx-container></span>
是正样本对集合（相似的对象对）</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="7.982ex" height="2.363ex" role="img" focusable="false" viewbox="0 -750 3527.9 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(939,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1793,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(2237.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g><g data-mml-node="mo" transform="translate(3138.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>
是相似度函数（如内积、余弦相似度）</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="2.507ex" height="1.666ex" role="img" focusable="false" viewbox="0 -442 1108.3 736.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></svg></mjx-container></span> 是标签（通常为 1
表示相似，0 表示不相似）</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.045ex;" xmlns="http://www.w3.org/2000/svg" width="0.943ex" height="1.64ex" role="img" focusable="false" viewbox="0 -705 417 725"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="2113" d="M345 104T349 104T361 95T369 80T352 59Q268 -20 206 -20Q170 -20 146 3T113 53T99 104L94 129Q94 130 79 116T48 86T28 70Q22 70 15 79T7 94Q7 98 12 103T58 147L91 179V185Q91 186 91 191T92 200Q92 282 128 400T223 612T336 705Q397 705 397 636V627Q397 453 194 233Q185 223 180 218T174 211T171 208T165 201L163 186Q159 142 159 123Q159 17 208 17Q228 17 253 30T293 56T335 94Q345 104 349 104ZM360 634Q360 655 354 661T336 668Q328 668 322 666T302 645T272 592Q252 547 229 467T192 330L179 273Q179 272 186 280T204 300T221 322Q327 453 355 590Q360 612 360 634Z"/></g></g></g></svg></mjx-container></span>
是损失函数（如交叉熵、均方误差）</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.104ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2256 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"/></g><g data-mml-node="mo" transform="translate(722,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1111,0)"><g data-mml-node="mi"><path data-c="1D404" d="M723 286Q721 284 700 145T677 3V0H39V62H147V618H39V680H660V676Q662 670 675 552T691 428V424H629V428Q629 429 627 448T618 494T601 541Q574 593 527 605T382 618H374H304V384H336Q338 384 347 384T361 384T376 386T392 390T407 397T421 407T432 423Q442 444 443 482V501H505V205H443V224Q442 258 435 278T411 307T380 318T336 322H304V62H375H394Q429 62 449 62T497 66T541 76T577 95T609 126T632 170T651 232Q661 287 661 289H723V286Z"/></g></g><g data-mml-node="mo" transform="translate(1867,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>
是正则化项</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.71ex" height="1.538ex" role="img" focusable="false" viewbox="0 -680 756 680"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D404" d="M723 286Q721 284 700 145T677 3V0H39V62H147V618H39V680H660V676Q662 670 675 552T691 428V424H629V428Q629 429 627 448T618 494T601 541Q574 593 527 605T382 618H374H304V384H336Q338 384 347 384T361 384T376 386T392 390T407 397T421 407T432 423Q442 444 443 482V501H505V205H443V224Q442 258 435 278T411 307T380 318T336 322H304V62H375H394Q429 62 449 62T497 66T541 76T577 95T609 126T632 170T651 232Q661 287 661 289H723V286Z"/></g></g></g></g></svg></mjx-container></span> 是所有对象的
Embedding 矩阵</li>
</ul>
<h2 id="item2vec-与-word2vec-类比">Item2Vec 与 Word2Vec 类比</h2>
<h3 id="word2vec-回顾">Word2Vec 回顾</h3>
<p>Word2Vec 是 Mikolov 等人在 2013
年提出的词向量学习算法，它通过"上下文预测词"或"词预测上下文"的方式学习词的
Embedding。Word2Vec 有两种架构：</p>
<p><strong>Skip-gram</strong>：给定中心词，预测上下文词
<strong>CBOW（Continuous Bag of
Words）</strong>：给定上下文词，预测中心词</p>
<p>Skip-gram 的目标函数为：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.112ex;" xmlns="http://www.w3.org/2000/svg" width="33.941ex" height="7.023ex" role="img" focusable="false" viewbox="0 -1728.7 15001.9 3104"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g><g data-mml-node="mo" transform="translate(958.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(2014.6,0)"><g data-mml-node="mn" transform="translate(322,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><rect width="904" height="60" x="120" y="220"/></g><g data-mml-node="munderover" transform="translate(3325.2,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(142.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(473.1,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g></g></g><g data-mml-node="munder" transform="translate(4935.9,0)"><g data-mml-node="mo" transform="translate(1250.8,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/></g><g data-mml-node="TeXAtom" transform="translate(0,-1123.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/></g><g data-mml-node="mo" transform="translate(1211,0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"/></g><g data-mml-node="mi" transform="translate(1989,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g><g data-mml-node="mo" transform="translate(2401,0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"/></g><g data-mml-node="mi" transform="translate(3179,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/></g><g data-mml-node="mo" transform="translate(3612,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(3890,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g><g data-mml-node="mo" transform="translate(4302,0)"><path data-c="2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"/></g><g data-mml-node="mn" transform="translate(5080,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g></g><g data-mml-node="mi" transform="translate(9048.2,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"/></g><g data-mml-node="mo" transform="translate(10326.2,0)"><path data-c="2061" d=""/></g><g data-mml-node="mi" transform="translate(10492.9,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mo" transform="translate(10995.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(11384.9,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1139,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mo" transform="translate(13280.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(13558.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(14612.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p>
<p>其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="10.201ex" height="2.363ex" role="img" focusable="false" viewbox="0 -750 4509 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(892,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1139,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mo" transform="translate(2787.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(3065.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(4120,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span> 通过
softmax 计算：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.112ex;" xmlns="http://www.w3.org/2000/svg" width="31.529ex" height="7.086ex" role="img" focusable="false" viewbox="0 -1756.8 13935.9 3132.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(892,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1139,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mo" transform="translate(2787.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msub" transform="translate(3065.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g><g data-mml-node="mo" transform="translate(4120,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(4786.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(5842.5,0)"><g data-mml-node="mrow" transform="translate(1160.7,915.1)"><g data-mml-node="mi"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"/><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"/></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(1917,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="mi" transform="translate(640,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="TeXAtom" transform="translate(640,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1139,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g><g data-mml-node="msub" transform="translate(3947.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g></g></g><g data-mml-node="mo" transform="translate(5383,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-1020)"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="TeXAtom" transform="translate(1089,530) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g><g data-mml-node="mo" transform="translate(1047,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mo" transform="translate(716,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1494,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mi" transform="translate(2715.6,0)"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"/><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"/></g><g data-mml-node="mo" transform="translate(4243.6,0)"><path data-c="2061" d=""/></g><g data-mml-node="mo" transform="translate(4243.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(4632.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="mi" transform="translate(640,289) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mi" transform="translate(640,-247) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g></g><g data-mml-node="msub" transform="translate(5828.9,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D42F" d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g></g></g><g data-mml-node="mo" transform="translate(7264.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><rect width="7853.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span></p>
<h3 id="item2vec-的核心思想">Item2Vec 的核心思想</h3>
<p>Item2Vec 将 Word2Vec
的思想迁移到推荐系统：<strong>将用户的行为序列看作句子，将物品看作词</strong>。如果两个物品经常在同一个用户的序列中出现（即"共现"），那么它们的
Embedding 应该相似。</p>
<h3 id="item2vec-算法流程">Item2Vec 算法流程</h3>
<ol type="1">
<li><strong>构建序列</strong>：将每个用户的历史交互记录按时间排序，形成物品序列</li>
<li><strong>定义上下文</strong>：对于序列中的每个物品，将其前后 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.98ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 433 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/></g></g></g></svg></mjx-container></span> 个物品作为上下文</li>
<li><strong>训练模型</strong>：使用 Skip-gram 或 CBOW 架构，学习物品的
Embedding</li>
</ol>
<h3 id="item2vec-完整实现">Item2Vec 完整实现</h3>
<h4 id="问题背景">问题背景</h4>
<p>在推荐系统中，如何学习物品的有效表示是一个核心问题。传统的协同过滤方法（如矩阵分解）需要显式的用户-物品交互矩阵，但在实际应用中，我们往往只有用户的行为序列数据（如点击序列、购买序列）。这些序列数据蕴含着丰富的物品共现信息：如果两个物品经常在同一个用户的行为序列中出现，它们很可能具有相似的性质或满足相似的用户需求。然而，如何从这些序列数据中提取物品的语义表示，并捕获物品之间的相似性，是一个挑战。</p>
<h4 id="解决思路">解决思路</h4>
<p>Item2Vec 借鉴了自然语言处理中 Word2Vec
的思想，将推荐问题转化为序列建模问题。核心洞察是：用户的行为序列可以类比为自然语言中的句子，每个物品类比为一个词。如果两个物品在序列中经常共现（出现在相近的位置），它们的
Embedding 向量应该相似。具体而言，Item2Vec 采用 Skip-gram
架构：对于序列中的每个中心物品，模型学习预测其上下文物品（窗口内的其他物品）。通过最大化正样本对的相似度、最小化负样本对的相似度，模型能够学习到物品的向量表示，使得语义相似的物品在向量空间中距离更近。</p>
<h4 id="设计考虑">设计考虑</h4>
<p>在实现 Item2Vec 时，有几个关键的设计决策需要考虑：</p>
<ol type="1">
<li><strong>两套 Embedding 矩阵</strong>：与 Word2Vec
类似，我们使用两套独立的 Embedding 矩阵——中心物品 Embedding 和上下文物品
Embedding。这种设计提供了更大的模型容量，让模型能够更灵活地学习不同角色的表示。最终我们只使用中心物品
Embedding 作为物品的最终表示。</li>
<li><strong>负采样策略</strong>：直接计算所有物品的 softmax
在物品数量很大时计算开销巨大。负采样技术通过随机采样少量负样本（通常5-20个）来近似完整的
softmax，大幅提升训练效率。负采样分布采用频率的 3/4
次幂，既避免高频词被过度采样，又给低频词一定的学习机会。</li>
<li><strong>窗口大小选择</strong>：上下文窗口大小决定了模型考虑多远的共现关系。窗口太小（如1-2）只能捕获局部共现，窗口太大（如10+）可能引入噪声。通常窗口大小设置为3-5，在捕获共现关系和计算效率之间取得平衡。</li>
<li><strong>数值稳定性</strong>：内积计算可能产生很大的值，导致 sigmoid
函数溢出。需要在计算损失前对得分进行裁剪（clamp），限制在合理范围内（如[-10,
10]）。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Item2Vec</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Item2Vec 模型实现</span></span><br><span class="line"><span class="string">    基于 Skip-gram 架构，通过预测上下文物品来学习物品的向量表示</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    核心思想：如果两个物品经常在同一个用户的行为序列中出现（共现），</span></span><br><span class="line"><span class="string">    那么它们的 Embedding 向量应该相似</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化 Item2Vec 模型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            vocab_size: 物品词汇表大小（即物品总数）</span></span><br><span class="line"><span class="string">            embedding_dim: Embedding 向量的维度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(Item2Vec, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 中心词（物品）的 Embedding 矩阵</span></span><br><span class="line">        <span class="comment"># 这是我们最终要使用的物品表示</span></span><br><span class="line">        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 上下文词（物品）的 Embedding 矩阵</span></span><br><span class="line">        <span class="comment"># 这是一个辅助的 Embedding，用于训练过程</span></span><br><span class="line">        <span class="comment"># Word2Vec 论文建议使用两套独立的 Embedding 可以提高训练效果</span></span><br><span class="line">        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 初始化中心词 Embedding：使用小范围的均匀分布</span></span><br><span class="line">        <span class="comment"># 这样可以避免初始值过大导致的梯度爆炸问题</span></span><br><span class="line">        self.center_embeddings.weight.data.uniform_(-<span class="number">0.5</span> / embedding_dim, <span class="number">0.5</span> / embedding_dim)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 初始化上下文词 Embedding：全部置零</span></span><br><span class="line">        <span class="comment"># 上下文 Embedding 从零开始学习可以提供更好的训练稳定性</span></span><br><span class="line">        self.context_embeddings.weight.data.zero_()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, center_words, context_words, neg_words</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        前向传播：计算损失函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        使用负采样（Negative Sampling）技术简化 softmax 计算：</span></span><br><span class="line"><span class="string">        - 对于每个正样本对（中心词-上下文词），最大化它们的相似度</span></span><br><span class="line"><span class="string">        - 对于每个负样本对（中心词-负样本词），最小化它们的相似度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            center_words: [batch_size] 中心物品 ID</span></span><br><span class="line"><span class="string">            context_words: [batch_size] 上下文物品 ID（正样本）</span></span><br><span class="line"><span class="string">            neg_words: [batch_size, num_neg] 负采样物品 ID</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            loss: 标量，负对数似然损失</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 获取中心词的 Embedding 向量</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, embedding_dim]</span></span><br><span class="line">        center_emb = self.center_embeddings(center_words)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 获取正样本上下文词的 Embedding 向量</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, embedding_dim]</span></span><br><span class="line">        context_emb = self.context_embeddings(context_words)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 获取负样本的 Embedding 向量</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, num_neg, embedding_dim]</span></span><br><span class="line">        neg_emb = self.context_embeddings(neg_words)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 计算正样本得分：中心词与上下文词的内积</span></span><br><span class="line">        <span class="comment"># 内积越大，表示两个物品越相似</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size]</span></span><br><span class="line">        pos_score = torch.<span class="built_in">sum</span>(center_emb * context_emb, dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 限制得分范围，防止数值溢出</span></span><br><span class="line">        <span class="comment"># 这是一个重要的数值稳定性技巧</span></span><br><span class="line">        pos_score = torch.clamp(pos_score, <span class="built_in">max</span>=<span class="number">10</span>, <span class="built_in">min</span>=-<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 计算负样本得分：中心词与每个负样本的内积</span></span><br><span class="line">        <span class="comment"># 使用 bmm (batch matrix multiplication) 高效计算</span></span><br><span class="line">        <span class="comment"># neg_emb: [batch_size, num_neg, embedding_dim]</span></span><br><span class="line">        <span class="comment"># center_emb.unsqueeze(2): [batch_size, embedding_dim, 1]</span></span><br><span class="line">        <span class="comment"># 结果形状: [batch_size, num_neg]</span></span><br><span class="line">        neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 同样限制负样本得分范围</span></span><br><span class="line">        neg_score = torch.clamp(neg_score, <span class="built_in">max</span>=<span class="number">10</span>, <span class="built_in">min</span>=-<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 计算正样本损失：希望 sigmoid(pos_score) 接近 1</span></span><br><span class="line">        <span class="comment"># 使用 -log(sigmoid(x)) 作为损失函数</span></span><br><span class="line">        <span class="comment"># 加上 1e-10 防止 log(0) 导致 NaN</span></span><br><span class="line">        pos_loss = -torch.log(torch.sigmoid(pos_score) + <span class="number">1e-10</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 计算负样本损失：希望 sigmoid(-neg_score) 接近 1</span></span><br><span class="line">        <span class="comment"># 即希望 sigmoid(neg_score) 接近 0，表示中心词与负样本不相似</span></span><br><span class="line">        <span class="comment"># 对所有负样本的损失求和</span></span><br><span class="line">        neg_loss = -torch.log(torch.sigmoid(-neg_score) + <span class="number">1e-10</span>).<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 总损失 = 正样本损失 + 负样本损失，然后对 batch 求平均</span></span><br><span class="line">        <span class="keyword">return</span> (pos_loss + neg_loss).mean()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_embeddings</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取最终的物品 Embedding</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        通常我们使用中心词 Embedding 作为最终的物品表示，</span></span><br><span class="line"><span class="string">        因为它直接对应于我们要建模的物品</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            embeddings: numpy array，形状 [vocab_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.center_embeddings.weight.data.cpu().numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Item2VecTrainer</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Item2Vec 训练器</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    负责管理整个训练流程，包括：</span></span><br><span class="line"><span class="string">    1. 从用户行为序列生成训练样本</span></span><br><span class="line"><span class="string">    2. 构建负采样分布</span></span><br><span class="line"><span class="string">    3. 执行训练过程</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequences, vocab_size, embedding_dim=<span class="number">128</span>, window_size=<span class="number">5</span>, </span></span><br><span class="line"><span class="params">                 num_negatives=<span class="number">5</span>, batch_size=<span class="number">256</span>, learning_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化训练器</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            sequences: 用户行为序列列表，每个序列是物品 ID 的列表</span></span><br><span class="line"><span class="string">                      例如: [[0,1,2,3], [1,2,4,5], ...]</span></span><br><span class="line"><span class="string">            vocab_size: 物品词汇表大小（物品总数）</span></span><br><span class="line"><span class="string">            embedding_dim: Embedding 维度</span></span><br><span class="line"><span class="string">            window_size: 上下文窗口大小，表示中心词前后各取多少个词</span></span><br><span class="line"><span class="string">                        例如 window_size=2 表示取前后各2个词</span></span><br><span class="line"><span class="string">            num_negatives: 负采样数量，每个正样本对应的负样本数</span></span><br><span class="line"><span class="string">            batch_size: 训练批次大小</span></span><br><span class="line"><span class="string">            learning_rate: 学习率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.sequences = sequences</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.num_negatives = num_negatives</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建词频表（用于负采样）</span></span><br><span class="line">        <span class="comment"># 统计每个物品在所有序列中出现的次数</span></span><br><span class="line">        self.word_freq = self._build_word_freq()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建负采样分布</span></span><br><span class="line">        <span class="comment"># 根据词频构建采样概率分布，高频词有更高的被采样概率</span></span><br><span class="line">        self.word_dist = self._build_word_distribution()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 初始化 Item2Vec 模型</span></span><br><span class="line">        self.model = Item2Vec(vocab_size, embedding_dim)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 使用 Adam 优化器</span></span><br><span class="line">        <span class="comment"># Adam 对学习率不敏感，能够自适应调整每个参数的学习率</span></span><br><span class="line">        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 生成训练样本对</span></span><br><span class="line">        <span class="comment"># 将所有序列转换为（中心词，上下文词）对</span></span><br><span class="line">        self.train_pairs = self._generate_training_pairs()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_word_freq</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        统计词频</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        遍历所有序列，统计每个物品出现的次数</span></span><br><span class="line"><span class="string">        这个频率将用于构建负采样分布</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            word_freq: 字典，{物品ID: 出现次数}</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_freq = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> self.sequences:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> seq:</span><br><span class="line">                word_freq[word] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(word_freq)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_word_distribution</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构建负采样分布（使用幂律分布）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Word2Vec 论文建议使用频率的 3/4 次幂作为采样概率，</span></span><br><span class="line"><span class="string">        这样可以平衡高频词和低频词的采样概率：</span></span><br><span class="line"><span class="string">        - 如果直接使用频率，高频词会被过度采样</span></span><br><span class="line"><span class="string">        - 如果使用均匀分布，低频词会被过度采样</span></span><br><span class="line"><span class="string">        - 使用 0.75 次幂是一个经验性的折中方案</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            word_dist: numpy array，每个物品的采样概率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 构建频率数组，索引对应物品 ID</span></span><br><span class="line">        word_freq_array = np.array([self.word_freq.get(i, <span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size)])</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 使用 3/4 次幂（0.75）</span></span><br><span class="line">        <span class="comment"># 这是 Word2Vec 论文的经验设置</span></span><br><span class="line">        word_dist = np.power(word_freq_array, <span class="number">0.75</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 归一化为概率分布</span></span><br><span class="line">        word_dist = word_dist / word_dist.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> word_dist</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_training_pairs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        生成训练样本对</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        对每个序列中的每个物品，提取其上下文物品，</span></span><br><span class="line"><span class="string">        形成（中心物品，上下文物品）训练样本对</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        例如：序列 [0, 1, 2, 3, 4]，window_size=2</span></span><br><span class="line"><span class="string">        - 物品 2 的上下文：[0, 1, 3, 4]</span></span><br><span class="line"><span class="string">        - 生成样本对：(2,0), (2,1), (2,3), (2,4)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            pairs: 训练样本对列表 [(中心词, 上下文词), ...]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pairs = []</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 遍历每个用户的行为序列</span></span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> self.sequences:</span><br><span class="line">            <span class="comment"># 遍历序列中的每个物品作为中心词</span></span><br><span class="line">            <span class="keyword">for</span> i, center_word <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">                <span class="comment"># 确定上下文窗口的范围</span></span><br><span class="line">                <span class="comment"># start: 窗口起始位置（不能小于0）</span></span><br><span class="line">                <span class="comment"># end: 窗口结束位置（不能超过序列长度）</span></span><br><span class="line">                start = <span class="built_in">max</span>(<span class="number">0</span>, i - self.window_size)</span><br><span class="line">                end = <span class="built_in">min</span>(<span class="built_in">len</span>(seq), i + self.window_size + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 遍历窗口内的所有物品作为上下文词</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start, end):</span><br><span class="line">                    <span class="keyword">if</span> j != i:  <span class="comment"># 排除中心词本身</span></span><br><span class="line">                        context_word = seq[j]</span><br><span class="line">                        pairs.append((center_word, context_word))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> pairs</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_negative_sampling</span>(<span class="params">self, center_words, num_samples</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        负采样</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        对每个中心词，根据构建的分布随机采样 num_samples 个负样本</span></span><br><span class="line"><span class="string">        负样本是那些不在当前中心词上下文中的物品</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            center_words: numpy array，中心词 ID 列表</span></span><br><span class="line"><span class="string">            num_samples: 每个中心词对应的负样本数量</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            neg_samples: numpy array，形状 [batch_size, num_samples]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size = <span class="built_in">len</span>(center_words)</span><br><span class="line">        neg_samples = []</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 为每个负样本位置独立采样</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">            <span class="comment"># 从构建的分布中采样</span></span><br><span class="line">            <span class="comment"># p=self.word_dist 表示按照预先计算的概率分布采样</span></span><br><span class="line">            sampled = np.random.choice(</span><br><span class="line">                self.vocab_size,  <span class="comment"># 从所有物品中采样</span></span><br><span class="line">                size=batch_size,  <span class="comment"># 为 batch 中的每个样本采样一个</span></span><br><span class="line">                p=self.word_dist  <span class="comment"># 使用构建的概率分布</span></span><br><span class="line">            )</span><br><span class="line">            neg_samples.append(sampled)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 转置：从 [num_samples, batch_size] 转为 [batch_size, num_samples]</span></span><br><span class="line">        <span class="keyword">return</span> np.array(neg_samples).T</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, num_epochs=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        训练模型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        执行多个 epoch 的训练，每个 epoch 中：</span></span><br><span class="line"><span class="string">        1. 打乱训练样本（提高泛化能力）</span></span><br><span class="line"><span class="string">        2. 按 batch 训练</span></span><br><span class="line"><span class="string">        3. 进行负采样</span></span><br><span class="line"><span class="string">        4. 计算损失并更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            num_epochs: 训练轮数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            embeddings: 训练好的物品 Embedding</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model.train()  <span class="comment"># 设置为训练模式</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            num_batches = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 打乱训练样本</span></span><br><span class="line">            <span class="comment"># 每个 epoch 都打乱样本顺序可以提高模型的泛化能力</span></span><br><span class="line">            np.random.shuffle(self.train_pairs)</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 批量训练</span></span><br><span class="line">            <span class="comment"># 使用 tqdm 显示训练进度</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.train_pairs), self.batch_size), </span><br><span class="line">                         desc=<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{num_epochs}</span>"</span>):</span><br><span class="line">                <span class="comment"># 获取当前 batch 的样本</span></span><br><span class="line">                batch_pairs = self.train_pairs[i:i+self.batch_size]</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 提取中心词和上下文词</span></span><br><span class="line">                center_words = torch.LongTensor([p[<span class="number">0</span>] <span class="keyword">for</span> p <span class="keyword">in</span> batch_pairs])</span><br><span class="line">                context_words = torch.LongTensor([p[<span class="number">1</span>] <span class="keyword">for</span> p <span class="keyword">in</span> batch_pairs])</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 执行负采样</span></span><br><span class="line">                <span class="comment"># 为每个正样本对采样 num_negatives 个负样本</span></span><br><span class="line">                neg_words = self._negative_sampling(center_words.numpy(), self.num_negatives)</span><br><span class="line">                neg_words = torch.LongTensor(neg_words)</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                <span class="comment"># PyTorch 默认会累积梯度，需要手动清零</span></span><br><span class="line">                self.optimizer.zero_grad()</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 前向传播：计算损失</span></span><br><span class="line">                loss = self.model(center_words, context_words, neg_words)</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 反向传播：计算梯度</span></span><br><span class="line">                loss.backward()</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 更新参数</span></span><br><span class="line">                self.optimizer.step()</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 累积损失（用于打印）</span></span><br><span class="line">                total_loss += loss.item()</span><br><span class="line">                num_batches += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 计算并打印平均损失</span></span><br><span class="line">            avg_loss = total_loss / num_batches</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, Average Loss: <span class="subst">{avg_loss:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 返回训练好的 Embedding</span></span><br><span class="line">        <span class="keyword">return</span> self.model.get_embeddings()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_item2vec</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Item2Vec 使用示例</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    展示如何使用 Item2VecTrainer 训练模型并获取物品 Embedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 模拟用户行为序列</span></span><br><span class="line">    <span class="comment"># 每个序列代表一个用户按时间顺序的交互历史</span></span><br><span class="line">    <span class="comment"># 例如：用户1先点击了物品0，然后是物品1，接着是2，依此类推</span></span><br><span class="line">    sequences = [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],    <span class="comment"># 用户1的交互序列</span></span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],    <span class="comment"># 用户2的交互序列</span></span><br><span class="line">        [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span>],    <span class="comment"># 用户3的交互序列</span></span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">11</span>],  <span class="comment"># 用户4的交互序列</span></span><br><span class="line">    ]</span><br><span class="line">  </span><br><span class="line">    vocab_size = <span class="number">12</span>  <span class="comment"># 物品总数（物品ID从0到11）</span></span><br><span class="line">    embedding_dim = <span class="number">64</span>  <span class="comment"># Embedding 维度</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建训练器</span></span><br><span class="line">    trainer = Item2VecTrainer(</span><br><span class="line">        sequences=sequences,</span><br><span class="line">        vocab_size=vocab_size,</span><br><span class="line">        embedding_dim=embedding_dim,</span><br><span class="line">        window_size=<span class="number">2</span>,  <span class="comment"># 上下文窗口大小为2，即考虑前后各2个物品</span></span><br><span class="line">        num_negatives=<span class="number">5</span>,  <span class="comment"># 每个正样本对应5个负样本</span></span><br><span class="line">        batch_size=<span class="number">32</span>,  <span class="comment"># 批次大小</span></span><br><span class="line">        learning_rate=<span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    embeddings = trainer.train(num_epochs=<span class="number">5</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Embedding shape: <span class="subst">{embeddings.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Item 0 embedding (前5维): <span class="subst">{embeddings[<span class="number">0</span>][:<span class="number">5</span>]}</span>"</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 计算物品相似度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">a, b</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算两个向量的余弦相似度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        余弦相似度 = 向量内积 / (向量模长的乘积)</span></span><br><span class="line"><span class="string">        取值范围 [-1, 1]，越接近1表示越相似</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 查找与物品0最相似的物品</span></span><br><span class="line">    item_0_emb = embeddings[<span class="number">0</span>]</span><br><span class="line">    similarities = [cosine_similarity(item_0_emb, embeddings[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(vocab_size)]</span><br><span class="line">    top_k = np.argsort(similarities)[::-<span class="number">1</span>][:<span class="number">5</span>]  <span class="comment"># 取相似度最高的5个物品</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"与物品0最相似的5个物品: <span class="subst">{top_k}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="关键点解读">关键点解读</h4>
<p>Item2Vec 的实现包含几个关键组件，每个组件都有其特定的作用：</p>
<ol type="1">
<li><strong>序列构建与窗口采样</strong>：将用户的历史交互按时间排序形成物品序列，这是模型的基础数据。对于序列中的每个中心物品，我们提取其窗口内的其他物品作为上下文。<code>window_size=2</code>
表示考虑前后各2个物品，这样既能捕获局部共现关系，又不会引入过多噪声。窗口大小的选择需要在捕获共现关系和计算效率之间权衡：窗口越大，训练样本越多，但可能引入不相关的共现。</li>
<li><strong>负采样机制</strong>：负采样是 Item2Vec
训练效率的关键。每个正样本对（中心物品-上下文物品）需要对应多个负样本对（中心物品-随机物品）。<code>num_negatives=5</code>
表示每个正样本对应5个负样本，这个数量通常设置为5-20之间。负样本数量越多，模型对不相似物品的区分能力越强，但训练时间也线性增长。</li>
<li><strong>两套 Embedding 矩阵</strong>：中心物品和上下文物品使用独立的
Embedding 矩阵，这是 Word2Vec
论文中的设计。这种设计提供了更大的模型容量，让模型能够学习不同角色的表示。虽然增加了参数量，但实验表明这种设计能够提升模型效果。最终我们只使用中心物品
Embedding 作为物品的最终表示。</li>
<li><strong>损失函数设计</strong>：使用负对数似然损失，对正样本最大化
sigmoid(内积)，对负样本最小化
sigmoid(内积)。这种设计使得相似物品的内积增大，不相似物品的内积减小，从而学习到有意义的
Embedding。</li>
</ol>
<h4 id="设计权衡">设计权衡</h4>
<p>在 Item2Vec 的实现中，存在多个设计权衡：</p>
<ol type="1">
<li><strong>窗口大小 vs
计算效率</strong>：窗口越大，能够捕获更远的共现关系，但训练样本数量会急剧增加（O(n²)）。通常窗口大小设置为3-5，在效果和效率之间取得平衡。</li>
<li><strong>负采样数量 vs
训练时间</strong>：负采样数量越多，模型对负样本的区分能力越强，但每个
batch
的计算时间也线性增长。通常设置为5-20个，在效果和效率之间权衡。</li>
<li><strong>Embedding 维度 vs
表达能力</strong>：维度越高，模型的表达能力越强，但参数量也越大，且可能过拟合。通常设置为64-256维，根据物品数量和计算资源选择。</li>
<li><strong>频率分布 vs 均匀分布</strong>：负采样使用频率的 3/4
次幂分布，既避免高频词被过度采样，又给低频词学习机会。如果使用均匀分布，低频词会被过度采样；如果直接使用频率，高频词会被过度采样。</li>
</ol>
<h4 id="常见问题">常见问题</h4>
<ol type="1">
<li><strong>如何处理冷启动物品？</strong>
对于训练集中未出现的新物品，Item2Vec 无法直接学习其
Embedding。常见的解决方案包括：（1）使用物品的内容特征（如类别、标签）通过额外的神经网络生成初始
Embedding；（2）使用相似物品的 Embedding
作为初始化；（3）在训练时为新物品分配随机初始化的
Embedding，通过少量交互快速学习。</li>
<li><strong>数值溢出问题</strong>：内积计算可能产生很大的值（特别是
Embedding 未归一化时），导致 sigmoid 函数溢出。代码中使用
<code>torch.clamp</code> 将得分限制在 [-10, 10]
范围内，这是一个重要的数值稳定性技巧。</li>
<li><strong>负样本重复问题</strong>：随机负采样可能采样到正样本（即上下文窗口内的物品）。严格来说应该排除正样本，但在物品数量很大时（如百万级），采样到正样本的概率很小（通常&lt;0.1%），可以忽略。如果物品数量较小，可以在采样时排除正样本。</li>
<li><strong>序列长度不一致</strong>：实际应用中用户序列长度差异很大，从几个物品到数百个物品不等。常见的处理方式包括：（1）截断：只保留最近N个物品（如最近50个）；（2）填充：将短序列补齐到固定长度；（3）动态批处理：将相似长度的序列放在同一个
batch 中。</li>
<li><strong>如何处理时间信息？</strong> Item2Vec
忽略了序列中的时间信息，将窗口内的所有物品同等对待。如果需要考虑时间衰减，可以在损失函数中为不同位置的上下文物品赋予不同的权重，距离中心物品越远权重越小。</li>
</ol>
<h4 id="使用示例">使用示例</h4>
<p>下面的示例展示了如何使用 Item2Vec 训练模型并获取物品 Embedding：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备用户行为序列数据</span></span><br><span class="line">sequences = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],    <span class="comment"># 用户1的交互序列</span></span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],    <span class="comment"># 用户2的交互序列</span></span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span>],    <span class="comment"># 用户3的交互序列</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练器并训练模型</span></span><br><span class="line">trainer = Item2VecTrainer(</span><br><span class="line">    sequences=sequences,</span><br><span class="line">    vocab_size=<span class="number">12</span>,</span><br><span class="line">    embedding_dim=<span class="number">64</span>,</span><br><span class="line">    window_size=<span class="number">2</span>,</span><br><span class="line">    num_negatives=<span class="number">5</span>,</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    learning_rate=<span class="number">0.01</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型（通常需要5-20个epoch）</span></span><br><span class="line">embeddings = trainer.train(num_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Embedding 进行相似物品推荐</span></span><br><span class="line">item_emb = embeddings[<span class="number">0</span>]  <span class="comment"># 获取物品0的Embedding</span></span><br><span class="line">similarities = [cosine_similarity(item_emb, embeddings[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>)]</span><br><span class="line">top_k_items = np.argsort(similarities)[::-<span class="number">1</span>][:<span class="number">5</span>]  <span class="comment"># 找到最相似的5个物品</span></span><br></pre></td></tr></table></figure>
<p>在实际应用中，Item2Vec 通常作为特征提取器，学习到的 Embedding
可以用于：（1）相似物品推荐；（2）作为下游模型的输入特征；（3）物品聚类和可视化分析。</p>
<h3 id="item2vec-的优缺点">Item2Vec 的优缺点</h3>
<p><strong>优点</strong>：</p>
<ul>
<li>简单直观，易于理解和实现</li>
<li>不需要额外的特征信息，只需要用户行为序列</li>
<li>能够捕获物品之间的共现关系</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>忽略了用户信息，所有用户共享同一套物品 Embedding</li>
<li>对于新物品（冷启动）效果较差</li>
<li>序列中的位置信息利用不充分（只考虑窗口内的共现）</li>
</ul>
<h2 id="node2vec-图嵌入">Node2Vec 图嵌入</h2>
<h3 id="图嵌入的基本概念">图嵌入的基本概念</h3>
<p>在推荐系统中，用户和物品可以构成一个二部图（Bipartite
Graph）：用户和物品是两类节点，交互行为是边。图嵌入的目标是将图中的节点映射到低维向量空间，使得图中相似的节点（如连接相似、结构相似）在向量空间中距离更近。</p>
<h3 id="node2vec-算法原理">Node2Vec 算法原理</h3>
<p>Node2Vec 是 Grover 和 Leskovec 在 2016
年提出的图嵌入算法，它通过有偏随机游走（Biased Random
Walk）来生成节点的上下文，然后使用 Skip-gram 学习节点的 Embedding。</p>
<p><strong>有偏随机游走</strong>：Node2Vec
的关键创新在于定义了一种灵活的游走策略，通过参数 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container></span> 和 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 控制游走的偏向性：</p>
<ul>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container></span>（return
parameter）：控制回到上一个节点的概率</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span>（in-out
parameter）：控制走向远离起始节点的概率</li>
</ul>
<p>从节点 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewbox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g></g></g></svg></mjx-container></span> 游走到下一个节点 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g></svg></mjx-container></span> 的未归一化概率为：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -4.066ex;" xmlns="http://www.w3.org/2000/svg" width="26.036ex" height="9.263ex" role="img" focusable="false" viewbox="0 -2297.1 11508.1 4094.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"/></g><g data-mml-node="TeXAtom" transform="translate(673,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mi" transform="translate(503,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g><g data-mml-node="mo" transform="translate(1403.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1792.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2153.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2598.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(3170.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3837.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mrow" transform="translate(4893.2,0)"><g data-mml-node="mo"><path data-c="23A7" d="M712 899L718 893V876V865Q718 854 704 846Q627 793 577 710T510 525Q510 524 509 521Q505 493 504 349Q504 345 504 334Q504 277 504 240Q504 -2 503 -4Q502 -8 494 -9T444 -10Q392 -10 390 -9Q387 -8 386 -5Q384 5 384 230Q384 262 384 312T383 382Q383 481 392 535T434 656Q510 806 664 892L677 899H712Z" transform="translate(0,1398.1)"/><path data-c="23A9" d="M718 -893L712 -899H677L666 -893Q542 -825 468 -714T385 -476Q384 -466 384 -282Q384 3 385 5L389 9Q392 10 444 10Q486 10 494 9T503 4Q504 2 504 -239V-310V-366Q504 -470 508 -513T530 -609Q546 -657 569 -698T617 -767T661 -812T699 -843T717 -856T718 -876V-893Z" transform="translate(0,-898.1)"/><path data-c="23A8" d="M389 1159Q391 1160 455 1160Q496 1160 498 1159Q501 1158 502 1155Q504 1145 504 924Q504 691 503 682Q494 549 425 439T243 259L229 250L243 241Q349 175 421 66T503 -182Q504 -191 504 -424Q504 -600 504 -629T499 -659H498Q496 -660 444 -660T390 -659Q387 -658 386 -655Q384 -645 384 -425V-282Q384 -176 377 -116T342 10Q325 54 301 92T255 155T214 196T183 222T171 232Q170 233 170 250T171 268Q171 269 191 284T240 331T300 407T354 524T383 679Q384 691 384 925Q384 1152 385 1155L389 1159Z" transform="translate(0,0)"/><svg width="889" height="428.1" y="1060" x="0" viewbox="0 75.4 889 428.1"><path data-c="23AA" d="M384 150V266Q384 304 389 309Q391 310 455 310Q496 310 498 309Q502 308 503 298Q504 283 504 150Q504 32 504 12T499 -9H498Q496 -10 444 -10T390 -9Q386 -8 385 2Q384 17 384 150Z" transform="scale(1,2.105)"/></svg><svg width="889" height="428.1" y="-988.1" x="0" viewbox="0 75.4 889 428.1"><path data-c="23AA" d="M384 150V266Q384 304 389 309Q391 310 455 310Q496 310 498 309Q502 308 503 298Q504 283 504 150Q504 32 504 12T499 -9H498Q496 -10 444 -10T390 -9Q386 -8 385 2Q384 17 384 150Z" transform="scale(1,2.105)"/></svg></g><g data-mml-node="mtable" transform="translate(889,0)"><g data-mml-node="mtr" transform="translate(0,1432.2)"><g data-mml-node="mtd"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(221.1,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><rect width="555.7" height="60" x="120" y="220"/></g></g><g data-mml-node="mtd" transform="translate(1795.7,0)"><g data-mml-node="mtext"><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(278,0)"/><path data-c="A0" d="" transform="translate(584,0)"/></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g><g data-mml-node="mo" transform="translate(2374.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(3430.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g></g><g data-mml-node="mtr" transform="translate(0,0)"><g data-mml-node="mtd"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mtd" transform="translate(1795.7,0)"><g data-mml-node="mtext"><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(278,0)"/><path data-c="A0" d="" transform="translate(584,0)"/></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g><g data-mml-node="mo" transform="translate(2374.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(3430.3,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mtr" transform="translate(0,-1314.9)"><g data-mml-node="mtd"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mi" transform="translate(234.1,-345) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><rect width="553.6" height="60" x="120" y="220"/></g></g><g data-mml-node="mtd" transform="translate(1795.7,0)"><g data-mml-node="mtext"><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(278,0)"/><path data-c="A0" d="" transform="translate(584,0)"/></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g><g data-mml-node="mo" transform="translate(2374.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(3430.3,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g><g data-mml-node="mo" transform="translate(6615,0) translate(0 250)"/></g></g></g></svg></mjx-container></span></p>
<p>其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.857ex" height="1.927ex" role="img" focusable="false" viewbox="0 -694 1262.7 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g></g></g></svg></mjx-container></span> 是节点 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.817ex" height="1.441ex" role="img" focusable="false" viewbox="0 -626 361 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g></g></g></svg></mjx-container></span>（上一个节点）到节点 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g></svg></mjx-container></span> 的最短距离。</p>
<h3 id="node2vec-完整实现">Node2Vec 完整实现</h3>
<h4 id="问题背景-1">问题背景</h4>
<p>在推荐系统中，用户和物品之间的交互可以自然地建模为图结构：用户和物品是两类节点，交互行为是边。这种图结构蕴含着丰富的语义信息：连接相似的用户（如都购买了相同物品）可能具有相似的兴趣；结构相似的物品（如被相似用户购买）可能具有相似的性质。然而，如何从图结构中学习节点（用户和物品）的有效表示，使得相似节点在向量空间中距离更近，是一个挑战。传统的图嵌入方法（如
DeepWalk）使用标准的随机游走生成节点序列，但这种方法无法灵活地控制游走策略，难以同时捕获图的局部结构（如社区结构）和全局结构（如节点角色）。</p>
<h4 id="解决思路-1">解决思路</h4>
<p>Node2Vec 通过引入有偏随机游走（Biased Random
Walk）来解决这个问题。基本思路：通过两个参数 p 和 q
控制游走策略，使得游走过程能够在深度优先搜索（DFS）和广度优先搜索（BFS）之间灵活切换。具体而言，p（return
parameter）控制返回上一个节点的概率，q（in-out
parameter）控制探索远离起始节点的概率。当 p 较小、q 较大时，游走倾向于
BFS，能够捕获局部社区结构；当 p 较大、q 较小时，游走倾向于
DFS，能够捕获全局结构。通过这种灵活的游走策略，Node2Vec
能够生成多样化的节点序列，然后使用 Skip-gram 学习节点的
Embedding，使得结构相似或连接相似的节点在向量空间中距离更近。</p>
<h4 id="设计考虑-1">设计考虑</h4>
<p>在实现 Node2Vec 时，需要考虑以下几个关键设计：</p>
<ol type="1">
<li><strong>有偏游走策略</strong>：游走概率的计算需要考虑上一个节点、当前节点和候选节点的关系。对于每个候选节点，需要计算其与上一个节点的最短距离，然后根据距离赋予不同的权重。这种设计使得游走过程能够灵活地在局部探索和全局探索之间平衡。</li>
<li><strong>图预处理</strong>：为了加速游走生成，需要预先计算并存储每个节点的邻居信息。这样可以避免在训练时重复查询图结构，显著提高游走生成的速度。对于大规模图，这种预处理是必要的。</li>
<li><strong>游走序列生成</strong>：对每个节点执行多次游走（通常10-50次），每次游走生成长度固定的序列（通常50-100）。游走次数越多，能够捕获的图结构信息越丰富，但计算开销也越大。</li>
<li><strong>Skip-gram 训练</strong>：将生成的游走序列看作"句子"，使用
Skip-gram 学习节点 Embedding。这与 Item2Vec
的训练过程类似，但数据来源不同：Item2Vec 使用用户行为序列，Node2Vec
使用图上的游走序列。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node2Vec</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Node2Vec 图嵌入算法</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    核心创新：有偏随机游走（Biased Random Walk）</span></span><br><span class="line"><span class="string">    - 参数 p 控制返回上一个节点的概率（return parameter）</span></span><br><span class="line"><span class="string">    - 参数 q 控制探索远离起点的概率（in-out parameter）</span></span><br><span class="line"><span class="string">    - p=q=1 时退化为标准的随机游走</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, graph, dimensions=<span class="number">128</span>, walk_length=<span class="number">80</span>, num_walks=<span class="number">10</span>, </span></span><br><span class="line"><span class="params">                 p=<span class="number">1.0</span>, q=<span class="number">1.0</span>, window_size=<span class="number">10</span>, num_negatives=<span class="number">5</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化 Node2Vec</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            graph: NetworkX 图对象（可以是有向图或无向图）</span></span><br><span class="line"><span class="string">            dimensions: Embedding 维度</span></span><br><span class="line"><span class="string">            walk_length: 每次游走的长度（生成的节点序列长度）</span></span><br><span class="line"><span class="string">            num_walks: 每个节点作为起点的游走次数</span></span><br><span class="line"><span class="string">            p: return parameter，控制返回上一个节点的倾向</span></span><br><span class="line"><span class="string">               - p &lt; 1: 倾向于返回上一个节点（类似BFS，关注局部结构）</span></span><br><span class="line"><span class="string">               - p &gt; 1: 不倾向返回（类似DFS，关注全局结构）</span></span><br><span class="line"><span class="string">            q: in-out parameter，控制探索新节点的倾向</span></span><br><span class="line"><span class="string">               - q &lt; 1: 倾向于探索远离起点的节点（DFS）</span></span><br><span class="line"><span class="string">               - q &gt; 1: 倾向于探索起点附近的节点（BFS）</span></span><br><span class="line"><span class="string">            window_size: Skip-gram 的上下文窗口大小</span></span><br><span class="line"><span class="string">            num_negatives: 负采样数量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.graph = graph</span><br><span class="line">        self.dimensions = dimensions</span><br><span class="line">        self.walk_length = walk_length</span><br><span class="line">        self.num_walks = num_walks</span><br><span class="line">        self.p = p</span><br><span class="line">        self.q = q</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.num_negatives = num_negatives</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 预处理图：为每个节点存储邻居信息</span></span><br><span class="line">        <span class="comment"># 这样可以避免重复查询，提高游走速度</span></span><br><span class="line">        self._preprocess_graph()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 生成游走序列</span></span><br><span class="line">        <span class="comment"># 这是 Node2Vec 的核心：通过有偏游走捕获图结构</span></span><br><span class="line">        self.walks = self._generate_walks()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建训练样本</span></span><br><span class="line">        <span class="comment"># 将游走序列转换为（中心节点，上下文节点）对</span></span><br><span class="line">        self.train_pairs = self._generate_training_pairs()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建节点频率分布（用于负采样）</span></span><br><span class="line">        self.node_freq = self._build_node_freq()</span><br><span class="line">        self.node_dist = self._build_node_distribution()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_preprocess_graph</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        预处理图：为每个节点存储邻居信息</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        将邻居信息提前存储在字典中，避免训练时重复查询图结构，</span></span><br><span class="line"><span class="string">        显著提高游走生成的速度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.neighbors = {}</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.graph.nodes():</span><br><span class="line">            <span class="comment"># 获取节点的所有邻居并转为列表</span></span><br><span class="line">            self.neighbors[node] = <span class="built_in">list</span>(self.graph.neighbors(node))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_walks</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        生成有偏随机游走序列</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        对每个节点，执行 num_walks 次游走，每次游走长度为 walk_length</span></span><br><span class="line"><span class="string">        总共生成 num_nodes * num_walks 条游走序列</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            walks: 游走序列列表，每个序列是节点ID的列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        walks = []</span><br><span class="line">        nodes = <span class="built_in">list</span>(self.graph.nodes())</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 执行 num_walks 轮游走</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_walks):</span><br><span class="line">            <span class="comment"># 每轮都打乱节点顺序，增加随机性</span></span><br><span class="line">            random.shuffle(nodes)</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 从每个节点开始一次游走</span></span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">                walk = self._node2vec_walk(node)</span><br><span class="line">                walks.append(walk)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> walks</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_node2vec_walk</span>(<span class="params">self, start_node</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从起始节点开始的有偏随机游走</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        这是 Node2Vec 的核心算法：通过参数 p 和 q 控制游走的偏向性</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        游走策略：</span></span><br><span class="line"><span class="string">        1. 第一步：从起点随机选择一个邻居</span></span><br><span class="line"><span class="string">        2. 后续步骤：根据 p 和 q 参数计算每个邻居的权重，按权重采样</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        权重计算规则（设当前在节点 v，上一个节点是 t，候选下一个节点是 x）：</span></span><br><span class="line"><span class="string">        - 如果 x == t（返回上一个节点）：权重 = 1/p</span></span><br><span class="line"><span class="string">        - 如果 x 与 t 相邻（距离为1）：权重 = 1</span></span><br><span class="line"><span class="string">        - 如果 x 不与 t 相邻（距离为2）：权重 = 1/q</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            start_node: 起始节点ID</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            walk: 游走序列（节点ID列表）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        walk = [start_node]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 如果起点没有邻居，直接返回</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.neighbors[start_node]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> walk</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 第一步：随机选择一个邻居</span></span><br><span class="line">        <span class="comment"># 第一步没有"上一个节点"，所以只能随机选择</span></span><br><span class="line">        first_step = random.choice(self.neighbors[start_node])</span><br><span class="line">        walk.append(first_step)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 后续步骤：根据 p 和 q 进行有偏选择</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.walk_length - <span class="number">2</span>):</span><br><span class="line">            current = walk[-<span class="number">1</span>]  <span class="comment"># 当前节点</span></span><br><span class="line">            prev = walk[-<span class="number">2</span>]     <span class="comment"># 上一个节点</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 获取当前节点的邻居</span></span><br><span class="line">            neighbors = self.neighbors[current]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(neighbors) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span>  <span class="comment"># 如果没有邻居，游走结束</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 计算每个邻居的权重</span></span><br><span class="line">            weights = []</span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbors:</span><br><span class="line">                <span class="keyword">if</span> neighbor == prev:</span><br><span class="line">                    <span class="comment"># 返回上一个节点：权重 = 1/p</span></span><br><span class="line">                    weight = <span class="number">1.0</span> / self.p</span><br><span class="line">                <span class="keyword">elif</span> neighbor <span class="keyword">in</span> self.neighbors[prev]:</span><br><span class="line">                    <span class="comment"># 邻居与上一个节点相邻（距离为1）：权重 = 1</span></span><br><span class="line">                    weight = <span class="number">1.0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 邻居与上一个节点不相邻（距离为2）：权重 = 1/q</span></span><br><span class="line">                    weight = <span class="number">1.0</span> / self.q</span><br><span class="line">                weights.append(weight)</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 归一化权重为概率分布</span></span><br><span class="line">            weights = np.array(weights)</span><br><span class="line">            weights = weights / weights.<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 根据权重采样下一个节点</span></span><br><span class="line">            next_node = np.random.choice(neighbors, p=weights)</span><br><span class="line">            walk.append(next_node)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> walk</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_training_pairs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从游走序列生成训练样本对</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        与 Item2Vec 类似，将游走序列中的节点对转换为训练样本</span></span><br><span class="line"><span class="string">        每个节点作为中心节点，其窗口内的节点作为上下文节点</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pairs = []</span><br><span class="line">        <span class="keyword">for</span> walk <span class="keyword">in</span> self.walks:</span><br><span class="line">            <span class="keyword">for</span> i, center_node <span class="keyword">in</span> <span class="built_in">enumerate</span>(walk):</span><br><span class="line">                <span class="comment"># 确定上下文窗口</span></span><br><span class="line">                start = <span class="built_in">max</span>(<span class="number">0</span>, i - self.window_size)</span><br><span class="line">                end = <span class="built_in">min</span>(<span class="built_in">len</span>(walk), i + self.window_size + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start, end):</span><br><span class="line">                    <span class="keyword">if</span> j != i:</span><br><span class="line">                        context_node = walk[j]</span><br><span class="line">                        pairs.append((center_node, context_node))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> pairs</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_node_freq</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        统计节点频率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        遍历所有游走序列，统计每个节点出现的次数</span></span><br><span class="line"><span class="string">        用于构建负采样分布</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        node_freq = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> walk <span class="keyword">in</span> self.walks:</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> walk:</span><br><span class="line">                node_freq[node] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(node_freq)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_node_distribution</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构建负采样分布</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        同样使用 0.75 次幂的频率分布</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_nodes = <span class="built_in">len</span>(self.graph.nodes())</span><br><span class="line">        node_freq_array = np.array([</span><br><span class="line">            self.node_freq.get(i, <span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_nodes)</span><br><span class="line">        ])</span><br><span class="line">        <span class="comment"># 加上 1e-8 防止除零</span></span><br><span class="line">        node_dist = np.power(node_freq_array + <span class="number">1e-8</span>, <span class="number">0.75</span>)</span><br><span class="line">        node_dist = node_dist / node_dist.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">return</span> node_dist</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_negative_sampling</span>(<span class="params">self, center_nodes, num_samples</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        负采样：为每个中心节点采样负样本节点</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size = <span class="built_in">len</span>(center_nodes)</span><br><span class="line">        num_nodes = <span class="built_in">len</span>(self.graph.nodes())</span><br><span class="line">    </span><br><span class="line">        neg_samples = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">            sampled = np.random.choice(</span><br><span class="line">                num_nodes,</span><br><span class="line">                size=batch_size,</span><br><span class="line">                p=self.node_dist</span><br><span class="line">            )</span><br><span class="line">            neg_samples.append(sampled)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> np.array(neg_samples).T</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, batch_size=<span class="number">256</span>, num_epochs=<span class="number">10</span>, learning_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        训练 Node2Vec 模型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        创建节点到索引的映射，然后使用 Skip-gram 训练节点 Embedding</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_nodes = <span class="built_in">len</span>(self.graph.nodes())</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 创建节点到索引的映射</span></span><br><span class="line">        <span class="comment"># 因为神经网络的 Embedding 层需要连续的整数索引</span></span><br><span class="line">        self.node_to_idx = {node: idx <span class="keyword">for</span> idx, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.graph.nodes())}</span><br><span class="line">        self.idx_to_node = {idx: node <span class="keyword">for</span> node, idx <span class="keyword">in</span> self.node_to_idx.items()}</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 将节点对转换为索引对</span></span><br><span class="line">        train_pairs_idx = [</span><br><span class="line">            (self.node_to_idx[p[<span class="number">0</span>]], self.node_to_idx[p[<span class="number">1</span>]])</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> self.train_pairs</span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 初始化模型</span></span><br><span class="line">        model = Node2VecModel(num_nodes, self.dimensions)</span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">    </span><br><span class="line">        model.train()</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            num_batches = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 打乱训练样本</span></span><br><span class="line">            np.random.shuffle(train_pairs_idx)</span><br><span class="line">        </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(train_pairs_idx), batch_size),</span><br><span class="line">                         desc=<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{num_epochs}</span>"</span>):</span><br><span class="line">                batch_pairs = train_pairs_idx[i:i+batch_size]</span><br><span class="line">            </span><br><span class="line">                center_nodes = torch.LongTensor([p[<span class="number">0</span>] <span class="keyword">for</span> p <span class="keyword">in</span> batch_pairs])</span><br><span class="line">                context_nodes = torch.LongTensor([p[<span class="number">1</span>] <span class="keyword">for</span> p <span class="keyword">in</span> batch_pairs])</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 负采样</span></span><br><span class="line">                neg_nodes = self._negative_sampling(</span><br><span class="line">                    center_nodes.numpy(), </span><br><span class="line">                    self.num_negatives</span><br><span class="line">                )</span><br><span class="line">                neg_nodes = torch.LongTensor(neg_nodes)</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 前向传播</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                loss = model(center_nodes, context_nodes, neg_nodes)</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 反向传播</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line">            </span><br><span class="line">                total_loss += loss.item()</span><br><span class="line">                num_batches += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">            avg_loss = total_loss / num_batches</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, Average Loss: <span class="subst">{avg_loss:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 获取 Embedding</span></span><br><span class="line">        embeddings = model.get_embeddings()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建节点到 Embedding 的映射</span></span><br><span class="line">        self.node_embeddings = {</span><br><span class="line">            self.idx_to_node[i]: embeddings[i]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_nodes)</span><br><span class="line">        }</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> self.node_embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node2VecModel</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Node2Vec 的神经网络模型</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    与 Item2Vec 的结构完全相同，只是语义上是节点而非物品</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_nodes, embedding_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Node2VecModel, self).__init__()</span><br><span class="line">        self.num_nodes = num_nodes</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 中心节点和上下文节点的 Embedding</span></span><br><span class="line">        self.center_embeddings = nn.Embedding(num_nodes, embedding_dim)</span><br><span class="line">        self.context_embeddings = nn.Embedding(num_nodes, embedding_dim)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        self.center_embeddings.weight.data.uniform_(-<span class="number">0.5</span> / embedding_dim, <span class="number">0.5</span> / embedding_dim)</span><br><span class="line">        self.context_embeddings.weight.data.zero_()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, center_nodes, context_nodes, neg_nodes</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        前向传播：与 Item2Vec 完全相同的损失函数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        center_emb = self.center_embeddings(center_nodes)</span><br><span class="line">        context_emb = self.context_embeddings(context_nodes)</span><br><span class="line">        neg_emb = self.context_embeddings(neg_nodes)</span><br><span class="line">    </span><br><span class="line">        pos_score = torch.<span class="built_in">sum</span>(center_emb * context_emb, dim=<span class="number">1</span>)</span><br><span class="line">        pos_score = torch.clamp(pos_score, <span class="built_in">max</span>=<span class="number">10</span>, <span class="built_in">min</span>=-<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">        neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">        neg_score = torch.clamp(neg_score, <span class="built_in">max</span>=<span class="number">10</span>, <span class="built_in">min</span>=-<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">        pos_loss = -torch.log(torch.sigmoid(pos_score) + <span class="number">1e-10</span>)</span><br><span class="line">        neg_loss = -torch.log(torch.sigmoid(-neg_score) + <span class="number">1e-10</span>).<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> (pos_loss + neg_loss).mean()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_embeddings</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""获取节点 Embedding"""</span></span><br><span class="line">        <span class="keyword">return</span> self.center_embeddings.weight.data.cpu().numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_node2vec</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Node2Vec 使用示例</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    构建一个用户-物品二部图，然后学习节点的 Embedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 创建一个简单的二部图（用户-物品图）</span></span><br><span class="line">    G = nx.Graph()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 添加用户节点（0-4）</span></span><br><span class="line">    users = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">    <span class="comment"># 添加物品节点（10-19）</span></span><br><span class="line">    items = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">10</span>, <span class="number">20</span>))</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 添加边：用户-物品交互</span></span><br><span class="line">    <span class="comment"># 例如：用户0与物品10、11、12有交互</span></span><br><span class="line">    edges = [</span><br><span class="line">        (<span class="number">0</span>, <span class="number">10</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">0</span>, <span class="number">12</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">12</span>), (<span class="number">1</span>, <span class="number">13</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">12</span>), (<span class="number">2</span>, <span class="number">13</span>), (<span class="number">2</span>, <span class="number">14</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">13</span>), (<span class="number">3</span>, <span class="number">14</span>), (<span class="number">3</span>, <span class="number">15</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">14</span>), (<span class="number">4</span>, <span class="number">15</span>), (<span class="number">4</span>, <span class="number">16</span>),</span><br><span class="line">    ]</span><br><span class="line">  </span><br><span class="line">    G.add_edges_from(edges)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建 Node2Vec 实例</span></span><br><span class="line">    node2vec = Node2Vec(</span><br><span class="line">        graph=G,</span><br><span class="line">        dimensions=<span class="number">64</span>,  <span class="comment"># Embedding 维度</span></span><br><span class="line">        walk_length=<span class="number">20</span>,  <span class="comment"># 每次游走20步</span></span><br><span class="line">        num_walks=<span class="number">10</span>,  <span class="comment"># 每个节点作为起点游走10次</span></span><br><span class="line">        p=<span class="number">1.0</span>,  <span class="comment"># return parameter</span></span><br><span class="line">        q=<span class="number">1.0</span>,  <span class="comment"># in-out parameter</span></span><br><span class="line">        window_size=<span class="number">5</span>,  <span class="comment"># 上下文窗口大小</span></span><br><span class="line">        num_negatives=<span class="number">5</span>  <span class="comment"># 负采样数量</span></span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    embeddings = node2vec.train(</span><br><span class="line">        batch_size=<span class="number">128</span>,</span><br><span class="line">        num_epochs=<span class="number">10</span>,</span><br><span class="line">        learning_rate=<span class="number">0.01</span></span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Embedding for node 0: <span class="subst">{embeddings[<span class="number">0</span>][:<span class="number">5</span>]}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Embedding for node 10: <span class="subst">{embeddings[<span class="number">10</span>][:<span class="number">5</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="关键点解读-1">关键点解读</h4>
<p>Node2Vec 的实现包含几个关键组件，每个组件都有其特定的作用：</p>
<ol type="1">
<li><strong>有偏随机游走机制</strong>：这是 Node2Vec
的核心创新。游走概率的计算需要考虑上一个节点、当前节点和候选节点的关系。对于每个候选节点，根据其与上一个节点的最短距离赋予不同的权重：如果候选节点是上一个节点（距离为0），权重为
1/p；如果候选节点与上一个节点相邻（距离为1），权重为1；如果候选节点与上一个节点不相邻（距离为2），权重为
1/q。通过调整 p 和 q，可以控制游走策略：p
小（如0.5）时倾向返回上一个节点，类似 BFS，能够捕获局部社区结构；q
小（如0.5）时倾向探索远离起点的节点，类似 DFS，能够捕获全局结构。</li>
<li><strong>图预处理优化</strong>：为了加速游走生成，代码预先计算并存储每个节点的邻居信息。这样可以避免在训练时重复查询图结构，显著提高游走生成的速度。对于大规模图（百万级节点），这种预处理是必要的，可以将游走生成时间从数小时降低到数分钟。</li>
<li><strong>游走序列生成</strong>：对每个节点执行多次游走（<code>num_walks</code>，通常10-50次），每次游走生成长度固定的序列（<code>walk_length</code>，通常50-100）。游走次数越多，能够捕获的图结构信息越丰富，但计算开销也越大。通常
<code>num_walks * walk_length</code>
的乘积决定了生成的序列数量，需要在效果和效率之间权衡。</li>
<li><strong>Skip-gram 训练</strong>：将生成的游走序列看作"句子"，使用
Skip-gram 学习节点 Embedding。这与 Item2Vec
的训练过程类似，但数据来源不同：Item2Vec 使用用户行为序列，Node2Vec
使用图上的游走序列。通过最大化正样本对的相似度、最小化负样本对的相似度，模型能够学习到节点的向量表示。</li>
</ol>
<h4 id="设计权衡-1">设计权衡</h4>
<p>在 Node2Vec 的实现中，存在多个设计权衡：</p>
<ol type="1">
<li><strong>游走策略 vs
计算复杂度</strong>：有偏游走需要计算每个候选节点与上一个节点的最短距离，这在稠密图中计算开销较大。对于大规模图，可以使用近似方法（如只考虑2跳邻居）来加速计算。</li>
<li><strong>游走数量 vs 训练时间</strong>：<code>num_walks</code> 和
<code>walk_length</code> 的乘积决定了生成的序列数量。数量越多，Embedding
质量越好，但训练时间也线性增长。通常设置
<code>num_walks=10-50</code>，<code>walk_length=50-100</code>，在效果和效率之间取得平衡。</li>
<li><strong>p 和 q
参数选择</strong>：这两个参数控制游走策略，没有通用的最优值，需要根据具体任务调整。对于同质性强的图（如社交网络），倾向于使用较小的
p 和较大的 q；对于结构等价性强的图（如知识图谱），倾向于使用较大的 p
和较小的 q。建议从 p=1, q=1 开始，然后尝试不同的组合。</li>
<li><strong>窗口大小 vs 上下文范围</strong>：Skip-gram
的窗口大小决定了模型考虑多远的上下文。窗口越大，能够捕获更远的共现关系，但训练样本也会急剧增加。通常窗口大小设置为5-10，在捕获共现关系和计算效率之间权衡。</li>
</ol>
<h4 id="常见问题-1">常见问题</h4>
<ol type="1">
<li><strong>如何处理有向图？</strong> Node2Vec
默认支持无向图，对于有向图，需要修改游走策略：只考虑出边邻居，忽略入边邻居。这样可以保持游走的方向性，但可能丢失一些结构信息。</li>
<li><strong>如何处理加权图？</strong>
对于加权图，可以在游走概率计算中考虑边的权重：权重越大的边，被选择的概率越高。这样可以使得游走更倾向于沿着重要边进行，捕获更重要的结构信息。</li>
<li><strong>如何处理大规模图？</strong>
对于百万级甚至千万级节点的大规模图，完整的游走生成可能非常耗时。可以采用以下优化策略：（1）并行化游走生成，利用多核CPU或GPU加速；（2）使用采样方法，只对部分节点进行游走；（3）使用近似方法，如只考虑2跳邻居。</li>
<li><strong>如何选择 Embedding 维度？</strong> Embedding
维度影响模型的表达能力和计算开销。维度越高，模型的表达能力越强，但参数量也越大，且可能过拟合。通常设置为64-256维，根据节点数量和计算资源选择。对于大规模图，可以使用较小的维度（如64-128）以节省内存。</li>
<li><strong>如何处理新节点？</strong>
对于训练集中未出现的新节点，Node2Vec 无法直接学习其
Embedding。常见的解决方案包括：（1）使用节点的特征（如度、邻居特征）通过额外的神经网络生成初始
Embedding；（2）使用相似节点的 Embedding
作为初始化；（3）在训练时为新节点分配随机初始化的
Embedding，通过少量交互快速学习。</li>
</ol>
<h4 id="使用示例-1">使用示例</h4>
<p>下面的示例展示了如何使用 Node2Vec 训练模型并获取节点 Embedding：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建用户-物品二部图</span></span><br><span class="line">G = nx.Graph()</span><br><span class="line">edges = [(<span class="number">0</span>, <span class="number">10</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">12</span>), ...]  <span class="comment"># 用户-物品交互边</span></span><br><span class="line">G.add_edges_from(edges)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Node2Vec 实例并训练</span></span><br><span class="line">node2vec = Node2Vec(</span><br><span class="line">    graph=G,</span><br><span class="line">    dimensions=<span class="number">64</span>,</span><br><span class="line">    walk_length=<span class="number">20</span>,</span><br><span class="line">    num_walks=<span class="number">10</span>,</span><br><span class="line">    p=<span class="number">1.0</span>,  <span class="comment"># return parameter</span></span><br><span class="line">    q=<span class="number">1.0</span>,  <span class="comment"># in-out parameter</span></span><br><span class="line">    window_size=<span class="number">5</span>,</span><br><span class="line">    num_negatives=<span class="number">5</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">embeddings = node2vec.train(batch_size=<span class="number">128</span>, num_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Embedding 进行相似节点推荐</span></span><br><span class="line">user_emb = embeddings[<span class="number">0</span>]  <span class="comment"># 获取用户0的Embedding</span></span><br><span class="line">similarities = [cosine_similarity(user_emb, embeddings[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_nodes)]</span><br><span class="line">top_k_users = np.argsort(similarities)[::-<span class="number">1</span>][:<span class="number">5</span>]  <span class="comment"># 找到最相似的5个用户</span></span><br></pre></td></tr></table></figure>
<p>在实际应用中，Node2Vec 通常用于：（1）学习用户和物品的
Embedding，用于相似度计算和推荐；（2）作为下游模型的输入特征；（3）节点聚类和可视化分析；（4）图结构分析和社区发现。</p>
<ul>
<li><strong>节点索引映射</strong>：NetworkX 的节点 ID
可以是任意类型（字符串、整数等），但 PyTorch 的 Embedding
层需要连续的整数索引，所以需要建立映射。</li>
</ul>
<p><strong>可能的坑</strong>：</p>
<ol type="1">
<li><strong>内存占用</strong>：游走序列会占用大量内存。如果图很大，考虑分批生成游走或使用流式处理。</li>
<li><strong>孤立节点</strong>：如果图中有孤立节点（没有邻居），游走会立即结束。需要在构图时注意处理。</li>
<li><strong>有向图 vs 无向图</strong>：Node2Vec
可以用于有向图和无向图。推荐系统中的用户-物品图通常是无向图。</li>
</ol>
<h3 id="node2vec-参数调优">Node2Vec 参数调优</h3>
<p><strong>p 和 q 的选择</strong>：</p>
<ul>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container></span> 小、<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span>
小：倾向于深度优先搜索（DFS），捕获结构相似性</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g></g></svg></mjx-container></span> 大、<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewbox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span>
大：倾向于广度优先搜索（BFS），捕获同质性（连接相似性）</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="11.481ex" height="1.946ex" role="img" focusable="false" viewbox="0 -666 5074.8 860"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mo" transform="translate(780.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1836.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2336.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2781.2,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(3519,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(4574.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container></span>：标准的随机游走</li>
</ul>
<p><strong>walk_length 和 num_walks</strong>：</p>
<ul>
<li><code>walk_length</code>：每次游走的长度，通常设置为 40-100</li>
<li><code>num_walks</code>：每个节点开始的游走次数，通常设置为
10-20</li>
</ul>
<h2 id="two-tower-模型详解">Two-Tower 模型详解</h2>
<h3 id="two-tower-架构概述">Two-Tower 架构概述</h3>
<p>Two-Tower（双塔）模型是推荐系统中广泛使用的架构，它将用户和物品分别通过两个独立的"塔"（Tower）网络映射到同一个向量空间，然后通过向量相似度（如内积、余弦相似度）计算匹配分数。</p>
<p><strong>架构特点</strong>：</p>
<ul>
<li><strong>用户塔（User Tower）</strong>：输入用户特征，输出用户
Embedding</li>
<li><strong>物品塔（Item Tower）</strong>：输入物品特征，输出物品
Embedding</li>
<li><strong>相似度计算</strong>：用户 Embedding 和物品 Embedding
的内积或余弦相似度</li>
</ul>
<h3 id="two-tower-的数学形式">Two-Tower 的数学形式</h3>
<p>给定用户特征 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.476ex" height="1.361ex" role="img" focusable="false" viewbox="0 -444 1094.5 601.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="mi" transform="translate(640,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span>
和物品特征 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.113ex" height="1.361ex" role="img" focusable="false" viewbox="0 -444 934 601.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="mi" transform="translate(640,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span>，Two-Tower
模型计算：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="14.93ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 6599.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1292.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msub" transform="translate(2348,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(3325.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(3714.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="mi" transform="translate(640,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(4809,0)"><path data-c="3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"/></g><g data-mml-node="msub" transform="translate(5253.6,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(6210.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.477ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 5957 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1131.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msub" transform="translate(2187.5,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(3004.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(3393.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="mi" transform="translate(640,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(4327.4,0)"><path data-c="3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"/></g><g data-mml-node="msub" transform="translate(4772.1,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(5568,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span></p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.576ex;" xmlns="http://www.w3.org/2000/svg" width="13.357ex" height="2.594ex" role="img" focusable="false" viewbox="0 -891.7 5904 1146.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mo" transform="translate(469,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(858,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1430,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(1874.7,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(2219.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(2886.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msubsup" transform="translate(3942.2,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g><g data-mml-node="mi" transform="translate(560,-247) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="msub" transform="translate(5050,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41E" d="M32 225Q32 332 102 392T272 452H283Q382 452 436 401Q494 343 494 243Q494 226 486 222T440 217Q431 217 394 217T327 218H175V209Q175 177 179 154T196 107T236 69T306 50Q312 49 323 49Q376 49 410 85Q421 99 427 111T434 127T442 133T463 135H468Q494 135 494 117Q494 110 489 97T468 66T431 32T373 5T292 -6Q181 -6 107 55T32 225ZM383 276Q377 346 348 374T280 402Q253 402 230 390T195 357Q179 331 176 279V266H383V276Z"/></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span></p>
<p>其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="2.211ex" height="2.059ex" role="img" focusable="false" viewbox="0 -705 977.5 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span> 和 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="2.059ex" role="img" focusable="false" viewbox="0 -705 817 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span> 分别是用户塔和物品塔的网络，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.164ex" height="1.952ex" role="img" focusable="false" viewbox="0 -705 956.5 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span> 和 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.801ex" height="1.952ex" role="img" focusable="false" viewbox="0 -705 796 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container></span> 是各自的参数。</p>
<h3 id="two-tower-完整实现">Two-Tower 完整实现</h3>
<h4 id="问题背景-2">问题背景</h4>
<p>在推荐系统中，需要计算用户对物品的兴趣分数，以决定是否向用户推荐某个物品。传统的协同过滤方法（如矩阵分解）需要显式的用户-物品交互矩阵，但在实际应用中，我们往往有丰富的用户特征（如年龄、性别、历史行为）和物品特征（如类别、标签、内容描述）。如何利用这些特征来学习用户和物品的表示，并高效地计算匹配分数，是一个挑战。此外，在实际的推荐系统中，需要在毫秒级时间内为每个用户从百万甚至千万级物品库中检索出最相关的物品，这就要求模型能够支持高效的相似度检索。</p>
<h4 id="解决思路-2">解决思路</h4>
<p>Two-Tower（双塔）模型通过将用户和物品分别通过两个独立的神经网络（"塔"）映射到同一个低维向量空间，然后通过向量相似度（如内积、余弦相似度）计算匹配分数。这种设计的核心优势在于：（1）用户塔和物品塔可以独立设计，分别处理不同类型的特征；（2）训练完成后，可以预先计算所有物品的
Embedding 并建立索引，在推理时只需要计算用户
Embedding，然后通过高效的向量检索（如
FAISS）找到最相似的物品，大幅提升检索效率；（3）两个塔的参数可以独立更新，训练过程更加灵活。</p>
<h4 id="设计考虑-2">设计考虑</h4>
<p>在实现 Two-Tower 模型时，需要考虑以下几个关键设计：</p>
<ol type="1">
<li><strong>塔的网络结构</strong>：用户塔和物品塔通常采用多层全连接网络，每层包含线性变换、批归一化、激活函数和
Dropout。网络深度和宽度需要根据特征维度和数据规模调整：特征维度高、数据量大时可以使用更深的网络；特征维度低、数据量小时可以使用较浅的网络以避免过拟合。</li>
<li><strong>Embedding
归一化</strong>：为了使用余弦相似度，通常对用户和物品的 Embedding 进行
L2 归一化。归一化后的内积等价于余弦相似度，取值范围为 [-1,
1]。归一化还可以稳定训练过程，避免 Embedding 的模长过大。</li>
<li><strong>损失函数选择</strong>：Two-Tower
模型可以使用多种损失函数：（1）二元交叉熵损失（BCE），适用于点击率预估任务；（2）对比学习损失（如
InfoNCE），适用于学习通用表示；（3）排序损失（如
BPR），适用于学习相对排序。选择哪种损失函数取决于具体的任务目标。</li>
<li><strong>负采样策略</strong>：在训练时，需要为每个正样本采样负样本。负采样策略影响模型的学习效果：随机负采样简单但可能不够困难；困难负采样（hard
negative mining）能够提升模型对困难样本的区分能力，但计算开销更大。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UserTower</span>(nn.Module):</span><br><span class="line">    <span class="string">"""用户塔网络"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, user_feature_dim, embedding_dim, hidden_dims=[<span class="number">256</span>, <span class="number">128</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(UserTower, self).__init__()</span><br><span class="line">    </span><br><span class="line">        layers = []</span><br><span class="line">        input_dim = user_feature_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建隐藏层</span></span><br><span class="line">        <span class="keyword">for</span> hidden_dim <span class="keyword">in</span> hidden_dims:</span><br><span class="line">            layers.append(nn.Linear(input_dim, hidden_dim))</span><br><span class="line">            layers.append(nn.BatchNorm1d(hidden_dim))</span><br><span class="line">            layers.append(nn.ReLU())</span><br><span class="line">            layers.append(nn.Dropout(<span class="number">0.2</span>))</span><br><span class="line">            input_dim = hidden_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        layers.append(nn.Linear(input_dim, embedding_dim))</span><br><span class="line">    </span><br><span class="line">        self.network = nn.Sequential(*layers)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, user_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        user_features: [batch_size, user_feature_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.network(user_features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItemTower</span>(nn.Module):</span><br><span class="line">    <span class="string">"""物品塔网络"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, item_feature_dim, embedding_dim, hidden_dims=[<span class="number">256</span>, <span class="number">128</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(ItemTower, self).__init__()</span><br><span class="line">    </span><br><span class="line">        layers = []</span><br><span class="line">        input_dim = item_feature_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 构建隐藏层</span></span><br><span class="line">        <span class="keyword">for</span> hidden_dim <span class="keyword">in</span> hidden_dims:</span><br><span class="line">            layers.append(nn.Linear(input_dim, hidden_dim))</span><br><span class="line">            layers.append(nn.BatchNorm1d(hidden_dim))</span><br><span class="line">            layers.append(nn.ReLU())</span><br><span class="line">            layers.append(nn.Dropout(<span class="number">0.2</span>))</span><br><span class="line">            input_dim = hidden_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        layers.append(nn.Linear(input_dim, embedding_dim))</span><br><span class="line">    </span><br><span class="line">        self.network = nn.Sequential(*layers)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, item_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        item_features: [batch_size, item_feature_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.network(item_features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoTowerModel</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Two-Tower 模型"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, user_feature_dim, item_feature_dim, embedding_dim=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 user_hidden_dims=[<span class="number">256</span>, <span class="number">128</span>], item_hidden_dims=[<span class="number">256</span>, <span class="number">128</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoTowerModel, self).__init__()</span><br><span class="line">    </span><br><span class="line">        self.user_tower = UserTower(user_feature_dim, embedding_dim, user_hidden_dims)</span><br><span class="line">        self.item_tower = ItemTower(item_feature_dim, embedding_dim, item_hidden_dims)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># L2 归一化（可选，用于余弦相似度）</span></span><br><span class="line">        self.normalize = <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, user_features, item_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        user_features: [batch_size, user_feature_dim]</span></span><br><span class="line"><span class="string">        item_features: [batch_size, item_feature_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size] 相似度分数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        user_emb = self.user_tower(user_features)</span><br><span class="line">        item_emb = self.item_tower(item_features)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> self.normalize:</span><br><span class="line">            user_emb = F.normalize(user_emb, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">            item_emb = F.normalize(item_emb, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 计算内积（余弦相似度，如果已归一化）</span></span><br><span class="line">        score = torch.<span class="built_in">sum</span>(user_emb * item_emb, dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_user_embedding</span>(<span class="params">self, user_features</span>):</span><br><span class="line">        <span class="string">"""获取用户 Embedding"""</span></span><br><span class="line">        user_emb = self.user_tower(user_features)</span><br><span class="line">        <span class="keyword">if</span> self.normalize:</span><br><span class="line">            user_emb = F.normalize(user_emb, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> user_emb</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_item_embedding</span>(<span class="params">self, item_features</span>):</span><br><span class="line">        <span class="string">"""获取物品 Embedding"""</span></span><br><span class="line">        item_emb = self.item_tower(item_features)</span><br><span class="line">        <span class="keyword">if</span> self.normalize:</span><br><span class="line">            item_emb = F.normalize(item_emb, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> item_emb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RecommendationDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""推荐系统数据集"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, user_features, item_features, labels</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        user_features: [num_samples, user_feature_dim]</span></span><br><span class="line"><span class="string">        item_features: [num_samples, item_feature_dim]</span></span><br><span class="line"><span class="string">        labels: [num_samples] 0 或 1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.user_features = torch.FloatTensor(user_features)</span><br><span class="line">        self.item_features = torch.FloatTensor(item_features)</span><br><span class="line">        self.labels = torch.FloatTensor(labels)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">'user_features'</span>: self.user_features[idx],</span><br><span class="line">            <span class="string">'item_features'</span>: self.item_features[idx],</span><br><span class="line">            <span class="string">'label'</span>: self.labels[idx]</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_two_tower</span>(<span class="params">model, train_loader, num_epochs=<span class="number">10</span>, learning_rate=<span class="number">0.001</span>, device=<span class="string">'cpu'</span></span>):</span><br><span class="line">    <span class="string">"""训练 Two-Tower 模型"""</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">  </span><br><span class="line">    model.train()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        num_batches = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            user_features = batch[<span class="string">'user_features'</span>].to(device)</span><br><span class="line">            item_features = batch[<span class="string">'item_features'</span>].to(device)</span><br><span class="line">            labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 前向传播</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            scores = model(user_features, item_features)</span><br><span class="line">            loss = criterion(scores, labels)</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        </span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            num_batches += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">        avg_loss = total_loss / num_batches</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{num_epochs}</span>, Average Loss: <span class="subst">{avg_loss:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_two_tower</span>():</span><br><span class="line">    <span class="string">"""Two-Tower 模型使用示例"""</span></span><br><span class="line">    <span class="comment"># 模拟数据</span></span><br><span class="line">    num_samples = <span class="number">10000</span></span><br><span class="line">    user_feature_dim = <span class="number">50</span></span><br><span class="line">    item_feature_dim = <span class="number">40</span></span><br><span class="line">    embedding_dim = <span class="number">128</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 生成随机特征</span></span><br><span class="line">    user_features = np.random.randn(num_samples, user_feature_dim)</span><br><span class="line">    item_features = np.random.randn(num_samples, item_feature_dim)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 生成标签（简单的相似度规则）</span></span><br><span class="line">    labels = (np.random.rand(num_samples) &gt; <span class="number">0.5</span>).astype(<span class="built_in">float</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建数据集</span></span><br><span class="line">    dataset = RecommendationDataset(user_features, item_features, labels)</span><br><span class="line">    train_loader = DataLoader(dataset, batch_size=<span class="number">256</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = TwoTowerModel(</span><br><span class="line">        user_feature_dim=user_feature_dim,</span><br><span class="line">        item_feature_dim=item_feature_dim,</span><br><span class="line">        embedding_dim=embedding_dim,</span><br><span class="line">        user_hidden_dims=[<span class="number">256</span>, <span class="number">128</span>],</span><br><span class="line">        item_hidden_dims=[<span class="number">256</span>, <span class="number">128</span>]</span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    train_two_tower(model, train_loader, num_epochs=<span class="number">10</span>, learning_rate=<span class="number">0.001</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 获取 Embedding</span></span><br><span class="line">    sample_user_features = torch.FloatTensor(user_features[:<span class="number">10</span>])</span><br><span class="line">    sample_item_features = torch.FloatTensor(item_features[:<span class="number">10</span>])</span><br><span class="line">  </span><br><span class="line">    user_embeddings = model.get_user_embedding(sample_user_features)</span><br><span class="line">    item_embeddings = model.get_item_embedding(sample_item_features)</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"User embeddings shape: <span class="subst">{user_embeddings.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Item embeddings shape: <span class="subst">{item_embeddings.shape}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="关键点解读-2">关键点解读</h4>
<p>Two-Tower 模型的实现包含几个关键组件，每个组件都有其特定的作用：</p>
<ol type="1">
<li><strong>双塔架构设计</strong>：用户塔和物品塔是两个独立的神经网络，分别处理用户特征和物品特征。这种设计使得两个塔可以独立设计，分别处理不同类型的特征。例如，用户塔可以处理用户画像、历史行为等特征，物品塔可以处理物品属性、内容特征等。两个塔的输出维度必须相同，这样才能计算相似度。</li>
<li><strong>Embedding 归一化</strong>：代码中对用户和物品的 Embedding
进行 L2 归一化，这使得内积等价于余弦相似度，取值范围为 [-1,
1]。归一化还可以稳定训练过程，避免 Embedding
的模长过大导致梯度爆炸。在实际应用中，归一化是 Two-Tower
模型的标准做法。</li>
<li><strong>网络结构设计</strong>：每个塔采用多层全连接网络，每层包含线性变换、批归一化、ReLU
激活和 Dropout。批归一化可以加速训练并稳定梯度，Dropout
可以防止过拟合。网络深度和宽度需要根据特征维度和数据规模调整：特征维度高、数据量大时可以使用更深的网络；特征维度低、数据量小时可以使用较浅的网络。</li>
<li><strong>损失函数选择</strong>：代码中使用二元交叉熵损失（BCEWithLogitsLoss），适用于点击率预估任务。对于其他任务，可以使用不同的损失函数：对比学习任务可以使用
InfoNCE 损失，排序任务可以使用 BPR 损失。</li>
</ol>
<h4 id="设计权衡-2">设计权衡</h4>
<p>在 Two-Tower 模型的实现中，存在多个设计权衡：</p>
<ol type="1">
<li><strong>网络深度 vs
计算效率</strong>：网络越深，模型的表达能力越强，但计算开销也越大。在实际应用中，通常使用2-4层全连接网络，在效果和效率之间取得平衡。对于大规模推荐系统，计算效率至关重要，因为需要在毫秒级时间内完成推理。</li>
<li><strong>Embedding 维度 vs 表达能力</strong>：Embedding
维度越高，模型的表达能力越强，但参数量也越大，且检索效率会降低（向量检索的时间复杂度与维度相关）。通常
Embedding 维度设置为64-256维，根据物品数量和计算资源选择。</li>
<li><strong>归一化 vs 灵活性</strong>：L2
归一化使得内积等价于余弦相似度，简化了相似度计算，但限制了模型的灵活性。如果不需要归一化，可以移除归一化操作，使用原始的内积或点积。</li>
<li><strong>特征工程 vs 端到端学习</strong>：Two-Tower
模型需要手工设计特征，特征质量直接影响模型效果。相比之下，端到端的深度学习方法（如深度协同过滤）可以自动学习特征表示，但需要更多的数据和计算资源。</li>
</ol>
<h4 id="常见问题-2">常见问题</h4>
<ol type="1">
<li><strong>如何处理稀疏特征？</strong> 对于稀疏特征（如用户 ID、物品
ID），可以使用 Embedding 层将其映射到稠密向量。Embedding
层的维度通常设置为16-64维，根据特征数量和数据规模调整。</li>
<li><strong>如何处理多模态特征？</strong>
对于多模态特征（如文本、图像），可以使用不同的编码器：文本特征可以使用
CNN 或 Transformer，图像特征可以使用
CNN。然后将不同模态的特征拼接或融合，输入到塔网络中。</li>
<li><strong>如何提升检索效率？</strong> Two-Tower
模型的核心优势在于支持高效的向量检索。训练完成后，可以预先计算所有物品的
Embedding 并建立索引（如 FAISS），在推理时只需要计算用户
Embedding，然后通过向量检索找到最相似的物品。这种方法可以将检索时间从秒级降低到毫秒级。</li>
<li><strong>如何处理冷启动问题？</strong>
对于新用户或新物品，可以使用内容特征（如用户画像、物品属性）通过塔网络生成初始
Embedding。如果内容特征不足，可以使用相似用户或相似物品的 Embedding
作为初始化。</li>
<li><strong>如何选择负采样策略？</strong>
负采样策略影响模型的学习效果。随机负采样简单但可能不够困难；困难负采样（hard
negative
mining）能够提升模型对困难样本的区分能力，但计算开销更大。通常采用混合策略：大部分使用随机负采样，小部分使用困难负采样。</li>
</ol>
<h4 id="使用示例-2">使用示例</h4>
<p>下面的示例展示了如何使用 Two-Tower 模型进行训练和推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">user_features = ...  <span class="comment"># [num_samples, user_feature_dim]</span></span><br><span class="line">item_features = ...  <span class="comment"># [num_samples, item_feature_dim]</span></span><br><span class="line">labels = ...  <span class="comment"># [num_samples] 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集和数据加载器</span></span><br><span class="line">dataset = RecommendationDataset(user_features, item_features, labels)</span><br><span class="line">train_loader = DataLoader(dataset, batch_size=<span class="number">256</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = TwoTowerModel(</span><br><span class="line">    user_feature_dim=<span class="number">50</span>,</span><br><span class="line">    item_feature_dim=<span class="number">40</span>,</span><br><span class="line">    embedding_dim=<span class="number">128</span>,</span><br><span class="line">    user_hidden_dims=[<span class="number">256</span>, <span class="number">128</span>],</span><br><span class="line">    item_hidden_dims=[<span class="number">256</span>, <span class="number">128</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">train_two_tower(model, train_loader, num_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理：计算用户对物品的兴趣分数</span></span><br><span class="line">user_emb = model.get_user_embedding(user_features)</span><br><span class="line">item_emb = model.get_item_embedding(item_features)</span><br><span class="line">scores = torch.<span class="built_in">sum</span>(user_emb * item_emb, dim=<span class="number">1</span>)  <span class="comment"># 内积（余弦相似度）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索：找到与用户最相似的物品（使用 FAISS）</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">item_emb_np = item_emb.cpu().numpy()</span><br><span class="line">index = faiss.IndexFlatIP(<span class="number">128</span>)  <span class="comment"># 内积索引</span></span><br><span class="line">index.add(item_emb_np)</span><br><span class="line">user_emb_np = user_emb[<span class="number">0</span>:<span class="number">1</span>].cpu().numpy()</span><br><span class="line">distances, indices = index.search(user_emb_np, k=<span class="number">10</span>)  <span class="comment"># 找到最相似的10个物品</span></span><br></pre></td></tr></table></figure>
<p>在实际应用中，Two-Tower
模型通常用于：（1）大规模推荐系统的召回阶段，通过向量检索快速找到候选物品；（2）广告推荐，通过用户和广告的
Embedding 计算匹配分数；（3）搜索推荐，通过查询和物品的 Embedding
进行语义匹配。</p>
<h2 id="dssm-深度语义匹配">DSSM 深度语义匹配</h2>
<h3 id="dssm-算法原理">DSSM 算法原理</h3>
<p>DSSM（Deep Structured Semantic Models）是微软在 2013
年提出的深度语义匹配模型，最初用于信息检索中的查询-文档匹配。在推荐系统中，DSSM
可以用于用户查询与物品的语义匹配。</p>
<p>DSSM
的基本思路：<strong>将查询和文档分别通过深度神经网络映射到同一个语义空间，然后计算它们的相似度</strong>。</p>
<h3 id="dssm-网络结构">DSSM 网络结构</h3>
<p>DSSM 的网络结构包括：</p>
<ol type="1">
<li><strong>Term Hashing</strong>：将文本转换为 n-gram 特征</li>
<li><strong>Word Hashing</strong>：将 n-gram 特征映射到低维向量</li>
<li><strong>多层全连接网络</strong>：学习语义表示</li>
<li><strong>相似度计算</strong>：使用余弦相似度</li>
</ol>
<h3 id="dssm-完整实现">DSSM 完整实现</h3>
<h4 id="问题背景-3">问题背景</h4>
<p>在信息检索和推荐系统中，如何计算查询（query）和文档（document）之间的语义相似度是一个核心问题。传统的基于关键词匹配的方法（如
TF-IDF、BM25）只能捕获字面匹配，无法理解语义相似性。例如，查询"如何学习机器学习"和文档"机器学习入门教程"在语义上高度相关，但关键词重叠度很低。此外，在实际应用中，查询和文档往往以文本形式存在，如何将文本转换为数值特征，并通过深度神经网络学习语义表示，是一个挑战。</p>
<h4 id="解决思路-3">解决思路</h4>
<p>DSSM（Deep Structured Semantic
Models）通过将查询和文档分别通过深度神经网络映射到同一个低维语义空间，然后计算它们的余弦相似度。核心创新在于：（1）使用
Word
Hashing（词哈希）技术将文本转换为固定维度的特征向量，避免了传统词袋模型的高维稀疏问题；（2）使用多层全连接网络学习语义表示，能够捕获文本的深层语义信息；（3）通过最大化相关查询-文档对的相似度、最小化不相关对的相似度来训练模型，使得语义相似的查询和文档在向量空间中距离更近。</p>
<h4 id="设计考虑-3">设计考虑</h4>
<p>在实现 DSSM 时，需要考虑以下几个关键设计：</p>
<ol type="1">
<li><strong>Word Hashing 技术</strong>：DSSM 使用 n-gram（通常
n=3）将文本转换为特征向量。对于每个
n-gram，使用哈希函数将其映射到固定大小的特征空间（通常50000维）。这种方法避免了传统词袋模型的高维稀疏问题，同时能够处理未登录词（OOV）问题。</li>
<li><strong>网络结构设计</strong>：DSSM 使用多层全连接网络，每层使用
Tanh 激活函数。网络深度通常为3-4层，每层维度逐渐减小（如
300→300→128）。最后一层输出固定维度的 Embedding（通常128维），并进行 L2
归一化。</li>
<li><strong>双塔架构</strong>：查询塔和文档塔是两个独立的网络，但结构相同。这种设计使得查询和文档可以有不同的输入特征维度，但输出维度必须相同才能计算相似度。</li>
<li><strong>损失函数选择</strong>：DSSM
通常使用对比学习损失或排序损失。对于每个查询，选择相关的文档作为正样本，不相关的文档作为负样本，最大化正样本的相似度、最小化负样本的相似度。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WordHashing</span>:</span><br><span class="line">    <span class="string">"""Word Hashing：将文本转换为 n-gram 特征"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n=<span class="number">3</span></span>):</span><br><span class="line">        self.n = n  <span class="comment"># n-gram 大小</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ngram_hash</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">"""将文本转换为 n-gram 哈希特征"""</span></span><br><span class="line">        text = text.lower()</span><br><span class="line">        ngrams = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(text) - self.n + <span class="number">1</span>):</span><br><span class="line">            ngram = text[i:i+self.n]</span><br><span class="line">            ngrams.append(ngram)</span><br><span class="line">        <span class="keyword">return</span> ngrams</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">text_to_features</span>(<span class="params">self, text, vocab_size=<span class="number">50000</span></span>):</span><br><span class="line">        <span class="string">"""将文本转换为特征向量（使用简单的哈希）"""</span></span><br><span class="line">        ngrams = self.ngram_hash(text)</span><br><span class="line">        <span class="comment"># 使用简单的哈希函数映射到固定大小的特征空间</span></span><br><span class="line">        features = np.zeros(vocab_size)</span><br><span class="line">        <span class="keyword">for</span> ngram <span class="keyword">in</span> ngrams:</span><br><span class="line">            idx = <span class="built_in">hash</span>(ngram) % vocab_size</span><br><span class="line">            features[idx] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DSSM</span>(nn.Module):</span><br><span class="line">    <span class="string">"""DSSM 深度语义匹配模型"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>], embedding_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DSSM, self).__init__()</span><br><span class="line"></span><br><span class="line">    layers = []</span><br><span class="line">        current_dim = input_dim</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建隐藏层</span></span><br><span class="line">        <span class="keyword">for</span> hidden_dim <span class="keyword">in</span> hidden_dims:</span><br><span class="line">            layers.append(nn.Linear(current_dim, hidden_dim))</span><br><span class="line">            layers.append(nn.Tanh())</span><br><span class="line">            current_dim = hidden_dim</span><br><span class="line"></span><br><span class="line">    self.network = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层</span></span><br><span class="line">        self.output_layer = nn.Linear(current_dim, embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        x: [batch_size, input_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        hidden = self.network(x)</span><br><span class="line">        output = self.output_layer(hidden)</span><br><span class="line">        <span class="comment"># L2 归一化</span></span><br><span class="line">        output = F.normalize(output, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DSSMModel</span>(nn.Module):</span><br><span class="line">    <span class="string">"""DSSM 双塔模型（查询塔和文档塔）"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, query_input_dim, doc_input_dim,</span></span><br><span class="line"><span class="params">                 query_hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>],</span></span><br><span class="line"><span class="params">                 doc_hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>],</span></span><br><span class="line"><span class="params">                 embedding_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DSSMModel, self).__init__()</span><br><span class="line"></span><br><span class="line">    self.query_tower = DSSM(query_input_dim, query_hidden_dims, embedding_dim)</span><br><span class="line">        self.doc_tower = DSSM(doc_input_dim, doc_hidden_dims, embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query_features, doc_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        query_features: [batch_size, query_input_dim]</span></span><br><span class="line"><span class="string">        doc_features: [batch_size, doc_input_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size] 相似度分数（余弦相似度）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        query_emb = self.query_tower(query_features)</span><br><span class="line">        doc_emb = self.doc_tower(doc_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算余弦相似度（已经归一化，所以是内积）</span></span><br><span class="line">        similarity = torch.<span class="built_in">sum</span>(query_emb * doc_emb, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> similarity</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_query_embedding</span>(<span class="params">self, query_features</span>):</span><br><span class="line">        <span class="string">"""获取查询 Embedding"""</span></span><br><span class="line">        <span class="keyword">return</span> self.query_tower(query_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_doc_embedding</span>(<span class="params">self, doc_features</span>):</span><br><span class="line">        <span class="string">"""获取文档 Embedding"""</span></span><br><span class="line">        <span class="keyword">return</span> self.doc_tower(doc_features)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_dssm</span>(<span class="params">model, train_loader, num_epochs=<span class="number">10</span>, learning_rate=<span class="number">0.001</span>, device=<span class="string">'cpu'</span></span>):</span><br><span class="line">    <span class="string">"""训练 DSSM 模型"""</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        num_batches = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            query_features = batch[<span class="string">'query_features'</span>].to(device)</span><br><span class="line">            doc_features = batch[<span class="string">'doc_features'</span>].to(device)</span><br><span class="line">            labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">            scores = model(query_features, doc_features)</span><br><span class="line">            loss = criterion(scores, labels)</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">    total_loss += loss.item()</span><br><span class="line">            num_batches += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    avg_loss = total_loss / num_batches</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{num_epochs}</span>, Average Loss: <span class="subst">{avg_loss:<span class="number">.4</span>f}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="使用示例-3">使用示例</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_dssm</span>():</span><br><span class="line">    <span class="string">"""DSSM 使用示例"""</span></span><br><span class="line">    <span class="comment"># 模拟查询和文档特征</span></span><br><span class="line">    num_samples = <span class="number">10000</span></span><br><span class="line">    query_input_dim = <span class="number">50000</span>  <span class="comment"># Word Hashing 后的特征维度</span></span><br><span class="line">    doc_input_dim = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成随机特征（模拟 Word Hashing 的输出）</span></span><br><span class="line">    query_features = np.random.rand(num_samples, query_input_dim)</span><br><span class="line">    doc_features = np.random.rand(num_samples, doc_input_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化（模拟词频特征）</span></span><br><span class="line">    query_features = query_features / (query_features.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) + <span class="number">1e-8</span>)</span><br><span class="line">    doc_features = doc_features / (doc_features.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) + <span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成标签</span></span><br><span class="line">    labels = (np.random.rand(num_samples) &gt; <span class="number">0.5</span>).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据集</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">DSSMDataset</span>:</span><br><span class="line">        def__init__(self, query_features, doc_features, labels):</span><br><span class="line">            self.query_features = torch.FloatTensor(query_features)</span><br><span class="line">            self.doc_features = torch.FloatTensor(doc_features)</span><br><span class="line">            self.labels = torch.FloatTensor(labels)</span><br><span class="line"></span><br><span class="line">    def__len__(self):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line"></span><br><span class="line">    def__getitem__(self, idx):</span><br><span class="line">            <span class="keyword">return</span> {</span><br><span class="line">                <span class="string">'query_features'</span>: self.query_features[idx],</span><br><span class="line">                <span class="string">'doc_features'</span>: self.doc_features[idx],</span><br><span class="line">                <span class="string">'label'</span>: self.labels[idx]</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">    dataset = DSSMDataset(query_features, doc_features, labels)</span><br><span class="line">    train_loader = DataLoader(dataset, batch_size=<span class="number">256</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = DSSMModel(</span><br><span class="line">        query_input_dim=query_input_dim,</span><br><span class="line">        doc_input_dim=doc_input_dim,</span><br><span class="line">        query_hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>],</span><br><span class="line">        doc_hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>],</span><br><span class="line">        embedding_dim=<span class="number">128</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    train_dssm(model, train_loader, num_epochs=<span class="number">10</span>, learning_rate=<span class="number">0.001</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="关键点解读-3">关键点解读</h4>
<p>DSSM 的实现包含几个关键组件，每个组件都有其特定的作用：</p>
<ol type="1">
<li><strong>Word Hashing 技术</strong>：这是 DSSM
的核心创新之一。传统的词袋模型需要维护一个巨大的词汇表，对于新词或未登录词（OOV）无法处理。Word
Hashing 通过 n-gram（通常
n=3）将文本转换为固定维度的特征向量，使用哈希函数将每个 n-gram
映射到固定大小的特征空间（通常50000维）。这种方法避免了高维稀疏问题，同时能够处理任意文本，包括未登录词。</li>
<li><strong>多层全连接网络</strong>：DSSM
使用多层全连接网络学习语义表示，每层使用 Tanh
激活函数。网络深度通常为3-4层，每层维度逐渐减小（如
300→300→128）。这种设计使得模型能够学习文本的深层语义信息，将高维稀疏的
n-gram 特征映射到低维稠密的语义空间。</li>
<li><strong>L2 归一化</strong>：代码中对输出的 Embedding 进行 L2
归一化，这使得内积等价于余弦相似度，取值范围为 [-1,
1]。归一化还可以稳定训练过程，避免 Embedding 的模长过大。</li>
<li><strong>双塔架构</strong>：查询塔和文档塔是两个独立的网络，但结构相同。这种设计使得查询和文档可以有不同的输入特征维度，但输出维度必须相同才能计算相似度。训练时，通过最大化相关查询-文档对的相似度、最小化不相关对的相似度来学习参数。</li>
</ol>
<h4 id="设计权衡-3">设计权衡</h4>
<p>在 DSSM 的实现中，存在多个设计权衡：</p>
<ol type="1">
<li><strong>Word Hashing vs 词嵌入</strong>：Word Hashing
避免了词汇表问题，但可能引入哈希冲突。现代方法通常使用预训练的词嵌入（如
Word2Vec、BERT），能够更好地捕获语义信息，但需要维护词汇表。</li>
<li><strong>全连接网络 vs CNN/RNN</strong>：DSSM
使用全连接网络，简单但可能无法很好地捕获序列信息。CDSSM 使用 CNN
能够捕获局部特征和序列信息，但计算开销更大。对于长文本，可以使用 RNN 或
Transformer 更好地捕获序列依赖。</li>
<li><strong>特征维度 vs 计算效率</strong>：Word Hashing
的特征维度（通常50000维）很大，但可以通过稀疏表示和哈希技巧加速计算。现代方法通常使用预训练的词嵌入（通常300-768维），维度更小但需要维护词汇表。</li>
<li><strong>损失函数选择</strong>：DSSM
可以使用多种损失函数：二元交叉熵损失适用于点击率预估，对比学习损失适用于学习通用表示，排序损失适用于学习相对排序。选择哪种损失函数取决于具体的任务目标。</li>
</ol>
<h4 id="常见问题-3">常见问题</h4>
<ol type="1">
<li><strong>如何处理哈希冲突？</strong> Word Hashing
可能引入哈希冲突，即不同的 n-gram
映射到同一个特征位置。虽然冲突概率较低（对于50000维特征空间），但可能影响模型效果。可以使用更大的特征空间或使用多个哈希函数来减少冲突。</li>
<li><strong>如何处理长文本？</strong> DSSM 的 Word Hashing 对所有 n-gram
一视同仁，可能无法很好地处理长文本。对于长文本，可以：（1）截断或采样部分
n-gram；（2）使用 CNN 或 RNN
捕获序列信息；（3）使用注意力机制关注重要部分。</li>
<li><strong>如何提升检索效率？</strong> 与 Two-Tower 模型类似，DSSM
也支持高效的向量检索。训练完成后，可以预先计算所有文档的 Embedding
并建立索引（如 FAISS），在推理时只需要计算查询
Embedding，然后通过向量检索找到最相似的文档。</li>
<li><strong>如何处理多语言？</strong> Word Hashing
对于不同语言可能需要不同的 n-gram 大小。对于中文，通常使用字符级别的
n-gram；对于英文，通常使用词级别的
n-gram。现代方法通常使用多语言预训练模型（如
mBERT），能够更好地处理多语言场景。</li>
</ol>
<h4 id="使用示例-4">使用示例</h4>
<p>下面的示例展示了如何使用 DSSM 进行训练和推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Word Hashing：将文本转换为特征向量</span></span><br><span class="line">word_hashing = WordHashing(n=<span class="number">3</span>)</span><br><span class="line">query_text = <span class="string">"how to learn machine learning"</span></span><br><span class="line">query_features = word_hashing.text_to_features(query_text, vocab_size=<span class="number">50000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = DSSMModel(</span><br><span class="line">    query_input_dim=<span class="number">50000</span>,</span><br><span class="line">    doc_input_dim=<span class="number">50000</span>,</span><br><span class="line">    query_hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>],</span><br><span class="line">    doc_hidden_dims=[<span class="number">300</span>, <span class="number">300</span>, <span class="number">128</span>],</span><br><span class="line">    embedding_dim=<span class="number">128</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型（使用对比学习损失）</span></span><br><span class="line"><span class="comment"># ... 训练代码 ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理：计算查询和文档的相似度</span></span><br><span class="line">query_emb = model.get_query_embedding(query_features)</span><br><span class="line">doc_emb = model.get_doc_embedding(doc_features)</span><br><span class="line">similarity = torch.<span class="built_in">sum</span>(query_emb * doc_emb, dim=<span class="number">1</span>)  <span class="comment"># 余弦相似度</span></span><br></pre></td></tr></table></figure>
<p>在实际应用中，DSSM
通常用于：（1）搜索引擎中的查询-文档匹配；（2）推荐系统中的用户查询-物品匹配；（3）问答系统中的问题-答案匹配。</p>
<h3 id="dssm-的改进cdssm">DSSM 的改进：CDSSM</h3>
<p>CDSSM（Convolutional
DSSM）使用卷积神经网络替代全连接网络，能够更好地捕获局部特征和序列信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CDSSM</span>(nn.Module):</span><br><span class="line">    <span class="string">"""CDSSM：使用 CNN 的 DSSM"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, embedding_dim=<span class="number">128</span>, </span></span><br><span class="line"><span class="params">                 num_filters=<span class="number">300</span>, filter_sizes=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(CDSSM, self).__init__()</span><br><span class="line">    </span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">            nn.Conv1d(input_dim, num_filters, kernel_size=fs)</span><br><span class="line">            <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">        self.fc = nn.Linear(<span class="built_in">len</span>(filter_sizes) * num_filters, embedding_dim)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        x: [batch_size, seq_len, input_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 转换为 [batch_size, input_dim, seq_len]</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 卷积 + 池化</span></span><br><span class="line">        conv_outputs = []</span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs:</span><br><span class="line">            conv_out = F.relu(conv(x))  <span class="comment"># [batch_size, num_filters, new_seq_len]</span></span><br><span class="line">            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(<span class="number">2</span>))  <span class="comment"># [batch_size, num_filters, 1]</span></span><br><span class="line">            conv_outputs.append(pooled.squeeze(<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 拼接</span></span><br><span class="line">        concatenated = torch.cat(conv_outputs, dim=<span class="number">1</span>)  <span class="comment"># [batch_size, len(filter_sizes)*num_filters]</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        output = self.fc(concatenated)</span><br><span class="line">        output = F.normalize(output, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h2 id="youtube-双塔召回">YouTube 双塔召回</h2>
<h3 id="youtube-推荐系统架构">YouTube 推荐系统架构</h3>
<p>YouTube 的推荐系统采用多阶段架构：</p>
<ol type="1">
<li><strong>候选生成（Candidate
Generation）</strong>：从百万级物品中快速召回数百个候选</li>
<li><strong>排序（Ranking）</strong>：对候选物品进行精细排序</li>
<li><strong>重排（Re-ranking）</strong>：考虑多样性、新鲜度等因素</li>
</ol>
<p>双塔模型主要用于<strong>候选生成</strong>阶段，需要快速从海量物品中召回相关物品。</p>
<h3 id="youtube-双塔模型设计">YouTube 双塔模型设计</h3>
<p><strong>用户塔输入特征</strong>：</p>
<ul>
<li>用户观看历史（最近 N 个视频的 Embedding 平均）</li>
<li>用户搜索历史</li>
<li>用户画像特征（地理位置、设备等）</li>
<li>用户行为统计（观看时长、点击率等）</li>
</ul>
<p><strong>物品塔输入特征</strong>：</p>
<ul>
<li>视频 ID Embedding</li>
<li>视频类别 Embedding</li>
<li>视频统计特征（观看次数、点赞数等）</li>
<li>视频内容特征（标题、描述等）</li>
</ul>
<h3 id="youtube-双塔完整实现">YouTube 双塔完整实现</h3>
<h4 id="问题背景-4">问题背景</h4>
<p>YouTube
作为全球最大的视频平台，每天需要为数亿用户推荐数十亿个视频。传统的协同过滤方法无法满足如此大规模的需求，因为计算用户-物品相似度矩阵需要巨大的计算和存储开销。此外，YouTube
需要处理多种类型的特征：用户观看历史、搜索历史、用户画像、视频属性、视频统计等，如何有效地融合这些特征并学习用户和视频的表示，是一个挑战。更重要的是，YouTube
需要在毫秒级时间内从百万级视频库中召回数百个候选视频，这就要求模型必须支持高效的向量检索。</p>
<h4 id="解决思路-4">解决思路</h4>
<p>YouTube
双塔模型通过将用户和视频分别通过两个独立的神经网络（"塔"）映射到同一个低维向量空间，然后通过向量相似度计算匹配分数。核心设计包括：（1）用户塔融合多种用户特征（观看历史、搜索历史、用户画像、行为统计），通过平均池化处理变长序列，然后通过全连接网络学习用户
Embedding；（2）视频塔融合多种视频特征（视频
ID、类别、统计特征、内容特征），通过 Embedding 层和全连接网络学习视频
Embedding；（3）训练完成后，预先计算所有视频的 Embedding
并建立索引，在推理时只需要计算用户 Embedding，然后通过高效的向量检索（如
FAISS）找到最相似的视频，大幅提升检索效率。</p>
<h4 id="设计考虑-4">设计考虑</h4>
<p>在实现 YouTube 双塔模型时，需要考虑以下几个关键设计：</p>
<ol type="1">
<li><strong>历史序列处理</strong>：用户观看历史是变长序列，需要将其转换为固定维度的特征。YouTube
使用平均池化对历史视频的 Embedding
进行聚合，简单有效。也可以使用注意力机制或 RNN/Transformer
更好地捕获序列信息，但计算开销更大。</li>
<li><strong>特征融合策略</strong>：用户塔和视频塔都需要融合多种类型的特征（稀疏特征、稠密特征、序列特征）。通常使用
Embedding 层处理稀疏特征（如视频
ID、类别），使用全连接网络处理稠密特征（如统计特征），然后拼接或加权融合。</li>
<li><strong>训练样本构造</strong>：YouTube
使用曝光样本（impression）作为训练数据，即用户看到但未点击的视频作为负样本，点击的视频作为正样本。这种构造方式更符合实际推荐场景，但可能导致样本分布不平衡问题。</li>
<li><strong>负采样策略</strong>：由于正样本（点击）远少于负样本（未点击），需要采用负采样策略。YouTube
使用随机负采样，简单但可能不够困难。也可以使用困难负采样（hard negative
mining）提升模型效果，但计算开销更大。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YouTubeUserTower</span>(nn.Module):</span><br><span class="line">    <span class="string">"""YouTube 用户塔"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 video_embedding_dim=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                 category_embedding_dim=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">                 user_feature_dim=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">                 embedding_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YouTubeUserTower, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 视频历史 Embedding（假设最多 50 个历史视频）</span></span><br><span class="line">        self.max_history_length = <span class="number">50</span></span><br><span class="line">        self.video_embedding_dim = video_embedding_dim</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用户特征处理</span></span><br><span class="line">        self.user_fc = nn.Sequential(</span><br><span class="line">            nn.Linear(user_feature_dim, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终融合层</span></span><br><span class="line">        self.fusion = nn.Sequential(</span><br><span class="line">            nn.Linear(video_embedding_dim + <span class="number">64</span>, <span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, embedding_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, user_features, watch_history</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        user_features: [batch_size, user_feature_dim]</span></span><br><span class="line"><span class="string">        watch_history: [batch_size, max_history_length, video_embedding_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size = user_features.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理用户特征</span></span><br><span class="line">        user_emb = self.user_fc(user_features)  <span class="comment"># [batch_size, 64]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理观看历史：平均池化</span></span><br><span class="line">        <span class="comment"># watch_history 中可能包含 padding，需要 mask</span></span><br><span class="line">        history_mask = (watch_history.<span class="built_in">sum</span>(dim=<span class="number">2</span>) != <span class="number">0</span>).<span class="built_in">float</span>()  <span class="comment"># [batch_size, max_history_length]</span></span><br><span class="line">        history_sum = watch_history.<span class="built_in">sum</span>(dim=<span class="number">1</span>)  <span class="comment"># [batch_size, video_embedding_dim]</span></span><br><span class="line">        history_count = history_mask.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="number">1e-8</span></span><br><span class="line">        history_avg = history_sum / history_count  <span class="comment"># [batch_size, video_embedding_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 融合</span></span><br><span class="line">        combined = torch.cat([history_avg, user_emb], dim=<span class="number">1</span>)  <span class="comment"># [batch_size, video_embedding_dim + 64]</span></span><br><span class="line">        output = self.fusion(combined)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L2 归一化</span></span><br><span class="line">        output = F.normalize(output, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YouTubeItemTower</span>(nn.Module):</span><br><span class="line">    <span class="string">"""YouTube 物品塔"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_videos=<span class="number">1000000</span>,</span></span><br><span class="line"><span class="params">                 num_categories=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 video_embedding_dim=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                 category_embedding_dim=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">                 item_feature_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">                 embedding_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YouTubeItemTower, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 视频 ID Embedding</span></span><br><span class="line">        self.video_embedding = nn.Embedding(num_videos, video_embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 类别 Embedding</span></span><br><span class="line">        self.category_embedding = nn.Embedding(num_categories, category_embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 物品特征处理</span></span><br><span class="line">        self.item_fc = nn.Sequential(</span><br><span class="line">            nn.Linear(item_feature_dim, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 融合层</span></span><br><span class="line">        self.fusion = nn.Sequential(</span><br><span class="line">            nn.Linear(video_embedding_dim + category_embedding_dim + <span class="number">64</span>, <span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, embedding_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, video_ids, category_ids, item_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        video_ids: [batch_size]</span></span><br><span class="line"><span class="string">        category_ids: [batch_size]</span></span><br><span class="line"><span class="string">        item_features: [batch_size, item_feature_dim]</span></span><br><span class="line"><span class="string">        返回: [batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 获取 Embedding</span></span><br><span class="line">        video_emb = self.video_embedding(video_ids)  <span class="comment"># [batch_size, video_embedding_dim]</span></span><br><span class="line">        category_emb = self.category_embedding(category_ids)  <span class="comment"># [batch_size, category_embedding_dim]</span></span><br><span class="line">        item_emb = self.item_fc(item_features)  <span class="comment"># [batch_size, 64]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 融合</span></span><br><span class="line">        combined = torch.cat([video_emb, category_emb, item_emb], dim=<span class="number">1</span>)</span><br><span class="line">        output = self.fusion(combined)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L2 归一化</span></span><br><span class="line">        output = F.normalize(output, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YouTubeTwoTower</span>(nn.Module):</span><br><span class="line">    <span class="string">"""YouTube 双塔模型"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_videos=<span class="number">1000000</span>,</span></span><br><span class="line"><span class="params">                 num_categories=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 video_embedding_dim=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                 category_embedding_dim=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">                 user_feature_dim=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">                 item_feature_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">                 embedding_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YouTubeTwoTower, self).__init__()</span><br><span class="line"></span><br><span class="line">    self.user_tower = YouTubeUserTower(</span><br><span class="line">            video_embedding_dim=video_embedding_dim,</span><br><span class="line">            category_embedding_dim=category_embedding_dim,</span><br><span class="line">            user_feature_dim=user_feature_dim,</span><br><span class="line">            embedding_dim=embedding_dim</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    self.item_tower = YouTubeItemTower(</span><br><span class="line">            num_videos=num_videos,</span><br><span class="line">            num_categories=num_categories,</span><br><span class="line">            video_embedding_dim=video_embedding_dim,</span><br><span class="line">            category_embedding_dim=category_embedding_dim,</span><br><span class="line">            item_feature_dim=item_feature_dim,</span><br><span class="line">            embedding_dim=embedding_dim</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, user_features, watch_history, video_ids, category_ids, item_features</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        返回: [batch_size] 相似度分数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        user_emb = self.user_tower(user_features, watch_history)</span><br><span class="line">        item_emb = self.item_tower(video_ids, category_ids, item_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算内积（余弦相似度）</span></span><br><span class="line">        score = torch.<span class="built_in">sum</span>(user_emb * item_emb, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_user_embedding</span>(<span class="params">self, user_features, watch_history</span>):</span><br><span class="line">        <span class="string">"""获取用户 Embedding（用于召回）"""</span></span><br><span class="line">        <span class="keyword">return</span> self.user_tower(user_features, watch_history)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_item_embedding</span>(<span class="params">self, video_ids, category_ids, item_features</span>):</span><br><span class="line">        <span class="string">"""获取物品 Embedding（用于索引）"""</span></span><br><span class="line">        <span class="keyword">return</span> self.item_tower(video_ids, category_ids, item_features)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="关键点解读-4">关键点解读</h4>
<p>YouTube
双塔模型的实现包含几个关键组件，每个组件都有其特定的作用：</p>
<ol type="1">
<li><p><strong>用户塔设计</strong>：用户塔融合多种用户特征，包括观看历史、搜索历史、用户画像和行为统计。观看历史是变长序列，代码使用平均池化对历史视频的
Embedding
进行聚合，简单有效。用户画像和行为统计通过全连接网络处理，然后与历史
Embedding 拼接，通过融合层学习用户
Embedding。这种设计能够同时捕获用户的长期兴趣（历史）和短期偏好（画像、统计）。</p></li>
<li><p><strong>视频塔设计</strong>：视频塔融合多种视频特征，包括视频
ID、类别、统计特征和内容特征。视频 ID 和类别使用 Embedding
层处理，统计特征通过全连接网络处理，然后拼接并通过融合层学习视频
Embedding。这种设计能够同时捕获视频的语义信息（ID、类别）和统计信息（观看次数、点赞数等）。</p></li>
<li><p><strong>特征融合策略</strong>：代码使用拼接（concatenation）方式融合不同特征，然后通过全连接网络学习融合表示。也可以使用加权融合或注意力机制，但拼接方式简单有效，是常用的做法。</p></li>
<li><p><strong>L2 归一化</strong>：代码中对用户和视频的 Embedding 进行
L2 归一化，这使得内积等价于余弦相似度，取值范围为 [-1,
1]。归一化还可以稳定训练过程，避免 Embedding 的模长过大。</p></li>
</ol>
<h4 id="设计权衡-4">设计权衡</h4>
<p>在 YouTube 双塔模型的实现中，存在多个设计权衡：</p>
<ol type="1">
<li><p><strong>历史序列处理 vs
计算效率</strong>：平均池化简单高效，但可能丢失序列信息。使用注意力机制或
RNN/Transformer
能够更好地捕获序列信息，但计算开销更大。对于大规模推荐系统，平均池化是常用的折中方案。</p></li>
<li><p><strong>特征融合 vs
模型复杂度</strong>：拼接方式简单但可能引入冗余信息。使用注意力机制或门控机制能够更好地融合特征，但模型复杂度更高。对于大规模推荐系统，拼接方式是常用的折中方案。</p></li>
<li><p><strong>负采样策略 vs
训练效果</strong>：随机负采样简单但可能不够困难；批量内负采样高效但负样本质量可能不高；困难负采样能够提升模型效果，但计算开销更大。通常采用混合策略：大部分使用批量内负采样，小部分使用困难负采样。</p></li>
<li><p><strong>Embedding 维度 vs 检索效率</strong>：Embedding
维度越高，模型的表达能力越强，但检索效率会降低（向量检索的时间复杂度与维度相关）。通常
Embedding 维度设置为64-256维，根据视频数量和计算资源选择。</p></li>
</ol>
<h4 id="常见问题-4">常见问题</h4>
<ol type="1">
<li><p><strong>如何处理新用户或新视频？</strong>
对于新用户，可以使用用户画像和行为统计特征通过用户塔生成初始
Embedding。对于新视频，可以使用视频类别和内容特征通过视频塔生成初始
Embedding。如果特征不足，可以使用相似用户或相似视频的 Embedding
作为初始化。</p></li>
<li><p><strong>如何提升召回效果？</strong> YouTube
双塔模型的核心优势在于支持高效的向量检索。训练完成后，可以预先计算所有视频的
Embedding 并建立索引（如 FAISS），在推理时只需要计算用户
Embedding，然后通过向量检索找到最相似的视频。为了提升召回效果，可以：（1）使用更大的
Embedding
维度；（2）使用困难负采样训练；（3）使用多任务学习同时优化多个目标。</p></li>
<li><p><strong>如何处理长尾视频？</strong> 长尾视频（观看次数少）的
Embedding
可能不够准确。可以使用内容特征（如标题、描述）通过额外的网络生成初始
Embedding，或者使用相似视频的 Embedding 作为初始化。</p></li>
<li><p><strong>如何平衡准确率和多样性？</strong> YouTube
双塔模型主要优化准确率，可能推荐相似度高的视频导致多样性不足。可以在召回后使用重排（re-ranking）模块考虑多样性、新鲜度等因素，或者使用多目标优化同时优化准确率和多样性。</p></li>
</ol>
<h4 id="使用示例-5">使用示例</h4>
<p>下面的示例展示了如何使用 YouTube 双塔模型进行训练和推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = YouTubeTwoTower(</span><br><span class="line">    num_videos=<span class="number">1000000</span>,</span><br><span class="line">    num_categories=<span class="number">1000</span>,</span><br><span class="line">    video_embedding_dim=<span class="number">64</span>,</span><br><span class="line">    category_embedding_dim=<span class="number">32</span>,</span><br><span class="line">    user_feature_dim=<span class="number">100</span>,</span><br><span class="line">    item_feature_dim=<span class="number">50</span>,</span><br><span class="line">    embedding_dim=<span class="number">128</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型（使用批量内负采样）</span></span><br><span class="line"><span class="comment"># ... 训练代码 ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理：计算用户对视频的兴趣分数</span></span><br><span class="line">user_emb = model.get_user_embedding(user_features, watch_history)</span><br><span class="line">video_emb = model.get_item_embedding(video_ids, category_ids, item_features)</span><br><span class="line">scores = torch.<span class="built_in">sum</span>(user_emb * video_emb, dim=<span class="number">1</span>)  <span class="comment"># 内积（余弦相似度）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 召回：找到与用户最相似的视频（使用 FAISS）</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">video_emb_np = video_emb.cpu().numpy()</span><br><span class="line">index = faiss.IndexFlatIP(<span class="number">128</span>)  <span class="comment"># 内积索引</span></span><br><span class="line">index.add(video_emb_np)</span><br><span class="line">user_emb_np = user_emb[<span class="number">0</span>:<span class="number">1</span>].cpu().numpy()</span><br><span class="line">distances, indices = index.search(user_emb_np, k=<span class="number">100</span>)  <span class="comment"># 找到最相似的100个视频</span></span><br></pre></td></tr></table></figure>
<p>在实际应用中，YouTube
双塔模型通常用于：（1）大规模推荐系统的召回阶段，通过向量检索快速找到候选视频；（2）视频搜索，通过查询和视频的
Embedding 进行语义匹配；（3）相似视频推荐，通过视频 Embedding
找到相似视频。</p>
<h3 id="youtube-双塔的训练策略">YouTube 双塔的训练策略</h3>
<p><strong>负采样策略</strong>：</p>
<ul>
<li><strong>随机负采样</strong>：从所有视频中随机采样</li>
<li><strong>批量内负采样</strong>：在同一个 batch
中，其他样本的视频作为负样本（更高效）</li>
<li><strong>困难负采样</strong>：选择模型预测分数较高的负样本（需要额外训练）</li>
</ul>
<p><strong>训练目标</strong>： 使用 softmax 交叉熵损失：</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="118.1ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 52200 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="merror" data-mjx-error="Extra close brace or missing open brace" title="Extra close brace or missing open brace"><rect data-background="true" width="52200" height="950" y="-200"/><title>Extra close brace or missing open brace</title><g data-mml-node="mtext" style="font-family: serif;"><text data-variant="-explicitFont" transform="scale(1,-1)" font-size="884px"> L = -\log \frac{\exp(s(u, i^+))}{\exp(s(u, i^+)) + \sum_{j=1}^{N} \exp(s(u, i_j^-))}} </text></g></g></g></g></svg></mjx-container></span></p>
<p>其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.213ex" height="1.779ex" role="img" focusable="false" viewbox="0 -775.2 978.1 786.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(378,363) scale(0.707)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g></g></g></g></svg></mjx-container></span> 是正样本，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.991ex;" xmlns="http://www.w3.org/2000/svg" width="2.213ex" height="2.855ex" role="img" focusable="false" viewbox="0 -823.8 978.1 1261.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(378,411.6) scale(0.707)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(378,-293.8) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></svg></mjx-container></span> 是负样本。</p>
<h2 id="负采样策略详解">负采样策略详解</h2>
<h3 id="负采样的必要性">负采样的必要性</h3>
<p>在推荐系统中，正样本（用户-物品交互）通常只占所有可能的用户-物品对的很小一部分。如果使用所有负样本训练，会导致：</p>
<ol type="1">
<li><strong>计算成本过高</strong>：需要计算所有负样本的损失</li>
<li><strong>类别不平衡</strong>：负样本远多于正样本</li>
<li><strong>训练效率低</strong>：大部分负样本对模型学习没有帮助</li>
</ol>
<p>负采样通过只选择一部分负样本进行训练，解决了这些问题。</p>
<h3 id="随机负采样">随机负采样</h3>
<p>最简单的负采样策略是随机采样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_negative_sampling</span>(<span class="params">positive_items, all_items, num_negatives</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    随机负采样</span></span><br><span class="line"><span class="string">    positive_items: 正样本物品列表</span></span><br><span class="line"><span class="string">    all_items: 所有物品集合</span></span><br><span class="line"><span class="string">    num_negatives: 每个正样本对应的负样本数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    negative_items = []</span><br><span class="line">    <span class="keyword">for</span> pos_item <span class="keyword">in</span> positive_items:</span><br><span class="line">        <span class="comment"># 从所有物品中随机采样，排除正样本</span></span><br><span class="line">        candidates = <span class="built_in">list</span>(<span class="built_in">set</span>(all_items) - {pos_item})</span><br><span class="line">        negs = np.random.choice(candidates, size=num_negatives, replace=<span class="literal">False</span>)</span><br><span class="line">        negative_items.append(negs)</span><br><span class="line">    <span class="keyword">return</span> negative_items</span><br></pre></td></tr></table></figure>
<h3 id="基于频率的负采样">基于频率的负采样</h3>
<p>根据物品的流行度（频率）进行采样，流行物品更容易被采样为负样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">frequency_based_negative_sampling</span>(<span class="params">positive_items, item_freq, num_negatives, alpha=<span class="number">0.75</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    基于频率的负采样</span></span><br><span class="line"><span class="string">    item_freq: 物品频率字典</span></span><br><span class="line"><span class="string">    alpha: 频率的幂次（通常为 0.75）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 构建采样分布</span></span><br><span class="line">    items = <span class="built_in">list</span>(item_freq.keys())</span><br><span class="line">    frequencies = np.array([item_freq[item] <span class="keyword">for</span> item <span class="keyword">in</span> items])</span><br><span class="line">    <span class="comment"># 使用 alpha 次幂</span></span><br><span class="line">    probs = np.power(frequencies, alpha)</span><br><span class="line">    probs = probs / probs.<span class="built_in">sum</span>()</span><br><span class="line">  </span><br><span class="line">    negative_items = []</span><br><span class="line">    <span class="keyword">for</span> pos_item <span class="keyword">in</span> positive_items:</span><br><span class="line">        <span class="comment"># 排除正样本</span></span><br><span class="line">        mask = np.array([item != pos_item <span class="keyword">for</span> item <span class="keyword">in</span> items])</span><br><span class="line">        filtered_items = np.array(items)[mask]</span><br><span class="line">        filtered_probs = probs[mask]</span><br><span class="line">        filtered_probs = filtered_probs / filtered_probs.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">        negs = np.random.choice(</span><br><span class="line">            filtered_items,</span><br><span class="line">            size=num_negatives,</span><br><span class="line">            p=filtered_probs,</span><br><span class="line">            replace=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        negative_items.append(negs)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> negative_items</span><br></pre></td></tr></table></figure>
<h3 id="批量内负采样">批量内负采样</h3>
<p>在同一个 batch 内，其他样本的物品可以作为负样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">in_batch_negative_sampling</span>(<span class="params">batch_items</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    批量内负采样</span></span><br><span class="line"><span class="string">    batch_items: [batch_size] 当前 batch 的物品 ID</span></span><br><span class="line"><span class="string">    返回: [batch_size, batch_size-1] 每个样本的负样本</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    batch_size = <span class="built_in">len</span>(batch_items)</span><br><span class="line">    negative_items = []</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        <span class="comment"># 当前样本的物品</span></span><br><span class="line">        pos_item = batch_items[i]</span><br><span class="line">        <span class="comment"># 其他样本的物品作为负样本</span></span><br><span class="line">        negs = [batch_items[j] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size) <span class="keyword">if</span> j != i]</span><br><span class="line">        negative_items.append(negs)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> np.array(negative_items)</span><br></pre></td></tr></table></figure>
<h3 id="困难负采样hard-negative-sampling">困难负采样（Hard Negative
Sampling）</h3>
<p>选择模型预测分数较高的负样本，这些样本更难区分，有助于提升模型性能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hard_negative_sampling</span>(<span class="params">model, user_features, positive_items, candidate_items, </span></span><br><span class="line"><span class="params">                          num_negatives, k=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    困难负采样</span></span><br><span class="line"><span class="string">    k: 先采样 k 倍候选，然后选择分数最高的</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 扩展候选集</span></span><br><span class="line">    extended_candidates = np.random.choice(</span><br><span class="line">        candidate_items,</span><br><span class="line">        size=<span class="built_in">min</span>(<span class="built_in">len</span>(candidate_items), num_negatives * k),</span><br><span class="line">        replace=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 计算分数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        user_emb = model.get_user_embedding(user_features)</span><br><span class="line">        item_embeddings = model.get_item_embedding(extended_candidates)</span><br><span class="line">    </span><br><span class="line">        scores = torch.matmul(user_emb, item_embeddings.T)  <span class="comment"># [1, len(candidates)]</span></span><br><span class="line">        scores = scores.squeeze(<span class="number">0</span>).cpu().numpy()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 选择分数最高的 num_negatives 个</span></span><br><span class="line">    top_indices = np.argsort(scores)[-num_negatives:]</span><br><span class="line">    hard_negatives = extended_candidates[top_indices]</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> hard_negatives</span><br></pre></td></tr></table></figure>
<h3 id="混合负采样策略">混合负采样策略</h3>
<p>结合多种采样策略：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MixedNegativeSampling</span>:</span><br><span class="line">    <span class="string">"""混合负采样策略"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, item_freq, alpha=<span class="number">0.75</span></span>):</span><br><span class="line">        self.item_freq = item_freq</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self._build_distribution()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_distribution</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""构建采样分布"""</span></span><br><span class="line">        items = <span class="built_in">list</span>(self.item_freq.keys())</span><br><span class="line">        frequencies = np.array([self.item_freq[item] <span class="keyword">for</span> item <span class="keyword">in</span> items])</span><br><span class="line">        probs = np.power(frequencies, self.alpha)</span><br><span class="line">        self.distribution = probs / probs.<span class="built_in">sum</span>()</span><br><span class="line">        self.items = np.array(items)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, positive_items, num_negatives, strategy=<span class="string">'mixed'</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        strategy: 'random', 'frequency', 'mixed'</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> strategy == <span class="string">'random'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._random_sample(positive_items, num_negatives)</span><br><span class="line">        <span class="keyword">elif</span> strategy == <span class="string">'frequency'</span>:</span><br><span class="line">            <span class="keyword">return</span> self._frequency_sample(positive_items, num_negatives)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># mixed</span></span><br><span class="line">            <span class="keyword">return</span> self._mixed_sample(positive_items, num_negatives)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_random_sample</span>(<span class="params">self, positive_items, num_negatives</span>):</span><br><span class="line">        <span class="string">"""随机采样"""</span></span><br><span class="line">        negative_items = []</span><br><span class="line">        <span class="keyword">for</span> pos_item <span class="keyword">in</span> positive_items:</span><br><span class="line">            mask = self.items != pos_item</span><br><span class="line">            candidates = self.items[mask]</span><br><span class="line">            negs = np.random.choice(candidates, size=num_negatives, replace=<span class="literal">False</span>)</span><br><span class="line">            negative_items.append(negs)</span><br><span class="line">        <span class="keyword">return</span> np.array(negative_items)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_frequency_sample</span>(<span class="params">self, positive_items, num_negatives</span>):</span><br><span class="line">        <span class="string">"""频率采样"""</span></span><br><span class="line">        negative_items = []</span><br><span class="line">        <span class="keyword">for</span> pos_item <span class="keyword">in</span> positive_items:</span><br><span class="line">            mask = self.items != pos_item</span><br><span class="line">            candidates = self.items[mask]</span><br><span class="line">            probs = self.distribution[mask]</span><br><span class="line">            probs = probs / probs.<span class="built_in">sum</span>()</span><br><span class="line">            negs = np.random.choice(candidates, size=num_negatives, p=probs, replace=<span class="literal">False</span>)</span><br><span class="line">            negative_items.append(negs)</span><br><span class="line">        <span class="keyword">return</span> np.array(negative_items)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mixed_sample</span>(<span class="params">self, positive_items, num_negatives</span>):</span><br><span class="line">        <span class="string">"""混合采样：一半随机，一半频率"""</span></span><br><span class="line">        num_random = num_negatives // <span class="number">2</span></span><br><span class="line">        num_freq = num_negatives - num_random</span><br><span class="line">    </span><br><span class="line">        random_negs = self._random_sample(positive_items, num_random)</span><br><span class="line">        freq_negs = self._frequency_sample(positive_items, num_freq)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> np.concatenate([random_negs, freq_negs], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="ann-近邻检索">ANN 近邻检索</h2>
<h3 id="ann-检索的必要性">ANN 检索的必要性</h3>
<p>在推荐系统的召回阶段，需要从百万甚至千万级物品中快速找到与用户
Embedding 最相似的物品。如果使用暴力搜索（Brute Force），计算复杂度为
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.672ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2949 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(2040,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(2560,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>，其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g></svg></mjx-container></span> 是物品数量，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 是 Embedding
维度，这在大规模场景下是不可接受的。</p>
<p>近似最近邻（Approximate Nearest Neighbor,
ANN）检索通过牺牲一定的精度来换取速度，能够在毫秒级时间内完成检索。</p>
<h3 id="faiss-库使用">FAISS 库使用</h3>
<h4 id="问题背景-5">问题背景</h4>
<p>在推荐系统的召回阶段，需要从百万甚至千万级物品中快速找到与用户
Embedding 最相似的物品。传统的暴力搜索（Brute Force）需要计算用户
Embedding 与所有物品 Embedding 的相似度，计算复杂度为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.672ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2949 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"/></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(2040,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(2560,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>，其中 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g></svg></mjx-container></span> 是物品数量，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g></g></g></svg></mjx-container></span> 是 Embedding
维度。对于百万级物品库，暴力搜索需要数秒甚至数十秒，无法满足实时推荐的需求。此外，随着物品数量的增长，计算时间线性增长，这在大规模场景下是不可接受的。</p>
<h4 id="解决思路-5">解决思路</h4>
<p>近似最近邻（Approximate Nearest Neighbor,
ANN）检索通过牺牲一定的精度来换取速度，能够在毫秒级时间内完成检索。FAISS（Facebook
AI Similarity Search）是 Facebook
开源的向量相似度检索库，提供了多种索引类型，包括：（1）Flat
Index：暴力搜索，精度100%但速度慢；（2）IVF（Inverted File
Index）：通过聚类将向量分组，只搜索相关聚类，速度更快；（3）HNSW（Hierarchical
Navigable Small World）：基于图的索引，速度快且精度高；（4）Product
Quantization：通过向量压缩减少内存占用。通过选择合适的索引类型和参数，可以在精度和速度之间取得平衡。</p>
<h4 id="设计考虑-5">设计考虑</h4>
<p>在实现 FAISS 索引时，需要考虑以下几个关键设计：</p>
<ol type="1">
<li><strong>索引类型选择</strong>：Flat Index
精度最高但速度最慢，适用于小规模场景（&lt;100万）；IVF
索引通过聚类加速检索，适用于中等规模场景（100万-1000万）；HNSW
索引速度快且精度高，适用于大规模场景（&gt;1000万）。选择哪种索引类型取决于物品数量、精度要求和计算资源。</li>
<li><strong>距离度量选择</strong>：FAISS 支持多种距离度量，包括 L2
距离和内积（IP）。对于归一化的
Embedding，内积等价于余弦相似度，是推荐系统中常用的度量。选择哪种度量取决于
Embedding 是否归一化。</li>
<li><strong>索引参数调优</strong>：IVF
索引需要设置聚类数量（nlist）和搜索时检查的聚类数量（nprobe）。nlist
越大，检索精度越高但构建时间越长；nprobe
越大，检索精度越高但检索时间越长。需要在精度和速度之间权衡。</li>
<li><strong>内存和持久化</strong>：大规模索引可能占用大量内存，需要考虑内存限制。FAISS
支持索引的保存和加载，可以将索引持久化到磁盘，避免每次重新构建。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FAISSIndex</span>:</span><br><span class="line">    <span class="string">"""基于 FAISS 的向量检索"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dimension, index_type=<span class="string">'L2'</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        dimension: Embedding 维度</span></span><br><span class="line"><span class="string">        index_type: 'L2' 或 'IP'（内积）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.dimension = dimension</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> index_type == <span class="string">'L2'</span>:</span><br><span class="line">            <span class="comment"># L2 距离索引</span></span><br><span class="line">            self.index = faiss.IndexFlatL2(dimension)</span><br><span class="line">        <span class="keyword">elif</span> index_type == <span class="string">'IP'</span>:</span><br><span class="line">            <span class="comment"># 内积索引（需要归一化）</span></span><br><span class="line">            self.index = faiss.IndexFlatIP(dimension)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Unknown index type: <span class="subst">{index_type}</span>"</span>)</span><br><span class="line">    </span><br><span class="line">        self.index_type = index_type</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, vectors</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        添加向量到索引</span></span><br><span class="line"><span class="string">        vectors: [N, dimension] numpy array</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.index_type == <span class="string">'IP'</span>:</span><br><span class="line">            <span class="comment"># 内积索引需要归一化</span></span><br><span class="line">            faiss.normalize_L2(vectors)</span><br><span class="line">    </span><br><span class="line">        self.index.add(vectors.astype(<span class="string">'float32'</span>))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, query_vectors, k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        搜索最相似的 k 个向量</span></span><br><span class="line"><span class="string">        query_vectors: [M, dimension] numpy array</span></span><br><span class="line"><span class="string">        返回: (distances, indices)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.index_type == <span class="string">'IP'</span>:</span><br><span class="line">            <span class="comment"># 归一化查询向量</span></span><br><span class="line">            query_vectors = query_vectors.copy().astype(<span class="string">'float32'</span>)</span><br><span class="line">            faiss.normalize_L2(query_vectors)</span><br><span class="line">    </span><br><span class="line">        distances, indices = self.index.search(query_vectors.astype(<span class="string">'float32'</span>), k)</span><br><span class="line">        <span class="keyword">return</span> distances, indices</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_faiss</span>():</span><br><span class="line">    <span class="string">"""FAISS 使用示例"""</span></span><br><span class="line">    dimension = <span class="number">128</span></span><br><span class="line">    num_items = <span class="number">1000000</span></span><br><span class="line">    num_queries = <span class="number">100</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 生成物品 Embedding</span></span><br><span class="line">    item_embeddings = np.random.randn(num_items, dimension).astype(<span class="string">'float32'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建索引（使用内积，适合归一化的 Embedding）</span></span><br><span class="line">    index = FAISSIndex(dimension, index_type=<span class="string">'IP'</span>)</span><br><span class="line">    index.add(item_embeddings)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 生成查询向量</span></span><br><span class="line">    query_embeddings = np.random.randn(num_queries, dimension).astype(<span class="string">'float32'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 搜索</span></span><br><span class="line">    distances, indices = index.search(query_embeddings, k=<span class="number">10</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Query 0 top-10 items: <span class="subst">{indices[<span class="number">0</span>]}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Distances: <span class="subst">{distances[<span class="number">0</span>]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用更高效的索引（IVF）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IVFIndex</span>:</span><br><span class="line">    <span class="string">"""IVF（Inverted File）索引：更快的检索速度"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dimension, nlist=<span class="number">100</span>, nprobe=<span class="number">10</span>, index_type=<span class="string">'L2'</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        nlist: 聚类中心数量</span></span><br><span class="line"><span class="string">        nprobe: 搜索时检查的聚类数量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.dimension = dimension</span><br><span class="line">        quantizer = faiss.IndexFlatL2(dimension)</span><br><span class="line">        self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)</span><br><span class="line">        self.index.nprobe = nprobe</span><br><span class="line">        self.index_type = index_type</span><br><span class="line">        self.is_trained = <span class="literal">False</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, vectors</span>):</span><br><span class="line">        <span class="string">"""训练索引（需要先训练才能添加向量）"""</span></span><br><span class="line">        <span class="keyword">if</span> self.index_type == <span class="string">'IP'</span>:</span><br><span class="line">            vectors = vectors.copy().astype(<span class="string">'float32'</span>)</span><br><span class="line">            faiss.normalize_L2(vectors)</span><br><span class="line">    </span><br><span class="line">        self.index.train(vectors.astype(<span class="string">'float32'</span>))</span><br><span class="line">        self.is_trained = <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, vectors</span>):</span><br><span class="line">        <span class="string">"""添加向量"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_trained:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Index must be trained before adding vectors"</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> self.index_type == <span class="string">'IP'</span>:</span><br><span class="line">            vectors = vectors.copy().astype(<span class="string">'float32'</span>)</span><br><span class="line">            faiss.normalize_L2(vectors)</span><br><span class="line">    </span><br><span class="line">        self.index.add(vectors.astype(<span class="string">'float32'</span>))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, query_vectors, k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""搜索"""</span></span><br><span class="line">        <span class="keyword">if</span> self.index_type == <span class="string">'IP'</span>:</span><br><span class="line">            query_vectors = query_vectors.copy().astype(<span class="string">'float32'</span>)</span><br><span class="line">            faiss.normalize_L2(query_vectors)</span><br><span class="line">    </span><br><span class="line">        distances, indices = self.index.search(query_vectors.astype(<span class="string">'float32'</span>), k)</span><br><span class="line">        <span class="keyword">return</span> distances, indices</span><br></pre></td></tr></table></figure>
<h4 id="关键点解读-5">关键点解读</h4>
<p>FAISS 索引的实现包含几个关键组件，每个组件都有其特定的作用：</p>
<ol type="1">
<li><strong>索引类型选择</strong>：Flat Index
使用暴力搜索，精度100%但速度最慢，适用于小规模场景（&lt;100万物品）。IVF
Index 通过聚类将向量分组，只搜索相关聚类，速度更快但需要先训练索引。HNSW
Index
基于图的索引，速度快且精度高，适用于大规模场景（&gt;1000万物品）。</li>
<li><strong>距离度量选择</strong>：FAISS 支持 L2
距离和内积（IP）两种度量。对于归一化的
Embedding，内积等价于余弦相似度，是推荐系统中常用的度量。代码中根据索引类型自动处理归一化，确保内积计算的正确性。</li>
<li><strong>索引训练</strong>：IVF
索引需要先训练才能添加向量。训练过程使用 K-means
聚类算法将向量分组，训练时间与物品数量和聚类数量相关。通常训练一次后可以多次使用，训练好的索引可以保存到磁盘。</li>
<li><strong>检索参数调优</strong>：IVF 索引的 nprobe
参数控制搜索时检查的聚类数量。nprobe
越大，检索精度越高但检索时间越长。通常设置
nprobe=10-100，在精度和速度之间权衡。</li>
</ol>
<h4 id="设计权衡-5">设计权衡</h4>
<p>在 FAISS 索引的实现中，存在多个设计权衡：</p>
<ol type="1">
<li><strong>精度 vs 速度</strong>：Flat Index 精度最高但速度最慢；IVF
Index 通过聚类加速检索，精度略降但速度大幅提升；HNSW Index
速度快且精度高，但构建时间较长。选择哪种索引类型取决于物品数量、精度要求和计算资源。</li>
<li><strong>内存 vs 速度</strong>：Product
Quantization（PQ）通过向量压缩减少内存占用，但检索精度会降低。对于内存受限的场景，可以使用
IVFPQ 索引，在内存和精度之间权衡。</li>
<li><strong>构建时间 vs 检索时间</strong>：HNSW
索引构建时间较长，但检索速度最快；IVF
索引构建时间中等，检索速度较快；Flat Index
无需构建，但检索时间最长。如果索引需要频繁更新，应该选择构建时间短的索引类型。</li>
</ol>
<h4 id="常见问题-5">常见问题</h4>
<ol type="1">
<li><strong>如何选择索引类型？</strong>
对于小规模场景（&lt;100万），使用 Flat
Index；对于中等规模场景（100万-1000万），使用 IVF
Index；对于大规模场景（&gt;1000万），使用 HNSW
Index。如果内存受限，可以使用 IVFPQ 索引。</li>
<li><strong>如何调优索引参数？</strong> IVF 索引的 nlist
参数控制聚类数量，通常设置为 sqrt(N)，其中 N 是物品数量。nprobe
参数控制搜索时检查的聚类数量，通常设置为 nlist 的 1/10 到
1/100。可以通过实验找到精度和速度的平衡点。</li>
<li><strong>如何处理索引更新？</strong> FAISS
支持增量添加向量，但删除向量需要重建索引。对于频繁更新的场景，可以使用支持删除的索引类型（如
HNSW），或者定期重建索引。</li>
<li><strong>如何持久化索引？</strong> FAISS
支持索引的保存和加载，可以将索引持久化到磁盘。保存时使用
<code>faiss.write_index()</code>，加载时使用
<code>faiss.read_index()</code>。持久化可以避免每次重新构建索引，大幅提升启动速度。</li>
</ol>
<h4 id="使用示例-6">使用示例</h4>
<p>下面的示例展示了如何使用 FAISS 索引进行向量检索：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建索引（使用内积，适合归一化的 Embedding）</span></span><br><span class="line">index = FAISSIndex(dimension=<span class="number">128</span>, index_type=<span class="string">'IP'</span>)</span><br><span class="line">index.add(item_embeddings)  <span class="comment"># 添加物品 Embedding</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索最相似的物品</span></span><br><span class="line">distances, indices = index.search(query_embeddings, k=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 IVF 索引加速检索（适用于大规模场景）</span></span><br><span class="line">ivf_index = IVFIndex(dimension=<span class="number">128</span>, nlist=<span class="number">1000</span>, nprobe=<span class="number">50</span>)</span><br><span class="line">ivf_index.train(item_embeddings)  <span class="comment"># 先训练</span></span><br><span class="line">ivf_index.add(item_embeddings)  <span class="comment"># 再添加</span></span><br><span class="line">distances, indices = ivf_index.search(query_embeddings, k=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>在实际应用中，FAISS
索引通常用于：（1）大规模推荐系统的召回阶段，通过向量检索快速找到候选物品；（2）相似物品推荐，通过物品
Embedding 找到相似物品；（3）用户画像匹配，通过用户 Embedding
找到相似用户。</p>
<h3 id="annoy-库使用">Annoy 库使用</h3>
<p>Annoy（Approximate Nearest Neighbors Oh Yeah）是 Spotify 开源的 ANN
库，基于随机投影树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> annoy <span class="keyword">import</span> AnnoyIndex</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AnnoyIndexWrapper</span>:</span><br><span class="line">    <span class="string">"""Annoy 索引封装"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dimension, metric=<span class="string">'angular'</span>, n_trees=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        dimension: Embedding 维度</span></span><br><span class="line"><span class="string">        metric: 'angular'（余弦相似度）或 'euclidean'（欧氏距离）</span></span><br><span class="line"><span class="string">        n_trees: 树的数量（越多越准确，但构建时间越长）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.dimension = dimension</span><br><span class="line">        self.metric = metric</span><br><span class="line">        self.index = AnnoyIndex(dimension, metric)</span><br><span class="line">        self.n_trees = n_trees</span><br><span class="line">        self.item_count = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_item</span>(<span class="params">self, vector</span>):</span><br><span class="line">        <span class="string">"""添加一个向量"""</span></span><br><span class="line">        self.index.add_item(self.item_count, vector)</span><br><span class="line">        self.item_count += <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""构建索引"""</span></span><br><span class="line">        self.index.build(self.n_trees)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, query_vector, k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""搜索最相似的 k 个向量"""</span></span><br><span class="line">        indices = self.index.get_nns_by_vector(query_vector, k)</span><br><span class="line">        <span class="keyword">return</span> indices</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        <span class="string">"""保存索引"""</span></span><br><span class="line">        self.index.save(filepath)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        <span class="string">"""加载索引"""</span></span><br><span class="line">        self.index.load(filepath)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_annoy</span>():</span><br><span class="line">    <span class="string">"""Annoy 使用示例"""</span></span><br><span class="line">    dimension = <span class="number">128</span></span><br><span class="line">    num_items = <span class="number">1000000</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建索引</span></span><br><span class="line">    index = AnnoyIndexWrapper(dimension, metric=<span class="string">'angular'</span>, n_trees=<span class="number">50</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 添加物品 Embedding</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_items):</span><br><span class="line">        vector = np.random.randn(dimension).astype(<span class="string">'float32'</span>)</span><br><span class="line">        <span class="comment"># 归一化（用于余弦相似度）</span></span><br><span class="line">        vector = vector / (np.linalg.norm(vector) + <span class="number">1e-8</span>)</span><br><span class="line">        index.add_item(vector)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 构建索引</span></span><br><span class="line">    index.build()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 搜索</span></span><br><span class="line">    query_vector = np.random.randn(dimension).astype(<span class="string">'float32'</span>)</span><br><span class="line">    query_vector = query_vector / (np.linalg.norm(query_vector) + <span class="number">1e-8</span>)</span><br><span class="line">  </span><br><span class="line">    top_k = index.search(query_vector, k=<span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Top-10 items: <span class="subst">{top_k}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="hnsw-算法">HNSW 算法</h3>
<p>HNSW（Hierarchical Navigable Small World）是一种基于图的 ANN
算法，在精度和速度之间取得了很好的平衡。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hnswlib</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HNSWIndex</span>:</span><br><span class="line">    <span class="string">"""HNSW 索引"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dimension, metric=<span class="string">'cosine'</span>, max_elements=<span class="number">1000000</span>, </span></span><br><span class="line"><span class="params">                 ef_construction=<span class="number">200</span>, M=<span class="number">16</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        dimension: Embedding 维度</span></span><br><span class="line"><span class="string">        metric: 'cosine', 'l2', 'ip'</span></span><br><span class="line"><span class="string">        max_elements: 最大元素数量</span></span><br><span class="line"><span class="string">        ef_construction: 构建时的候选数量</span></span><br><span class="line"><span class="string">        M: 每个节点的最大连接数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.dimension = dimension</span><br><span class="line">        self.index = hnswlib.Index(space=metric, dim=dimension)</span><br><span class="line">        self.index.init_index(max_elements=max_elements, </span><br><span class="line">                              ef_construction=ef_construction, </span><br><span class="line">                              M=M)</span><br><span class="line">        self.item_count = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, vectors</span>):</span><br><span class="line">        <span class="string">"""批量添加向量"""</span></span><br><span class="line">        num_vectors = <span class="built_in">len</span>(vectors)</span><br><span class="line">        self.index.add_items(vectors.astype(<span class="string">'float32'</span>), </span><br><span class="line">                            np.arange(self.item_count, </span><br><span class="line">                                     self.item_count + num_vectors))</span><br><span class="line">        self.item_count += num_vectors</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, query_vectors, k=<span class="number">10</span>, ef_search=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        搜索</span></span><br><span class="line"><span class="string">        ef_search: 搜索时的候选数量（越大越准确，但越慢）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> ef_search <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            ef_search = <span class="built_in">max</span>(k * <span class="number">2</span>, <span class="number">50</span>)</span><br><span class="line">    </span><br><span class="line">        self.index.set_ef(ef_search)</span><br><span class="line">        labels, distances = self.index.knn_query(query_vectors.astype(<span class="string">'float32'</span>), k=k)</span><br><span class="line">        <span class="keyword">return</span> distances, labels</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        <span class="string">"""保存索引"""</span></span><br><span class="line">        self.index.save_index(filepath)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        <span class="string">"""加载索引"""</span></span><br><span class="line">        self.index.load_index(filepath)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_hnsw</span>():</span><br><span class="line">    <span class="string">"""HNSW 使用示例"""</span></span><br><span class="line">    dimension = <span class="number">128</span></span><br><span class="line">    num_items = <span class="number">1000000</span></span><br><span class="line">    num_queries = <span class="number">100</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 生成物品 Embedding</span></span><br><span class="line">    item_embeddings = np.random.randn(num_items, dimension).astype(<span class="string">'float32'</span>)</span><br><span class="line">    <span class="comment"># 归一化（用于余弦相似度）</span></span><br><span class="line">    norms = np.linalg.norm(item_embeddings, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    item_embeddings = item_embeddings / (norms + <span class="number">1e-8</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 创建索引</span></span><br><span class="line">    index = HNSWIndex(dimension, metric=<span class="string">'cosine'</span>, max_elements=num_items)</span><br><span class="line">    index.add(item_embeddings)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 生成查询向量</span></span><br><span class="line">    query_embeddings = np.random.randn(num_queries, dimension).astype(<span class="string">'float32'</span>)</span><br><span class="line">    query_norms = np.linalg.norm(query_embeddings, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    query_embeddings = query_embeddings / (query_norms + <span class="number">1e-8</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 搜索</span></span><br><span class="line">    distances, indices = index.search(query_embeddings, k=<span class="number">10</span>, ef_search=<span class="number">100</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Query 0 top-10 items: <span class="subst">{indices[<span class="number">0</span>]}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Distances: <span class="subst">{distances[<span class="number">0</span>]}</span>"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ann-索引选择指南">ANN 索引选择指南</h3>
<p><strong>FAISS</strong>：</p>
<ul>
<li>优点：功能丰富，支持多种索引类型，速度快</li>
<li>缺点：需要编译，依赖较多</li>
<li>适用场景：大规模生产环境</li>
</ul>
<p><strong>Annoy</strong>：</p>
<ul>
<li>优点：简单易用，支持持久化，内存占用小</li>
<li>缺点：构建索引较慢，精度略低于 HNSW</li>
<li>适用场景：中小规模场景，需要持久化的场景</li>
</ul>
<p><strong>HNSW</strong>：</p>
<ul>
<li>优点：精度高，速度快，内存占用适中</li>
<li>缺点：构建时间较长</li>
<li>适用场景：对精度要求较高的场景</li>
</ul>
<h2 id="embedding-质量评估">Embedding 质量评估</h2>
<h3 id="评估指标">评估指标</h3>
<p><strong>1. 相似度分布</strong></p>
<p>检查 Embedding 的相似度分布是否合理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_similarity_distribution</span>(<span class="params">embeddings, sample_size=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">"""评估相似度分布"""</span></span><br><span class="line">    num_items = <span class="built_in">len</span>(embeddings)</span><br><span class="line">    indices = np.random.choice(num_items, size=sample_size, replace=<span class="literal">False</span>)</span><br><span class="line">  </span><br><span class="line">    similarities = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(indices)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(indices)):</span><br><span class="line">            sim = np.dot(embeddings[indices[i]], embeddings[indices[j]])</span><br><span class="line">            similarities.append(sim)</span><br><span class="line">  </span><br><span class="line">    similarities = np.array(similarities)</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Mean similarity: <span class="subst">{similarities.mean():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Std similarity: <span class="subst">{similarities.std():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Min similarity: <span class="subst">{similarities.<span class="built_in">min</span>():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Max similarity: <span class="subst">{similarities.<span class="built_in">max</span>():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> similarities</span><br></pre></td></tr></table></figure>
<p><strong>2. 召回率（Recall@K）</strong></p>
<p>在测试集上评估召回率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_recall_at_k</span>(<span class="params">user_embeddings, item_embeddings, test_pairs, k=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    评估 Recall@K</span></span><br><span class="line"><span class="string">    test_pairs: [(user_id, item_id), ...] 测试集正样本对</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    recalls = []</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> user_id, true_item_id <span class="keyword">in</span> test_pairs:</span><br><span class="line">        user_emb = user_embeddings[user_id]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 计算与所有物品的相似度</span></span><br><span class="line">        similarities = np.dot(item_embeddings, user_emb)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 获取 top-k</span></span><br><span class="line">        top_k_indices = np.argsort(similarities)[::-<span class="number">1</span>][:k]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 检查真实物品是否在 top-k 中</span></span><br><span class="line">        recall = <span class="number">1.0</span> <span class="keyword">if</span> true_item_id <span class="keyword">in</span> top_k_indices <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">        recalls.append(recall)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> np.mean(recalls)</span><br></pre></td></tr></table></figure>
<p><strong>3. 覆盖率（Coverage）</strong></p>
<p>评估推荐结果的多样性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_coverage</span>(<span class="params">recommendations, num_items</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    评估覆盖率</span></span><br><span class="line"><span class="string">    recommendations: [[item_id, ...], ...] 每个用户的推荐列表</span></span><br><span class="line"><span class="string">    num_items: 物品总数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    recommended_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> rec_list <span class="keyword">in</span> recommendations:</span><br><span class="line">        recommended_items.update(rec_list)</span><br><span class="line">  </span><br><span class="line">    coverage = <span class="built_in">len</span>(recommended_items) / num_items</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br></pre></td></tr></table></figure>
<p><strong>4. 新颖性（Novelty）</strong></p>
<p>评估推荐物品的新颖性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_novelty</span>(<span class="params">recommendations, item_popularity</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    评估新颖性</span></span><br><span class="line"><span class="string">    item_popularity: {item_id: popularity_score}</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    novelty_scores = []</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> rec_list <span class="keyword">in</span> recommendations:</span><br><span class="line">        scores = [item_popularity.get(item_id, <span class="number">0</span>) <span class="keyword">for</span> item_id <span class="keyword">in</span> rec_list]</span><br><span class="line">        <span class="comment"># 新颖性 = 1 - 平均流行度</span></span><br><span class="line">        novelty = <span class="number">1.0</span> - np.mean(scores)</span><br><span class="line">        novelty_scores.append(novelty)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> np.mean(novelty_scores)</span><br></pre></td></tr></table></figure>
<h3 id="embedding-可视化">Embedding 可视化</h3>
<p>使用 t-SNE 或 UMAP 将高维 Embedding 降维到 2D 进行可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_embeddings</span>(<span class="params">embeddings, labels=<span class="literal">None</span>, title=<span class="string">"Embedding Visualization"</span></span>):</span><br><span class="line">    <span class="string">"""可视化 Embedding"""</span></span><br><span class="line">    <span class="comment"># 使用 t-SNE 降维</span></span><br><span class="line">    tsne = TSNE(n_components=<span class="number">2</span>, random_state=<span class="number">42</span>, perplexity=<span class="number">30</span>)</span><br><span class="line">    embeddings_2d = tsne.fit_transform(embeddings)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 绘制</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scatter = plt.scatter(embeddings_2d[:, <span class="number">0</span>], embeddings_2d[:, <span class="number">1</span>], </span><br><span class="line">                            c=labels, cmap=<span class="string">'viridis'</span>, alpha=<span class="number">0.6</span>)</span><br><span class="line">        plt.colorbar(scatter)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.scatter(embeddings_2d[:, <span class="number">0</span>], embeddings_2d[:, <span class="number">1</span>], alpha=<span class="number">0.6</span>)</span><br><span class="line">  </span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.xlabel(<span class="string">"t-SNE 1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"t-SNE 2"</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="在线评估">在线评估</h3>
<p><strong>A/B
测试</strong>：将用户分为两组，一组使用旧模型，一组使用新模型，比较关键指标（CTR、停留时间等）。</p>
<p><strong>实时监控</strong>：监控 Embedding
的统计特性（均值、方差、相似度分布等），发现异常及时告警。</p>
<h2 id="完整代码实现示例">完整代码实现示例</h2>
<h3 id="端到端的推荐系统实现">端到端的推荐系统实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CompleteRecommendationSystem</span>:</span><br><span class="line">    <span class="string">"""完整的推荐系统实现"""</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_users, num_items, embedding_dim=<span class="number">128</span></span>):</span><br><span class="line">        self.num_users = num_users</span><br><span class="line">        self.num_items = num_items</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 创建 Two-Tower 模型</span></span><br><span class="line">        self.model = TwoTowerModel(</span><br><span class="line">            user_feature_dim=<span class="number">50</span>,</span><br><span class="line">            item_feature_dim=<span class="number">40</span>,</span><br><span class="line">            embedding_dim=embedding_dim</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 创建 FAISS 索引</span></span><br><span class="line">        self.item_index = <span class="literal">None</span></span><br><span class="line">        self.item_embeddings = <span class="literal">None</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, train_data, num_epochs=<span class="number">10</span>, batch_size=<span class="number">256</span>, learning_rate=<span class="number">0.001</span></span>):</span><br><span class="line">        <span class="string">"""训练模型"""</span></span><br><span class="line">        <span class="comment"># train_data: [(user_features, item_features, label), ...]</span></span><br><span class="line">    </span><br><span class="line">        dataset = RecommendationDataset(</span><br><span class="line">            user_features=np.array([d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> train_data]),</span><br><span class="line">            item_features=np.array([d[<span class="number">1</span>] <span class="keyword">for</span> d <span class="keyword">in</span> train_data]),</span><br><span class="line">            labels=np.array([d[<span class="number">2</span>] <span class="keyword">for</span> d <span class="keyword">in</span> train_data])</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">        train_two_tower(self.model, train_loader, num_epochs, learning_rate)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_index</span>(<span class="params">self, all_item_features</span>):</span><br><span class="line">        <span class="string">"""构建物品索引"""</span></span><br><span class="line">        <span class="comment"># 获取所有物品的 Embedding</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            item_features_tensor = torch.FloatTensor(all_item_features)</span><br><span class="line">            self.item_embeddings = self.model.get_item_embedding(item_features_tensor)</span><br><span class="line">            self.item_embeddings = self.item_embeddings.numpy()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 创建 FAISS 索引</span></span><br><span class="line">        self.item_index = FAISSIndex(self.embedding_dim, index_type=<span class="string">'IP'</span>)</span><br><span class="line">        self.item_index.add(self.item_embeddings)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">recommend</span>(<span class="params">self, user_features, k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""为用户推荐物品"""</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            user_features_tensor = torch.FloatTensor(user_features).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            user_emb = self.model.get_user_embedding(user_features_tensor)</span><br><span class="line">            user_emb = user_emb.numpy()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 使用 FAISS 搜索</span></span><br><span class="line">        distances, indices = self.item_index.search(user_emb, k=k)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> indices[<span class="number">0</span>], distances[<span class="number">0</span>]</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, test_data, k=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">"""评估模型"""</span></span><br><span class="line">        recalls = []</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> user_features, item_features, label <span class="keyword">in</span> test_data:</span><br><span class="line">            <span class="keyword">if</span> label == <span class="number">1</span>:  <span class="comment"># 只评估正样本</span></span><br><span class="line">                <span class="comment"># 获取用户 Embedding</span></span><br><span class="line">                self.model.<span class="built_in">eval</span>()</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    user_emb = self.model.get_user_embedding(</span><br><span class="line">                        torch.FloatTensor(user_features).unsqueeze(<span class="number">0</span>)</span><br><span class="line">                    ).numpy()</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 搜索 top-k</span></span><br><span class="line">                _, top_k_indices = self.item_index.search(user_emb, k=k)</span><br><span class="line">                top_k_indices = top_k_indices[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 计算真实物品的相似度</span></span><br><span class="line">                item_emb = self.model.get_item_embedding(</span><br><span class="line">                    torch.FloatTensor(item_features).unsqueeze(<span class="number">0</span>)</span><br><span class="line">                ).numpy()</span><br><span class="line">            </span><br><span class="line">                similarity = np.dot(user_emb[<span class="number">0</span>], item_emb[<span class="number">0</span>])</span><br><span class="line">            </span><br><span class="line">                <span class="comment"># 检查是否在 top-k 中</span></span><br><span class="line">                <span class="comment"># 这里简化处理，实际应该比较 item_id</span></span><br><span class="line">                recalls.append(<span class="number">1.0</span>)  <span class="comment"># 简化版本</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> np.mean(recalls) <span class="keyword">if</span> recalls <span class="keyword">else</span> <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<h2 id="常见问题与解答">常见问题与解答</h2>
<h3 id="q1-embedding-维度如何选择">Q1: Embedding 维度如何选择？</h3>
<p><strong>A</strong>: Embedding
维度的选择需要在表达能力和计算效率之间平衡：</p>
<ul>
<li><strong>小维度（32-64）</strong>：计算快，内存占用小，但表达能力有限</li>
<li><strong>中等维度（128-256）</strong>：平衡点，适合大多数场景</li>
<li><strong>大维度（512+）</strong>：表达能力强，但计算和存储成本高</li>
</ul>
<p>建议从 128
维开始，根据实际效果调整。可以通过实验不同维度下的召回率来选择。</p>
<h3 id="q2-如何处理冷启动问题">Q2: 如何处理冷启动问题？</h3>
<p><strong>A</strong>: 冷启动问题可以通过以下方式缓解：</p>
<ol type="1">
<li><strong>内容特征</strong>：使用物品的内容特征（类别、描述等）初始化
Embedding</li>
<li><strong>迁移学习</strong>：从相似物品或类别中迁移 Embedding</li>
<li><strong>多任务学习</strong>：同时学习 Embedding 和内容理解任务</li>
<li><strong>探索策略</strong>：对新物品给予更高的曝光机会</li>
</ol>
<h3 id="q3-用户-embedding-和物品-embedding-是否需要在同一空间">Q3: 用户
Embedding 和物品 Embedding 是否需要在同一空间？</h3>
<p><strong>A</strong>: 是的，为了计算相似度，用户和物品的 Embedding
必须在同一向量空间中。Two-Tower
模型通过共享相同的输出维度来保证这一点。</p>
<h3 id="q4-如何更新-embedding">Q4: 如何更新 Embedding？</h3>
<p><strong>A</strong>: 有几种更新策略：</p>
<ol type="1">
<li><strong>全量重训</strong>：定期使用所有数据重新训练</li>
<li><strong>增量更新</strong>：使用新数据微调模型</li>
<li><strong>在线学习</strong>：实时更新
Embedding（需要谨慎，可能不稳定）</li>
</ol>
<h3 id="q5-负采样数量如何选择">Q5: 负采样数量如何选择？</h3>
<p><strong>A</strong>: 负采样数量通常选择：</p>
<ul>
<li><strong>小规模</strong>：5-10 个负样本</li>
<li><strong>中大规模</strong>：10-50 个负样本</li>
<li><strong>超大规模</strong>：50-100+ 个负样本</li>
</ul>
<p>可以通过实验选择最优值。注意负采样数量增加会提升训练时间。</p>
<h3 id="q6-two-tower-模型和深度协同过滤的区别">Q6: Two-Tower
模型和深度协同过滤的区别？</h3>
<p><strong>A</strong>:</p>
<ul>
<li><strong>Two-Tower</strong>：用户和物品分别通过独立的网络，适合特征丰富的场景</li>
<li><strong>深度协同过滤</strong>：通常将用户 ID 和物品 ID 直接
Embedding，然后拼接输入网络</li>
</ul>
<p>Two-Tower 更适合利用丰富的特征信息。</p>
<h3 id="q7-如何加速训练">Q7: 如何加速训练？</h3>
<p><strong>A</strong>: 加速训练的方法：</p>
<ol type="1">
<li><strong>批量内负采样</strong>：减少负采样计算</li>
<li><strong>混合精度训练</strong>：使用 FP16 减少内存和加速</li>
<li><strong>分布式训练</strong>：多 GPU/多机训练</li>
<li><strong>梯度累积</strong>：模拟更大的 batch size</li>
</ol>
<h3 id="q8-embedding-是否需要归一化">Q8: Embedding 是否需要归一化？</h3>
<p><strong>A</strong>: 取决于相似度计算方式：</p>
<ul>
<li><strong>内积</strong>：如果使用内积计算相似度，建议归一化（等价于余弦相似度）</li>
<li><strong>欧氏距离</strong>：不需要归一化</li>
<li><strong>余弦相似度</strong>：必须归一化</li>
</ul>
<p>归一化还能稳定训练过程。</p>
<h3 id="q9-如何处理序列信息">Q9: 如何处理序列信息？</h3>
<p><strong>A</strong>: 处理序列信息的方法：</p>
<ol type="1">
<li><strong>平均池化</strong>：对历史序列的 Embedding 求平均</li>
<li><strong>RNN/LSTM</strong>：使用循环神经网络处理序列</li>
<li><strong>Transformer</strong>：使用注意力机制捕获序列依赖</li>
<li><strong>Graph Neural Network</strong>：将序列建模为图</li>
</ol>
<h3 id="q10-如何评估-embedding-质量">Q10: 如何评估 Embedding 质量？</h3>
<p><strong>A</strong>: 可以从多个维度评估：</p>
<ol type="1">
<li><strong>离线指标</strong>：Recall@K、NDCG@K、覆盖率等</li>
<li><strong>相似度分布</strong>：检查相似度分布是否合理</li>
<li><strong>可视化</strong>：使用 t-SNE 可视化 Embedding 分布</li>
<li><strong>在线指标</strong>：CTR、停留时间、转化率等</li>
</ol>
<h3 id="q11-faissannoyhnsw-如何选择">Q11: FAISS、Annoy、HNSW
如何选择？</h3>
<p><strong>A</strong>:</p>
<ul>
<li><strong>FAISS</strong>：功能最全，适合生产环境，需要编译</li>
<li><strong>Annoy</strong>：最简单，适合中小规模，支持持久化</li>
<li><strong>HNSW</strong>：精度和速度平衡最好，适合对精度要求高的场景</li>
</ul>
<p>建议先尝试 HNSW，如果满足需求就使用；如果需要更多功能，考虑
FAISS。</p>
<h3 id="q12-如何处理多模态特征">Q12: 如何处理多模态特征？</h3>
<p><strong>A</strong>: 多模态特征可以通过以下方式融合：</p>
<ol type="1">
<li><strong>早期融合</strong>：在输入层拼接所有特征</li>
<li><strong>晚期融合</strong>：每个模态单独处理，最后融合 Embedding</li>
<li><strong>注意力机制</strong>：学习不同模态的权重</li>
</ol>
<h3 id="q13-embedding-可以用于其他任务吗">Q13: Embedding
可以用于其他任务吗？</h3>
<p><strong>A</strong>: 可以！Embedding 可以用于：</p>
<ol type="1">
<li><strong>相似物品推荐</strong>：基于物品 Embedding 找相似物品</li>
<li><strong>用户聚类</strong>：基于用户 Embedding 进行用户分群</li>
<li><strong>异常检测</strong>：检测 Embedding 异常的物品或用户</li>
<li><strong>可视化分析</strong>：理解用户和物品的分布</li>
</ol>
<h3 id="q14-如何解决-embedding-的稀疏性问题">Q14: 如何解决 Embedding
的稀疏性问题？</h3>
<p><strong>A</strong>:</p>
<ol type="1">
<li><strong>正则化</strong>：L2 正则化防止过拟合</li>
<li><strong>Dropout</strong>：在训练时随机丢弃部分特征</li>
<li><strong>特征工程</strong>：提取更有意义的特征</li>
<li><strong>预训练</strong>：使用大规模数据预训练 Embedding</li>
</ol>
<h3 id="q15-训练时出现-nan-怎么办">Q15: 训练时出现 NaN 怎么办？</h3>
<p><strong>A</strong>: NaN 通常由以下原因引起：</p>
<ol type="1">
<li><strong>学习率过大</strong>：降低学习率</li>
<li><strong>梯度爆炸</strong>：使用梯度裁剪</li>
<li><strong>数值不稳定</strong>：检查输入数据是否有异常值</li>
<li><strong>损失函数</strong>：在损失函数中添加小的 epsilon 防止
log(0)</li>
</ol>
<h2 id="总结">总结</h2>
<p>Embedding
表示学习是推荐系统的核心技术之一。本文从基础理论出发，详细介绍了
Item2Vec、Node2Vec、Two-Tower、DSSM
等经典算法，并提供了完整的代码实现。同时，我们还深入探讨了负采样策略、ANN
检索技术以及 Embedding 质量评估方法。</p>
<p>在实际应用中，需要根据具体场景选择合适的算法和参数。建议：</p>
<ol type="1">
<li><strong>从小规模开始</strong>：先用小规模数据验证算法和流程</li>
<li><strong>迭代优化</strong>：根据评估结果不断调整模型和参数</li>
<li><strong>关注工程细节</strong>：负采样、索引构建、特征工程等细节对效果影响很大</li>
<li><strong>持续监控</strong>：建立完善的监控体系，及时发现和解决问题</li>
</ol>
<p>希望本文能够帮助读者深入理解推荐系统中的 Embedding
技术，并在实际项目中应用这些方法。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    
    
    
    
    
    
    <ul>
        <li>本文标题：推荐系统（五）—— Embedding表示学习</li>
        <li>本文作者：Chen Kai</li>
        <li>创建时间：2025-06-26 00:00:00</li>
        <li>
            本文链接：https://www.chenk.top/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94-Embedding%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Recommendation-Systems/">#Recommendation Systems</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/Deep-Learning/">#Deep Learning</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/Embedding/">#Embedding</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94-%E5%BA%8F%E5%88%97%E6%8E%A8%E8%8D%90%E4%B8%8E%E4%BC%9A%E8%AF%9D%E5%BB%BA%E6%A8%A1/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">推荐系统（六）—— 序列推荐与会话建模</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94-%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">推荐系统（二）—— 协同过滤与矩阵分解</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script data-pjax>
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'p2Cu9MgjoKzo3VmulhNLIusH-gzGzoHsz',
                    appKey: 'QThQHg3c8sVwGpzg9lu8zEG3',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: '😜 尽情赞美帅气伟大的ck吧~',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Chen Kai';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('true') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2026&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Chen Kai</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
            </div>
        
        <div class="theme-info info-item">
            <!-- 由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a> -->
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#embedding-%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">Embedding 基础理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-embedding"><span class="nav-number">1.1.</span> <span class="nav-text">什么是 Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">1.2.</span> <span class="nav-text">Embedding 的优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">1.3.</span> <span class="nav-text">Embedding 学习的基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="nav-number">1.4.</span> <span class="nav-text">损失函数与优化目标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#item2vec-%E4%B8%8E-word2vec-%E7%B1%BB%E6%AF%94"><span class="nav-number">2.</span> <span class="nav-text">Item2Vec 与 Word2Vec 类比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec-%E5%9B%9E%E9%A1%BE"><span class="nav-number">2.1.</span> <span class="nav-text">Word2Vec 回顾</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#item2vec-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">2.2.</span> <span class="nav-text">Item2Vec 的核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#item2vec-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="nav-number">2.3.</span> <span class="nav-text">Item2Vec 算法流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#item2vec-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.4.</span> <span class="nav-text">Item2Vec 完整实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF"><span class="nav-number">2.4.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF"><span class="nav-number">2.4.2.</span> <span class="nav-text">解决思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E8%80%83%E8%99%91"><span class="nav-number">2.4.3.</span> <span class="nav-text">设计考虑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%82%B9%E8%A7%A3%E8%AF%BB"><span class="nav-number">2.4.4.</span> <span class="nav-text">关键点解读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1"><span class="nav-number">2.4.5.</span> <span class="nav-text">设计权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">2.4.6.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.4.7.</span> <span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#item2vec-%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">2.5.</span> <span class="nav-text">Item2Vec 的优缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#node2vec-%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="nav-number">3.</span> <span class="nav-text">Node2Vec 图嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">3.1.</span> <span class="nav-text">图嵌入的基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#node2vec-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-number">3.2.</span> <span class="nav-text">Node2Vec 算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#node2vec-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.3.</span> <span class="nav-text">Node2Vec 完整实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF-1"><span class="nav-number">3.3.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-1"><span class="nav-number">3.3.2.</span> <span class="nav-text">解决思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E8%80%83%E8%99%91-1"><span class="nav-number">3.3.3.</span> <span class="nav-text">设计考虑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%82%B9%E8%A7%A3%E8%AF%BB-1"><span class="nav-number">3.3.4.</span> <span class="nav-text">关键点解读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1-1"><span class="nav-number">3.3.5.</span> <span class="nav-text">设计权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-1"><span class="nav-number">3.3.6.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">3.3.7.</span> <span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#node2vec-%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="nav-number">3.4.</span> <span class="nav-text">Node2Vec 参数调优</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#two-tower-%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3"><span class="nav-number">4.</span> <span class="nav-text">Two-Tower 模型详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#two-tower-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">Two-Tower 架构概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#two-tower-%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BD%A2%E5%BC%8F"><span class="nav-number">4.2.</span> <span class="nav-text">Two-Tower 的数学形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#two-tower-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.3.</span> <span class="nav-text">Two-Tower 完整实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF-2"><span class="nav-number">4.3.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-2"><span class="nav-number">4.3.2.</span> <span class="nav-text">解决思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E8%80%83%E8%99%91-2"><span class="nav-number">4.3.3.</span> <span class="nav-text">设计考虑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%82%B9%E8%A7%A3%E8%AF%BB-2"><span class="nav-number">4.3.4.</span> <span class="nav-text">关键点解读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1-2"><span class="nav-number">4.3.5.</span> <span class="nav-text">设计权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-2"><span class="nav-number">4.3.6.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-2"><span class="nav-number">4.3.7.</span> <span class="nav-text">使用示例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dssm-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D"><span class="nav-number">5.</span> <span class="nav-text">DSSM 深度语义匹配</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dssm-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-number">5.1.</span> <span class="nav-text">DSSM 算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dssm-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">5.2.</span> <span class="nav-text">DSSM 网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dssm-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.3.</span> <span class="nav-text">DSSM 完整实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF-3"><span class="nav-number">5.3.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-3"><span class="nav-number">5.3.2.</span> <span class="nav-text">解决思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E8%80%83%E8%99%91-3"><span class="nav-number">5.3.3.</span> <span class="nav-text">设计考虑</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-3"><span class="nav-number"></span> <span class="nav-text">使用示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%82%B9%E8%A7%A3%E8%AF%BB-3"><span class="nav-number">0.0.1.</span> <span class="nav-text">关键点解读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1-3"><span class="nav-number">0.0.2.</span> <span class="nav-text">设计权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-3"><span class="nav-number">0.0.3.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-4"><span class="nav-number">0.0.4.</span> <span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dssm-%E7%9A%84%E6%94%B9%E8%BF%9Bcdssm"><span class="nav-number">0.1.</span> <span class="nav-text">DSSM 的改进：CDSSM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#youtube-%E5%8F%8C%E5%A1%94%E5%8F%AC%E5%9B%9E"><span class="nav-number">1.</span> <span class="nav-text">YouTube 双塔召回</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#youtube-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text">YouTube 推荐系统架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#youtube-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="nav-number">1.2.</span> <span class="nav-text">YouTube 双塔模型设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#youtube-%E5%8F%8C%E5%A1%94%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.</span> <span class="nav-text">YouTube 双塔完整实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF-4"><span class="nav-number">1.3.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-4"><span class="nav-number">1.3.2.</span> <span class="nav-text">解决思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E8%80%83%E8%99%91-4"><span class="nav-number">1.3.3.</span> <span class="nav-text">设计考虑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%82%B9%E8%A7%A3%E8%AF%BB-4"><span class="nav-number">1.3.4.</span> <span class="nav-text">关键点解读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1-4"><span class="nav-number">1.3.5.</span> <span class="nav-text">设计权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-4"><span class="nav-number">1.3.6.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-5"><span class="nav-number">1.3.7.</span> <span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#youtube-%E5%8F%8C%E5%A1%94%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="nav-number">1.4.</span> <span class="nav-text">YouTube 双塔的训练策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%E7%AD%96%E7%95%A5%E8%AF%A6%E8%A7%A3"><span class="nav-number">2.</span> <span class="nav-text">负采样策略详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="nav-number">2.1.</span> <span class="nav-text">负采样的必要性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="nav-number">2.2.</span> <span class="nav-text">随机负采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%A2%91%E7%8E%87%E7%9A%84%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="nav-number">2.3.</span> <span class="nav-text">基于频率的负采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%86%85%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="nav-number">2.4.</span> <span class="nav-text">批量内负采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%B0%E9%9A%BE%E8%B4%9F%E9%87%87%E6%A0%B7hard-negative-sampling"><span class="nav-number">2.5.</span> <span class="nav-text">困难负采样（Hard Negative
Sampling）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E8%B4%9F%E9%87%87%E6%A0%B7%E7%AD%96%E7%95%A5"><span class="nav-number">2.6.</span> <span class="nav-text">混合负采样策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ann-%E8%BF%91%E9%82%BB%E6%A3%80%E7%B4%A2"><span class="nav-number">3.</span> <span class="nav-text">ANN 近邻检索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ann-%E6%A3%80%E7%B4%A2%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="nav-number">3.1.</span> <span class="nav-text">ANN 检索的必要性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#faiss-%E5%BA%93%E4%BD%BF%E7%94%A8"><span class="nav-number">3.2.</span> <span class="nav-text">FAISS 库使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF-5"><span class="nav-number">3.2.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-5"><span class="nav-number">3.2.2.</span> <span class="nav-text">解决思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E8%80%83%E8%99%91-5"><span class="nav-number">3.2.3.</span> <span class="nav-text">设计考虑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%82%B9%E8%A7%A3%E8%AF%BB-5"><span class="nav-number">3.2.4.</span> <span class="nav-text">关键点解读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1-5"><span class="nav-number">3.2.5.</span> <span class="nav-text">设计权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-5"><span class="nav-number">3.2.6.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-6"><span class="nav-number">3.2.7.</span> <span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#annoy-%E5%BA%93%E4%BD%BF%E7%94%A8"><span class="nav-number">3.3.</span> <span class="nav-text">Annoy 库使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hnsw-%E7%AE%97%E6%B3%95"><span class="nav-number">3.4.</span> <span class="nav-text">HNSW 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ann-%E7%B4%A2%E5%BC%95%E9%80%89%E6%8B%A9%E6%8C%87%E5%8D%97"><span class="nav-number">3.5.</span> <span class="nav-text">ANN 索引选择指南</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#embedding-%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0"><span class="nav-number">4.</span> <span class="nav-text">Embedding 质量评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.1.</span> <span class="nav-text">评估指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">4.2.</span> <span class="nav-text">Embedding 可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E7%BA%BF%E8%AF%84%E4%BC%B0"><span class="nav-number">4.3.</span> <span class="nav-text">在线评估</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E7%A4%BA%E4%BE%8B"><span class="nav-number">5.</span> <span class="nav-text">完整代码实现示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.1.</span> <span class="nav-text">端到端的推荐系统实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E7%AD%94"><span class="nav-number">6.</span> <span class="nav-text">常见问题与解答</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-embedding-%E7%BB%B4%E5%BA%A6%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="nav-number">6.1.</span> <span class="nav-text">Q1: Embedding 维度如何选择？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98"><span class="nav-number">6.2.</span> <span class="nav-text">Q2: 如何处理冷启动问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-%E7%94%A8%E6%88%B7-embedding-%E5%92%8C%E7%89%A9%E5%93%81-embedding-%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%9C%A8%E5%90%8C%E4%B8%80%E7%A9%BA%E9%97%B4"><span class="nav-number">6.3.</span> <span class="nav-text">Q3: 用户
Embedding 和物品 Embedding 是否需要在同一空间？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-%E5%A6%82%E4%BD%95%E6%9B%B4%E6%96%B0-embedding"><span class="nav-number">6.4.</span> <span class="nav-text">Q4: 如何更新 Embedding？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-%E8%B4%9F%E9%87%87%E6%A0%B7%E6%95%B0%E9%87%8F%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="nav-number">6.5.</span> <span class="nav-text">Q5: 负采样数量如何选择？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-two-tower-%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">6.6.</span> <span class="nav-text">Q6: Two-Tower
模型和深度协同过滤的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q7-%E5%A6%82%E4%BD%95%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83"><span class="nav-number">6.7.</span> <span class="nav-text">Q7: 如何加速训练？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q8-embedding-%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">6.8.</span> <span class="nav-text">Q8: Embedding 是否需要归一化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q9-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%BA%8F%E5%88%97%E4%BF%A1%E6%81%AF"><span class="nav-number">6.9.</span> <span class="nav-text">Q9: 如何处理序列信息？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q10-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0-embedding-%E8%B4%A8%E9%87%8F"><span class="nav-number">6.10.</span> <span class="nav-text">Q10: 如何评估 Embedding 质量？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q11-faissannoyhnsw-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="nav-number">6.11.</span> <span class="nav-text">Q11: FAISS、Annoy、HNSW
如何选择？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q12-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%A4%9A%E6%A8%A1%E6%80%81%E7%89%B9%E5%BE%81"><span class="nav-number">6.12.</span> <span class="nav-text">Q12: 如何处理多模态特征？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q13-embedding-%E5%8F%AF%E4%BB%A5%E7%94%A8%E4%BA%8E%E5%85%B6%E4%BB%96%E4%BB%BB%E5%8A%A1%E5%90%97"><span class="nav-number">6.13.</span> <span class="nav-text">Q13: Embedding
可以用于其他任务吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q14-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3-embedding-%E7%9A%84%E7%A8%80%E7%96%8F%E6%80%A7%E9%97%AE%E9%A2%98"><span class="nav-number">6.14.</span> <span class="nav-text">Q14: 如何解决 Embedding
的稀疏性问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q15-%E8%AE%AD%E7%BB%83%E6%97%B6%E5%87%BA%E7%8E%B0-nan-%E6%80%8E%E4%B9%88%E5%8A%9E"><span class="nav-number">6.15.</span> <span class="nav-text">Q15: 训练时出现 NaN 怎么办？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/lang-switch.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
